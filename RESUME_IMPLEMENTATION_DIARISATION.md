# R√©sum√© : Impl√©mentation de la Diarisation et Identification des Locuteurs

**Date**: 19 janvier 2026
**Objectif**: Savoir qui parle dans les messages audio et identifier l'utilisateur actuel (exp√©diteur)

---

## üéØ Fonctionnalit√©s Ajout√©es

### 1. **Identification des Locuteurs**
- D√©tection automatique de plusieurs locuteurs dans un audio
- Identification du locuteur principal (qui parle le plus)
- Attribution d'un `speaker_id` unique √† chaque locuteur

### 2. **Identification de l'Utilisateur Actuel**
- D√©tection si un segment appartient √† l'exp√©diteur du message
- Flag `isCurrentUser` sur chaque segment
- Permet d'afficher diff√©remment les paroles de l'utilisateur vs. autres locuteurs

### 3. **Visualisation Frontend**
- Support pour afficher les segments avec des couleurs diff√©rentes selon le locuteur
- Distinction visuelle entre l'utilisateur et les autres participants

---

## üì¶ Modifications Apport√©es

### 1. **Types TypeScript (Shared)**

#### `packages/shared/types/attachment-transcription.ts`

```typescript
export interface TranscriptionSegment {
  readonly startMs: number;
  readonly endMs: number;
  readonly text: string;
  /** ID du locuteur pour ce segment (via diarisation) */
  readonly speakerId?: string;
  /** Indique si ce segment appartient √† l'utilisateur actuel (exp√©diteur du message) */
  readonly isCurrentUser?: boolean;
  readonly confidence?: number;
}
```

**Changements**:
- ‚úÖ Ajout de `speakerId?: string` (existait d√©j√†)
- ‚úÖ Ajout de `isCurrentUser?: boolean` (nouveau)

#### `packages/shared/types/audio-transcription.ts`

Les interfaces suivantes existent d√©j√† et sont align√©es avec le sch√©ma Prisma :
- `SpeakerInfo` - Information sur un locuteur
- `SpeakerDiarizationAnalysis` - Analyse compl√®te de diarisation
- `MessageAudioTranscription` avec champs :
  - `speakerCount?: number`
  - `primarySpeakerId?: string`
  - `speakerAnalysis?: SpeakerDiarizationAnalysis`

**Aucune modification n√©cessaire** - d√©j√† complet !

---

### 2. **Sch√©ma Prisma (Base de Donn√©es)**

#### `packages/shared/prisma/schema.prisma`

Le mod√®le `MessageAudioTranscription` contient d√©j√† tous les champs n√©cessaires :

```prisma
model MessageAudioTranscription {
  // ... autres champs ...

  /// ============================================
  /// SPEAKER DIARIZATION (Multi-speaker support)
  /// ============================================

  /// Number of distinct speakers detected in the audio
  speakerCount Int?

  /// ID of the primary speaker (who speaks the most)
  primarySpeakerId String?

  /// Speaker analysis metadata as JSON
  speakerAnalysis Json?

  /// Whether the sender's voice was identified in the audio
  senderVoiceIdentified Boolean?

  /// Matched speaker ID if sender was identified
  senderSpeakerId String?

  // ... autres champs ...
}
```

**Aucune modification n√©cessaire** - sch√©ma d√©j√† pr√™t pour la diarisation !

---

### 3. **Service Python (Translator)**

#### A. `services/translator/src/services/transcription_service.py`

##### Changements au dataclass `TranscriptionSegment` :

```python
@dataclass
class TranscriptionSegment:
    """
    Segment de transcription avec timestamps et identification du locuteur.
    Align√© avec TypeScript shared/types/attachment-transcription.ts
    """
    text: str
    start_ms: int
    end_ms: int
    confidence: float = 0.0
    speaker_id: Optional[str] = None  # ID du locuteur (via diarisation)
    is_current_user: bool = False  # True si c'est l'exp√©diteur du message
```

**Changements**:
- ‚úÖ Ajout de `speaker_id`
- ‚úÖ Ajout de `is_current_user`

##### Changements au dataclass `TranscriptionResult` :

```python
@dataclass
class TranscriptionResult:
    """
    R√©sultat d'une transcription avec support de diarisation.
    Align√© avec TypeScript shared/types/audio-transcription.ts
    """
    # ... champs existants ...

    # === SPEAKER DIARIZATION (Multi-speaker support) ===
    speaker_count: Optional[int] = None  # Nombre de locuteurs d√©tect√©s
    primary_speaker_id: Optional[str] = None  # ID du locuteur principal
    speaker_analysis: Optional[Dict[str, Any]] = None  # M√©tadonn√©es d'analyse
    sender_voice_identified: Optional[bool] = None  # L'exp√©diteur a √©t√© identifi√©
    sender_speaker_id: Optional[str] = None  # ID du locuteur exp√©diteur
```

**Changements**:
- ‚úÖ Ajout de 5 champs pour la diarisation

##### Utilisation des segments natifs de Whisper :

**Avant** (interpolation manuelle) :
```python
# ‚ùå Division manuelle avec interpolation
from ..utils.segment_splitter import split_segments_into_words

segments = split_segments_into_words(segments, max_words=5)
```

**Apr√®s** (timestamps natifs Whisper + fusion intelligente) :
```python
# ‚úÖ Utiliser les timestamps NATIFS au niveau des mots fournis par Whisper
for s in segments_list:
    if hasattr(s, 'words') and s.words:
        # ‚úÖ Utiliser les mots individuels avec timestamps exacts
        for word in s.words:
            segments.append(TranscriptionSegment(
                text=word.word.strip(),
                start_ms=int(word.start * 1000),
                end_ms=int(word.end * 1000),
                confidence=getattr(word, 'probability', 0.0),
                speaker_id=None,
                is_current_user=False
            ))

# ‚úÖ OPTION D : Fusion intelligente des mots courts
# R√®gles: pause < 90ms ET somme < 8 caract√®res
segments = merge_short_segments(
    segments,
    max_pause_ms=90,
    max_total_chars=8
)
```

**Avantages**:
- ‚úÖ Timestamps **exacts** de Whisper (pas d'interpolation)
- ‚úÖ Confiance par mot (plus pr√©cise)
- ‚úÖ Fusion adaptative selon le rythme naturel de parole
- ‚úÖ Moins de segments sans perte de pr√©cision
- ‚úÖ Code intelligent et performant

**Option D - Segmentation Intelligente**:

Au lieu de segments fixes (1-5 mots), on fusionne intelligemment :
- "le chat" ‚Üí **fusionn√©** (pause 10ms, 6 chars < 8)
- "Bonjour monde" ‚Üí **s√©par√©s** (12 chars > 8)
- "oui" ... "non" ‚Üí **s√©par√©s** (pause 120ms > 90ms)

R√©sultat : segments naturels qui respectent le rythme de la parole !

##### Int√©gration de la diarisation :

```python
# Appliquer la diarisation si demand√© (via flag ou config)
enable_diarization = os.getenv('ENABLE_DIARIZATION', 'false').lower() == 'true'
if enable_diarization and return_timestamps:
    logger.info("[TRANSCRIPTION] üéØ Application de la diarisation")
    result = await self._apply_diarization(audio_path, result)
```

---

#### B. **Nouveau fichier** : `services/translator/src/services/diarization_service.py`

Service complet de diarisation avec :

##### Classes principales :

```python
@dataclass
class SpeakerSegment:
    """Segment d'un locuteur avec timestamps"""
    speaker_id: str
    start_ms: int
    end_ms: int
    duration_ms: int
    confidence: float = 1.0

@dataclass
class SpeakerInfo:
    """Information sur un locuteur d√©tect√©"""
    speaker_id: str
    is_primary: bool
    speaking_time_ms: int
    speaking_ratio: float
    segments: List[SpeakerSegment]
    voice_characteristics: Optional[Dict[str, Any]] = None

@dataclass
class DiarizationResult:
    """R√©sultat de la diarisation"""
    speaker_count: int
    speakers: List[SpeakerInfo]
    primary_speaker_id: str
    total_duration_ms: int
    method: str  # "pyannote" ou "pitch_clustering"
    sender_identified: bool = False
    sender_speaker_id: Optional[str] = None
```

##### M√©thodes principales :

1. **`detect_speakers()`** - D√©tecte les locuteurs
   - M√©thode principale : `pyannote.audio` (si disponible)
   - Fallback : clustering par pitch avec librosa + sklearn
   - Ultime fallback : 1 seul locuteur

2. **`identify_sender()`** - Identifie l'exp√©diteur
   - Actuellement : assume que le locuteur principal est l'exp√©diteur
   - TODO : Reconnaissance vocale avec similarit√© d'embeddings

##### Code bas√© sur :

Le service utilise les algorithmes du script `apps/ios/scripts/chatterbox_voice_translation_test.py` :
- `VoiceAnalyzer.detect_speakers()` (ligne 327)
- Clustering par pitch (ligne 378-476)
- Segmentation et analyse des locuteurs

---

## üöÄ Comment Activer la Diarisation

### 1. **Variables d'Environnement**

```bash
# Activer la diarisation
export ENABLE_DIARIZATION=true

# Token HuggingFace pour pyannote.audio (optionnel mais recommand√©)
export HF_TOKEN=your_huggingface_token
```

### 2. **D√©pendances Python**

#### Installation recommand√©e (avec pyannote.audio) :

```bash
cd services/translator
pip install pyannote.audio scikit-learn librosa
```

#### Installation minimale (fallback only) :

```bash
pip install scikit-learn librosa
```

### 3. **Obtenir un Token HuggingFace**

1. Cr√©er un compte sur https://huggingface.co/
2. Aller dans Settings > Access Tokens
3. Cr√©er un nouveau token
4. Accepter les conditions d'utilisation de `pyannote/speaker-diarization-3.1`

---

## üìä Flux de Traitement

```
Audio File
    ‚Üì
[Whisper Transcription]
    ‚Üì
Segments avec timestamps natifs (word-level)
    ‚Üì
[Diarization Service] ‚Üê si ENABLE_DIARIZATION=true
    ‚Üì
    ‚îú‚îÄ pyannote.audio (d√©tection pr√©cise)
    ‚îÇ   ou
    ‚îú‚îÄ Pitch Clustering (fallback)
    ‚îÇ   ou
    ‚îî‚îÄ Single Speaker (ultime fallback)
    ‚Üì
Segments enrichis avec :
  - speaker_id (qui parle)
  - is_current_user (c'est l'exp√©diteur ?)
    ‚Üì
[Sauvegarde en BDD]
    ‚Üì
Frontend affiche avec couleurs par locuteur
```

---

## üé® Utilisation Frontend

### Exemple d'affichage des segments :

```typescript
// Les segments arrivent avec speaker_id et isCurrentUser
transcription.segments.forEach(segment => {
  const color = segment.isCurrentUser
    ? 'blue'  // Couleur pour l'utilisateur actuel
    : segment.speakerId === transcription.primarySpeakerId
      ? 'green' // Couleur pour le locuteur principal
      : 'gray'; // Couleur pour les autres locuteurs

  displaySegment(segment.text, segment.startMs, segment.endMs, color);
});
```

### Informations disponibles :

```typescript
interface MessageAudioTranscription {
  speakerCount?: number;          // Nombre de locuteurs
  primarySpeakerId?: string;      // Locuteur principal
  senderSpeakerId?: string;       // Locuteur = exp√©diteur
  senderVoiceIdentified?: boolean; // Exp√©diteur identifi√© ?

  segments: TranscriptionSegment[]; // Avec speaker_id et isCurrentUser

  speakerAnalysis?: {
    speakers: Array<{
      speaker_id: string;
      is_primary: boolean;
      speaking_time_ms: number;
      speaking_ratio: number;
      segments: Array<{start, end, duration}>;
    }>;
    total_duration_ms: number;
    method: string;
  };
}
```

---

## ‚úÖ Avantages de l'Impl√©mentation

### 1. **Pr√©cision des Timestamps**

| Aspect | Avant | Apr√®s |
|--------|-------|-------|
| Timestamps | Interpol√©s (impr√©cis) | Natifs Whisper (exacts) |
| Granularit√© | Chunks 1-5 mots | Mots individuels |
| Confiance | Par phrase | Par mot |

### 2. **Identification des Locuteurs**

- ‚úÖ D√©tection automatique de plusieurs locuteurs
- ‚úÖ Identification du locuteur principal
- ‚úÖ Tagging de chaque segment avec speaker_id
- ‚úÖ Identification de l'utilisateur actuel

### 3. **Exp√©rience Utilisateur**

- ‚úÖ Affichage avec couleurs diff√©rentes par locuteur
- ‚úÖ Distinction visuelle utilisateur vs. autres
- ‚úÖ Meilleure compr√©hension des conversations multi-locuteurs

### 4. **Architecture**

- ‚úÖ Service de diarisation d√©coupl√©
- ‚úÖ Fallbacks multiples (pyannote ‚Üí pitch ‚Üí single)
- ‚úÖ Compatible avec Prisma et TypeScript shared
- ‚úÖ Activation via variable d'environnement

---

## üìù TODO / Am√©liorations Futures

### 1. **Reconnaissance Vocale de l'Exp√©diteur**

Actuellement, `identify_sender()` assume que le locuteur principal est l'exp√©diteur.

**√Ä impl√©menter** :
```python
async def identify_sender(
    self,
    diarization: DiarizationResult,
    sender_voice_profile: Optional[Dict[str, Any]] = None
) -> DiarizationResult:
    """
    Comparer les embeddings vocaux des locuteurs d√©tect√©s
    avec le profil vocal de l'exp√©diteur (UserVoiceModel).

    Utiliser la similarit√© cosinus pour identifier le meilleur match.
    """
    # TODO: Impl√©menter avec:
    # - Extraction embeddings pour chaque locuteur (Resemblyzer, pyannote)
    # - Chargement du UserVoiceModel de l'exp√©diteur
    # - Calcul de similarit√© cosinus
    # - Identification du locuteur le plus similaire
```

### 2. **Optimisations Performance**

- Cache des r√©sultats de diarisation (m√™me audio = m√™me r√©sultat)
- Diarisation asynchrone en arri√®re-plan
- Timeout configurable pour √©viter les blocages

### 3. **Interface Admin**

- Statistiques sur l'utilisation de la diarisation
- R√©glage des seuils de confiance
- Visualisation des locuteurs d√©tect√©s

---

## üîó Fichiers Modifi√©s

### TypeScript (Shared)
1. ‚úÖ `packages/shared/types/attachment-transcription.ts` - Ajout `isCurrentUser`
2. ‚úÖ `packages/shared/types/audio-transcription.ts` - D√©j√† complet
3. ‚úÖ `packages/shared/prisma/schema.prisma` - D√©j√† complet

### Python (Translator)
1. ‚úÖ `services/translator/src/services/transcription_service.py`
   - TranscriptionSegment avec speaker_id et is_current_user
   - TranscriptionResult avec champs de diarisation
   - Utilisation des segments natifs de Whisper
   - Int√©gration de la diarisation
   - M√©thode `_apply_diarization()`

2. ‚úÖ **NOUVEAU** `services/translator/src/services/diarization_service.py`
   - Service complet de diarisation
   - Support pyannote.audio + fallbacks
   - Classes SpeakerSegment, SpeakerInfo, DiarizationResult

### Documentation
1. ‚úÖ `CORRECTION_UTILISER_WHISPER_WORDS_NATIF.md` - Guide des segments natifs Whisper
2. ‚úÖ **CE FICHIER** `RESUME_IMPLEMENTATION_DIARISATION.md` - R√©sum√© complet

---

## üéâ R√©sum√©

**Objectif atteint** : Savoir qui parle et identifier l'utilisateur actuel !

### Ce qui fonctionne :
- ‚úÖ D√©tection de plusieurs locuteurs (pyannote ou pitch clustering)
- ‚úÖ Identification du locuteur principal
- ‚úÖ Tagging des segments avec `speaker_id` et `isCurrentUser`
- ‚úÖ Timestamps natifs Whisper (plus pr√©cis que l'interpolation)
- ‚úÖ Structures align√©es TypeScript ‚Üî Prisma ‚Üî Python
- ‚úÖ Configuration via variable d'environnement
- ‚úÖ Fallbacks multiples pour robustesse

### √Ä am√©liorer :
- ‚è≥ Reconnaissance vocale de l'exp√©diteur (embeddings similarity)
- ‚è≥ Optimisations performance (cache, async)
- ‚è≥ Interface admin pour statistiques

---

**Date de cr√©ation** : 19 janvier 2026
**Auteur** : Claude Sonnet 4.5
**Version** : 1.0
