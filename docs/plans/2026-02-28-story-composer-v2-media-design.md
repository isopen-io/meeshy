# Story Composer V2 — Media System Design

**Date:** 2026-02-28
**Scope:** Extension du Story Composer avec support média visuel (image/vidéo) et audio (bibliothèque/enregistrement), gestion arrière-plan/premier plan, mixage volume, et Prisme Linguistique appliqué aux textObjects + variantes audio/sous-titres.

---

## 1. Architecture & Modèles de données

### 1.1 Nouveaux types Swift SDK (`StoryModels.swift`)

#### `StoryMediaObject` — objet visuel draggable sur canvas
```swift
public struct StoryMediaObject: Codable, Identifiable, Sendable {
    public var id: String
    public var postMediaId: String      // référence PostMedia en DB
    public var mediaType: String        // "image" | "video"
    public var placement: String        // "foreground" | "background"
    public var x: CGFloat              // normalisé 0–1 (ignoré si background)
    public var y: CGFloat
    public var scale: CGFloat
    public var rotation: CGFloat
    public var volume: Float           // 0.0–1.0 (vidéos foreground seulement)
}
```

#### `StoryAudioPlayerObject` — composant player visible sur canvas
```swift
public struct StoryAudioPlayerObject: Codable, Identifiable, Sendable {
    public var id: String
    public var postMediaId: String      // référence PostMedia en DB
    public var placement: String        // "foreground" | "background"
    public var x: CGFloat              // normalisé 0–1 (foreground uniquement)
    public var y: CGFloat
    public var volume: Float           // 0.0–1.0
    public var waveformSamples: [Float] // ~80 samples extraits à la composition
}
```

#### `StoryAudioVariant` — variante TTS générée par langue
```swift
public struct StoryAudioVariant: Codable, Sendable {
    public var postMediaId: String      // référence PostMedia de la variante
    public var language: String         // code langue IETF (ex: "fr", "en")
    public var isAutoGenerated: Bool
}
```

#### Mise à jour `StoryEffects` (champs ajoutés)
```swift
public var mediaObjects: [StoryMediaObject]?          // images + vidéos sur canvas
public var audioPlayerObjects: [StoryAudioPlayerObject]? // players audio sur canvas
public var backgroundAudioVariants: [StoryAudioVariant]? // variantes TTS audio de fond
public var backgroundAudioVolume: Float?              // 0.0–1.0 (existant migré)
```

#### Mise à jour `StoryTextObject` (champ ajouté)
```swift
public var translations: [String: String]?  // { "en": "Hello", "es": "Hola" }
```

---

### 1.2 Prisma Schema (5 nouveaux champs)

#### `PostMedia` — 2 champs ajoutés
```prisma
/// Code langue du média (pour variantes TTS/sous-titres) ex: "fr", "en"
language    String?
/// ID PostMedia source si ce média est une variante générée automatiquement
variantOf   String?  @db.ObjectId
```

#### `StoryBackgroundAudio` — 3 champs ajoutés
```prisma
/// Généré automatiquement par le pipeline TTS
isAutoGenerated Boolean @default(false)
/// Code langue source de l'audio
sourceLanguage  String?
/// ID StoryBackgroundAudio original si cette entrée est une variante
variantOf       String? @db.ObjectId
```

---

## 2. UX Flow

### 2.1 Sélection média visuel (image/vidéo)

```
Bouton caméra/galerie → PhotosPicker (multi-select off)
  → PhotosPickerItem sélectionné
  → Sheet "Où placer ce média ?"
      ┌─────────────────────────────────────┐
      │  [Arrière-plan]    [Premier plan]   │
      │   Remplit toute     Élément         │
      │   la slide          draggable       │
      └─────────────────────────────────────┘
  → Si image : ImageEditView (crop/filtres) → onAccept
  → Si vidéo : VideoPreviewView (muet/trim) → onAccept(isMuted)
  → Upload via POST /media → postMediaId
  → Créer StoryMediaObject + ajouter à storyEffects.mediaObjects
```

### 2.2 Sélection audio

```
Bouton musique → Sheet "Source audio"
      ┌─────────────────────────────────────┐
      │  [Bibliothèque]  [Enregistrer]      │
      │   DocumentPicker  Micro in-app      │
      └─────────────────────────────────────┘
  → Placement : "Arrière-plan" | "Premier plan"
  → Upload → postMediaId
  → Créer StoryAudioPlayerObject + ajouter à storyEffects.audioPlayerObjects
  → WaveformGenerator extrait ~80 samples → stocké dans waveformSamples
```

### 2.3 Mixage volume (composition)

Si plusieurs sources sonores coexistent sur la même slide :
- Curseur "Son arrière-plan" → `audioPlayerObjects[background].volume`
- Curseur "Son vidéo / audio premier plan" → `audioPlayerObjects[foreground].volume` ou `mediaObjects[video].volume`
- Les deux jouent simultanément avec `AVAudioMixing`

### 2.4 Audio public automatique

Lors de la publication d'une story publique (`isPublic = true`) :
- Si `audioPlayerObjects` contient un objet `placement: "background"`
- Gateway crée une entrée `StoryBackgroundAudio` avec `isPublic = true`
- `postMediaId` référencé pour streaming

---

## 3. Prisme Linguistique — Résolution des langues

### 3.1 TextObjects (côté iOS)

À l'affichage dans `StoryCanvasReaderView` :
```swift
func resolvedText(for obj: StoryTextObject, userLang: String) -> String {
    return obj.translations?[userLang]
        ?? obj.translations?["en"]  // fallback anglais
        ?? obj.content              // fallback original
}
```
Pas de réseau requis — traductions pré-chargées dans `storyEffects.textObjects[n].translations`.

### 3.2 Audio variantes (côté iOS)

Pour l'audio de fond dans `StoryCanvasReaderView` :
```swift
func resolvedAudioPostMediaId(effects: StoryEffects, userLang: String) -> String? {
    let variant = effects.backgroundAudioVariants?.first { $0.language == userLang }
    return variant?.postMediaId ?? effects.backgroundAudioId
}
```

### 3.3 Captions/sous-titres vidéo

`PostMedia.transcription: Json?` contient déjà les segments Whisper.
`PostMedia.translations: Json?` contient les traductions par langue.
`StoryCanvasReaderView` affiche les captions traduits sur la vidéo foreground.

---

## 4. Backend — Pipeline traduction textObjects

### 4.1 Déclenchement

```typescript
// services/gateway/src/services/PostService.ts

async createPost(data: CreatePostData): Promise<Post> {
  const post = await prisma.post.create({ data: buildPostData(data) });

  // Post.content = index de recherche (concat des textObjects.text)
  const searchContent = data.storyEffects?.textObjects
    ?.map((t) => t.text).filter(Boolean).join(' ') ?? data.content ?? '';
  if (searchContent !== data.content) {
    await prisma.post.update({ where: { id: post.id }, data: { content: searchContent } });
  }

  // Pipeline traductions asynchrones
  if (data.storyEffects?.textObjects?.length) {
    this.triggerStoryTextObjectTranslation(post.id, data.storyEffects.textObjects);
  }

  return post;
}
```

### 4.2 Fonction de déclenchement (fire-and-forget)

```typescript
private triggerStoryTextObjectTranslation(
  postId: string,
  textObjects: StoryTextObjectRaw[]
): void {
  const targetLanguages = getActiveLanguageCodes(); // ex: ['en', 'es', 'de', 'pt', 'ar']

  textObjects.forEach((obj, index) => {
    if (!obj.text?.trim()) return;

    this.zmqClient.translateTextObject({
      postId,
      textObjectIndex: index,
      text: obj.text,
      sourceLanguage: obj.sourceLanguage ?? 'fr',
      targetLanguages,
    });
  });
}
```

### 4.3 ZMQ — nouveau type de message

```typescript
// services/gateway/src/services/zmq-translation/ZmqTranslationClient.ts

translateTextObject(params: {
  postId: string;
  textObjectIndex: number;
  text: string;
  sourceLanguage: string;
  targetLanguages: string[];
}): void {
  const metadata = JSON.stringify({
    type: 'story_text_object_translation',
    ...params,
  });
  this.pushSocket.send([metadata]); // pas de frames binaires
}
```

### 4.4 Réception + mise à jour DB (gateway EventEmitter)

```typescript
// Listener dans MessageSocketManager ou EventEmitter interne

on('story_text_object_translation_completed', async (data) => {
  const { postId, textObjectIndex, translations } = data;

  // Read-merge-write (Prisma Json? ne supporte pas les dot-notation $set)
  const post = await prisma.post.findUnique({
    where: { id: postId },
    select: { storyEffects: true }
  });
  if (!post?.storyEffects) return;

  const effects = post.storyEffects as Record<string, unknown>;
  const textObjects = (effects.textObjects as StoryTextObjectRaw[]) ?? [];
  if (textObjects[textObjectIndex]) {
    textObjects[textObjectIndex].translations = translations;
  }
  effects.textObjects = textObjects;

  await prisma.post.update({
    where: { id: postId },
    data: { storyEffects: effects }
  });

  // Notifier le client iOS — événement story-spécifique
  io.to(`feed:${post.authorId}`).emit('post:story-translation-updated', {
    postId,
    textObjectIndex,
    translations,
  });
});
```

### 4.5 Socket.IO — événement story-translation-updated

**Convention événements story** : seuls 3 événements story existent côté mise à jour client :
- `post:story-translation-updated` — traductions (textObjects ou content)
- `post:story-reaction-updated` — réaction reçue
- `post:story-comment-updated` — nouveau commentaire

**Nom :** `post:story-translation-updated`

**Payload :**
```typescript
{
  postId: string;
  textObjectIndex: number;
  translations: Record<string, string>;
}
```

**iOS** : `SocialSocketManager.storyEffectsUpdatedPublisher` (nouveau `PassthroughSubject`) → met à jour le cache local de la story.

---

## 5. iOS — Nouveaux composants

### 5.1 `WaveformGenerator`
- Utilise `AVAssetReader` pour lire les samples PCM bruts
- Extrait ~80 amplitudes normalisées (0.0–1.0)
- `actor` pour thread-safety
- Appelé après upload audio, résultat stocké dans `StoryAudioPlayerObject.waveformSamples`

### 5.2 `StoryAudioPlayerView`
- Composant SwiftUI draggable sur canvas (comme `StorySticker`)
- Affiche la waveform animée avec `TimelineView`
- Bouton play/pause
- Utilise `waveformSamples` pour dessiner les barres
- Placement : `x, y` normalisés → `GeometryReader` pour position absolue

### 5.3 `DraggableMediaView`
- Wrap `Image` ou `VideoPlayer` dans une vue draggable/pinchable
- `DragGesture` + `MagnificationGesture` + `RotationGesture` simultanés
- Position/scale/rotation sauvegardés dans `StoryMediaObject`

### 5.4 `MediaPlacementSheet`
- Bottom sheet SwiftUI `presentationDetents([.height(180)])`
- Deux boutons : "Arrière-plan" et "Premier plan" avec icônes
- Retourne `MediaPlacement` enum à la vue parent

### 5.5 `AudioSourceSheet`
- Bottom sheet SwiftUI `presentationDetents([.height(200)])`
- Deux boutons : "Bibliothèque" (DocumentPicker) et "Enregistrer" (micro in-app)
- Puis même `MediaPlacementSheet` pour placement

---

## 6. Décisions clés

| Décision | Choix retenu | Raison |
|----------|-------------|--------|
| Stockage translations textObjects | Embedded dans `StoryTextObject.translations` | Zéro nouveau champ Prisma, self-contained |
| Variantes langue audio | `PostMedia.language` + `variantOf` | Pattern existant pour médias |
| Post.content avec multi-texte | Concatenation textObjects pour index de recherche | Compatibilité recherche plein texte |
| ZMQ textObjects | 1 message par textObject (parallèle) | Réutilise infrastructure existante |
| Waveform samples | Stockés dans storyEffects.audioPlayerObjects | Évite recalcul côté iOS au rendu |
| Mixage volume | Valeurs Float 0.0–1.0 dans les objets | Portable, précis |
