# OpenVoice V2 Setup Guide for iOS

This guide explains how to set up OpenVoice V2 voice cloning for the Meeshy iOS app.

## Overview

OpenVoice V2 is an open-source voice cloning model by MyShell AI that enables:
- Instant voice cloning from ~6 seconds of reference audio
- Multi-language speech synthesis (EN, FR, ES, DE, ZH, JA, KO, AR, PT, IT, NL, RU)
- On-device inference via ONNX Runtime

## Prerequisites

1. **Python 3.9+** with PyTorch
2. **CocoaPods** for iOS dependencies
3. **Xcode 15+**

## Step 1: Install Python Dependencies

```bash
pip install torch torchaudio onnx huggingface_hub
```

## Step 2: Convert Models to ONNX

Run the conversion script:

```bash
cd ios/Scripts
python convert_openvoice_to_onnx.py --output_dir ../OpenVoiceModels
```

This creates:
- `OpenVoiceModels/onnx/speaker_embedding_extractor.onnx` (~15 MB)
- `OpenVoiceModels/onnx/hifigan_vocoder.onnx` (~55 MB)
- `OpenVoiceModels/onnx/model_info.json`

## Step 3: Install CocoaPods Dependencies

```bash
cd ios
pod install
```

This installs `onnxruntime-objc` for ONNX model inference on iOS.

## Step 4: Add Models to App

### Option A: Bundle with App (Recommended for testing)

1. Copy ONNX files to app bundle:
   ```bash
   cp OpenVoiceModels/onnx/*.onnx Meeshy/Resources/
   ```

2. Add files to Xcode project under Resources group

### Option B: Download at Runtime (Recommended for production)

Models will be downloaded to `Documents/OpenVoiceModels/` on first use.

Configure your model hosting URL in `OpenVoiceModelManager.swift`:
```swift
var modelBaseURL: URL = URL(string: "https://your-cdn.com/models/")!
```

## Step 5: Initialize at App Startup

In your app delegate or initial view:

```swift
import SwiftUI

@main
struct MeeshyApp: App {
    @StateObject private var openVoiceService = OpenVoiceService()
    
    var body: some Scene {
        WindowGroup {
            ContentView()
                .environmentObject(openVoiceService)
                .task {
                    do {
                        try await openVoiceService.loadModels()
                        print("OpenVoice models loaded successfully")
                    } catch {
                        print("Failed to load OpenVoice: \(error)")
                    }
                }
        }
    }
}
```

## Usage

### Extract Speaker Embedding

```swift
// From audio file
let embedding = try await openVoiceService.extractSpeakerEmbedding(
    from: audioURL,
    language: .french
)

// From audio buffer (real-time)
let embedding = try await openVoiceService.extractSpeakerEmbedding(
    from: audioBuffer,
    language: .english
)
```

### Generate Speech with Cloned Voice

```swift
let result = try await openVoiceService.generateSpeech(
    text: "Bonjour, comment allez-vous?",
    embedding: speakerEmbedding,
    language: .french
)

// Play the generated audio
let player = try AVAudioPlayer(contentsOf: result.audioURL)
player.play()
```

## Translation Preview Feature

The audio editor includes a translation preview feature that:

1. Extracts speaker embedding from recorded audio
2. Transcribes audio to text
3. Translates text to target language
4. Generates speech in target language with cloned voice

Access via the globe button (ğŸŒ) in the audio editor toolbar.

## Performance

- **Speaker Embedding Extraction**: ~85ms
- **Speech Synthesis**: ~50ms per second of output audio
- **Total Latency**: ~150-300ms end-to-end

Performance measured on iPhone 14 Pro with Neural Engine acceleration.

## Troubleshooting

### "Model not loaded" error

Ensure models are downloaded:
```swift
let manager = OpenVoiceModelManager()
if !manager.areAllModelsDownloaded {
    try await manager.downloadAllModels()
}
```

### "ONNX Runtime error"

Check that CocoaPods installed correctly:
```bash
pod deintegrate
pod install
```

### Poor voice quality

- Ensure reference audio is at least 6 seconds
- Use clean audio without background noise
- Match source language with detected language

## Model Sources

- OpenVoice V2: https://huggingface.co/myshell-ai/OpenVoiceV2
- Paper: https://arxiv.org/abs/2312.01479
- License: MIT (commercial use allowed)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Translation Preview                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Audio â†’ STT â†’ Translation â†’ Voice-Cloned TTS â†’ Audio  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                           â”‚
    â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Speaker Embedding â”‚               â”‚   HiFi-GAN        â”‚
â”‚    Extractor      â”‚               â”‚   Vocoder         â”‚
â”‚  (ONNX ~15 MB)    â”‚               â”‚  (ONNX ~55 MB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â–²
         â”‚ 256-dim embedding                  â”‚ mel spectrogram
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
