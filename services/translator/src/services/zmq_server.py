"""
Serveur ZeroMQ haute performance pour le service de traduction Meeshy
Architecture: PUB/SUB + REQ/REP avec pool de connexions et traitement asynchrone
"""

import asyncio
import json
import logging
import uuid
import zmq
import zmq.asyncio
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor
import time
import psutil
from collections import defaultdict

# Configuration du logging (must be before imports that use logger)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import du service de base de donnÃ©es
from .database_service import DatabaseService

# Import de la configuration des limites
from config.message_limits import can_translate_message, MessageLimits

# Import du pipeline audio (chargÃ© dynamiquement pour Ã©viter les imports circulaires)
AUDIO_PIPELINE_AVAILABLE = False
try:
    from .audio_message_pipeline import AudioMessagePipeline, AudioMessageMetadata, get_audio_pipeline
    AUDIO_PIPELINE_AVAILABLE = True
    logger.info("âœ… [ZMQ] AudioMessagePipeline disponible")
except ImportError as e:
    logger.warning(f"âš ï¸ [ZMQ] AudioMessagePipeline non disponible: {e}")

# Import du Voice API handler
VOICE_API_AVAILABLE = False
try:
    from .voice_api_handler import VoiceAPIHandler, get_voice_api_handler
    VOICE_API_AVAILABLE = True
    logger.info("âœ… [ZMQ] VoiceAPIHandler disponible")
except ImportError as e:
    logger.warning(f"âš ï¸ [ZMQ] VoiceAPIHandler non disponible: {e}")

# Import du Voice Profile handler (internal ZMQ processing)
VOICE_PROFILE_HANDLER_AVAILABLE = False
try:
    from .voice_profile_handler import VoiceProfileHandler, get_voice_profile_handler
    VOICE_PROFILE_HANDLER_AVAILABLE = True
    logger.info("âœ… [ZMQ] VoiceProfileHandler disponible")
except ImportError as e:
    logger.warning(f"âš ï¸ [ZMQ] VoiceProfileHandler non disponible: {e}")

# Import du service de cache Redis pour traductions
CACHE_AVAILABLE = False
try:
    from .redis_service import get_redis_service, get_translation_cache_service, TranslationCacheService
    CACHE_AVAILABLE = True
    logger.info("âœ… [ZMQ] Cache Redis disponible")
except ImportError as e:
    logger.warning(f"âš ï¸ [ZMQ] Cache Redis non disponible: {e}")

# Import des optimisations de performance
PERFORMANCE_MODULE_AVAILABLE = False
try:
    from utils.performance import Priority, PerformanceConfig
    PERFORMANCE_MODULE_AVAILABLE = True
    logger.info("âœ… [ZMQ] Module performance disponible")
except ImportError as e:
    logger.warning(f"âš ï¸ [ZMQ] Module performance non disponible: {e}")

@dataclass
class TranslationTask:
    """TÃ¢che de traduction avec support multi-langues et prioritÃ©"""
    task_id: str
    message_id: str
    text: str
    source_language: str
    target_languages: List[str]
    conversation_id: str
    model_type: str = "basic"
    created_at: float = None
    priority: int = 2  # 1=HIGH (short), 2=MEDIUM, 3=LOW (long), 4=BULK

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
        # Auto-assign priority based on text length if not set
        if PERFORMANCE_MODULE_AVAILABLE and self.priority == 2:
            text_len = len(self.text)
            if text_len < 100:
                self.priority = Priority.HIGH.value
            elif text_len < 500:
                self.priority = Priority.MEDIUM.value
            else:
                self.priority = Priority.LOW.value

class TranslationPoolManager:
    """
    Gestionnaire des pools FIFO de traduction avec gestion dynamique des workers
    
    XXX: PARALLÃ‰LISATION OPPORTUNITÃ‰ #4 - Architecture worker optimale
    TODO: Configuration actuelle:
          - normal_workers: 20 (threads sÃ©quentiels)
          - any_workers: 10 (threads sÃ©quentiels)
          - Chaque worker traite UNE tÃ¢che Ã  la fois
    TODO: Optimisations possibles:
          A) Worker hybride: asyncio + multiprocessing
             - Utiliser ProcessPoolExecutor au lieu de ThreadPoolExecutor
             - Chaque process peut traiter N tÃ¢ches en parallÃ¨le (asyncio)
             - Contourner le GIL Python pour vrai parallÃ©lisme
             
          B) Worker avec batch processing interne
             - Au lieu de prendre 1 tÃ¢che, prendre batch de 5-10 tÃ¢ches
             - Traduire toutes en batch (voir OPPORTUNITÃ‰ #2)
             - Gains: moins de setup, meilleur throughput
             
          C) Priority queue avec smart scheduling
             - Petits segments (< 50 chars): queue haute prioritÃ©
             - Grands paragraphes (> 200 chars): queue normale
             - Ã‰quilibrage charge automatique
    TODO: Configuration suggÃ©rÃ©e:
          NORMAL_WORKERS_DEFAULT=8  # Processus au lieu de threads
          WORKER_BATCH_SIZE=10       # TÃ¢ches par batch
          ENABLE_MULTIPROCESSING=true
    TODO: Gains attendus:
          - 3-5x throughput avec multiprocessing (contourne GIL)
          - 2-3x avec batch processing (moins d'overhead)
          - 10-15x avec les deux combinÃ©s
    """
    
    def __init__(self,
                 normal_pool_size: int = 10000,
                 any_pool_size: int = 10000,
                 normal_workers: int = 20,  # AugmentÃ© pour haute performance
                 any_workers: int = 10,     # AugmentÃ© pour haute performance
                 translation_service=None,
                 enable_dynamic_scaling: bool = True):

        # Pools FIFO sÃ©parÃ©es
        self.normal_pool = asyncio.Queue(maxsize=normal_pool_size)
        self.any_pool = asyncio.Queue(maxsize=any_pool_size)

        # Configuration des workers avec valeurs par dÃ©faut configurables
        import os

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # OPTIMISATION: Fast pool pour textes courts (haute prioritÃ©)
        # Les textes < 100 caractÃ¨res sont traitÃ©s en prioritÃ©
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.fast_pool = asyncio.Queue(maxsize=5000)
        self.enable_priority_queue = PERFORMANCE_MODULE_AVAILABLE and os.getenv("TRANSLATOR_PRIORITY_QUEUE", "true").lower() == "true"
        self.short_text_threshold = int(os.getenv("TRANSLATOR_SHORT_TEXT_THRESHOLD", "100"))
        
        # Valeurs par dÃ©faut configurables
        self.normal_workers_default = int(os.getenv('NORMAL_WORKERS_DEFAULT', '20'))
        self.any_workers_default = int(os.getenv('ANY_WORKERS_DEFAULT', '10'))
        
        # Limites minimales configurables
        self.normal_workers_min = int(os.getenv('NORMAL_WORKERS_MIN', '2'))
        self.any_workers_min = int(os.getenv('ANY_WORKERS_MIN', '2'))
        
        # Limites maximales configurables
        self.normal_workers_max = int(os.getenv('NORMAL_WORKERS_MAX', '40'))
        self.any_workers_max = int(os.getenv('ANY_WORKERS_MAX', '20'))
        
        # Utiliser les valeurs fournies ou les valeurs par dÃ©faut
        self.normal_workers = normal_workers if normal_workers is not None else self.normal_workers_default
        self.any_workers = any_workers if any_workers is not None else self.any_workers_default
        
        # S'assurer que les valeurs sont dans les limites
        self.normal_workers = max(self.normal_workers_min, min(self.normal_workers, self.normal_workers_max))
        self.any_workers = max(self.any_workers_min, min(self.any_workers, self.any_workers_max))
        
        # Limites max pour scaling (peuvent Ãªtre diffÃ©rentes des limites absolues)
        self.max_normal_workers = int(os.getenv('NORMAL_WORKERS_SCALING_MAX', str(self.normal_workers_max)))
        self.max_any_workers = int(os.getenv('ANY_WORKERS_SCALING_MAX', str(self.any_workers_max)))
        
        # Log de la configuration
        logger.info(f"[TRANSLATOR] ğŸ”§ Configuration workers:")
        logger.info(f"  Normal: {self.normal_workers} (min: {self.normal_workers_min}, max: {self.normal_workers_max}, scaling_max: {self.max_normal_workers})")
        logger.info(f"  Any: {self.any_workers} (min: {self.any_workers_min}, max: {self.any_workers_max}, scaling_max: {self.max_any_workers})")
        
        # Gestion dynamique
        self.enable_dynamic_scaling = enable_dynamic_scaling
        self.scaling_check_interval = 30  # VÃ©rifier toutes les 30 secondes
        self.last_scaling_check = time.time()
        
        # Thread pools pour les traductions
        self.normal_worker_pool = ThreadPoolExecutor(max_workers=self.max_normal_workers)
        self.any_worker_pool = ThreadPoolExecutor(max_workers=self.max_any_workers)
        
        # Service de traduction partagÃ©
        self.translation_service = translation_service

        # Service de cache Redis pour traductions
        self.translation_cache = None
        self.redis_service = None
        if CACHE_AVAILABLE:
            self.redis_service = get_redis_service()
            self.translation_cache = get_translation_cache_service()
            logger.info("[TRANSLATOR] âœ… Cache Redis initialisÃ© pour traductions")

        # Statistiques avancÃ©es
        self.stats = {
            'normal_pool_size': 0,
            'any_pool_size': 0,
            'normal_workers_active': 0,
            'any_workers_active': 0,
            'tasks_processed': 0,
            'tasks_failed': 0,
            'translations_completed': 0,
            'pool_full_rejections': 0,
            'avg_processing_time': 0.0,
            'queue_growth_rate': 0.0,
            'worker_utilization': 0.0,
            'dynamic_scaling_events': 0
        }
        
        # Workers actifs
        self.normal_workers_running = False
        self.any_workers_running = False
        self.normal_worker_tasks = []
        self.any_worker_tasks = []
        
        logger.info(f"[TRANSLATOR] TranslationPoolManager haute performance initialisÃ©: normal_pool({normal_pool_size}), any_pool({any_pool_size}), normal_workers({normal_workers}), any_workers({any_workers})")
        logger.info(f"[TRANSLATOR] Gestion dynamique des workers: {'activÃ©e' if enable_dynamic_scaling else 'dÃ©sactivÃ©e'}")
    
    async def enqueue_task(self, task: TranslationTask) -> bool:
        """Enfile une tÃ¢che dans la pool appropriÃ©e avec support prioritÃ©"""
        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # OPTIMISATION: Textes courts â†’ fast_pool (traitÃ©s en prioritÃ©)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.enable_priority_queue and len(task.text) < self.short_text_threshold:
                if not self.fast_pool.full():
                    await self.fast_pool.put(task)
                    logger.debug(f"âš¡ TÃ¢che {task.task_id} enfilÃ©e dans fast_pool (texte court: {len(task.text)} chars)")
                    return True
                # Si fast_pool pleine, continue vers les pools normales

            if task.conversation_id == "any":
                # Pool spÃ©ciale pour conversation "any"
                if self.any_pool.full():
                    logger.warning(f"Pool 'any' pleine, rejet de la tÃ¢che {task.task_id}")
                    self.stats['pool_full_rejections'] += 1
                    return False

                await self.any_pool.put(task)
                self.stats['any_pool_size'] = self.any_pool.qsize()
                logger.info(f"TÃ¢che {task.task_id} enfilÃ©e dans pool 'any' (taille: {self.stats['any_pool_size']})")
            else:
                # Pool normale pour autres conversations
                if self.normal_pool.full():
                    logger.warning(f"Pool normale pleine, rejet de la tÃ¢che {task.task_id}")
                    self.stats['pool_full_rejections'] += 1
                    return False

                await self.normal_pool.put(task)
                self.stats['normal_pool_size'] = self.normal_pool.qsize()
                logger.info(f"TÃ¢che {task.task_id} enfilÃ©e dans pool normale (taille: {self.stats['normal_pool_size']})")

            return True

        except Exception as e:
            logger.error(f"Erreur lors de l'enfilage de la tÃ¢che {task.task_id}: {e}")
            return False
    
    async def start_workers(self):
        """DÃ©marre tous les workers avec gestion dynamique"""
        logger.info(f"[TRANSLATOR] ğŸ”„ DÃ©but du dÃ©marrage des workers...")
        self.normal_workers_running = True
        self.any_workers_running = True
        
        logger.info(f"[TRANSLATOR] ğŸ”„ CrÃ©ation des workers normaux ({self.normal_workers})...")
        # DÃ©marrer les workers pour la pool normale
        self.normal_worker_tasks = [
            asyncio.create_task(self._normal_worker_loop(f"normal_worker_{i}"))
            for i in range(self.normal_workers)
        ]
        logger.info(f"[TRANSLATOR] âœ… Workers normaux crÃ©Ã©s: {len(self.normal_worker_tasks)}")
        
        logger.info(f"[TRANSLATOR] ğŸ”„ CrÃ©ation des workers 'any' ({self.any_workers})...")
        # DÃ©marrer les workers pour la pool "any"
        self.any_worker_tasks = [
            asyncio.create_task(self._any_worker_loop(f"any_worker_{i}"))
            for i in range(self.any_workers)
        ]
        logger.info(f"[TRANSLATOR] âœ… Workers 'any' crÃ©Ã©s: {len(self.any_worker_tasks)}")
        
        logger.info(f"[TRANSLATOR] Workers haute performance dÃ©marrÃ©s: {self.normal_workers} normal, {self.any_workers} any")
        logger.info(f"[TRANSLATOR] CapacitÃ© totale: {self.normal_workers + self.any_workers} traductions simultanÃ©es")
        return self.normal_worker_tasks + self.any_worker_tasks
    
    async def stop_workers(self):
        """ArrÃªte tous les workers"""
        self.normal_workers_running = False
        self.any_workers_running = False
        logger.info("ArrÃªt des workers demandÃ©")
    
    async def _dynamic_scaling_check(self):
        """VÃ©rifie et ajuste dynamiquement le nombre de workers"""
        if not self.enable_dynamic_scaling:
            return
            
        current_time = time.time()
        if current_time - self.last_scaling_check < self.scaling_check_interval:
            return
            
        self.last_scaling_check = current_time
        
        # Calculer les mÃ©triques
        normal_queue_size = self.normal_pool.qsize()
        any_queue_size = self.any_pool.qsize()
        normal_utilization = self.stats['normal_workers_active'] / self.normal_workers if self.normal_workers > 0 else 0
        any_utilization = self.stats['any_workers_active'] / self.any_workers if self.any_workers > 0 else 0
        
        # Ajuster les workers normaux
        if normal_queue_size > 100 and normal_utilization > 0.8 and self.normal_workers < self.max_normal_workers:
            new_normal_workers = min(self.normal_workers + 5, self.max_normal_workers)
            if new_normal_workers > self.normal_workers:
                logger.info(f"[TRANSLATOR] ğŸ”§ Scaling UP normal workers: {self.normal_workers} â†’ {new_normal_workers}")
                await self._scale_normal_workers(new_normal_workers)
        
        elif normal_queue_size < 10 and normal_utilization < 0.3 and self.normal_workers > self.normal_workers_min:
            new_normal_workers = max(self.normal_workers - 2, self.normal_workers_min)
            if new_normal_workers < self.normal_workers:
                logger.info(f"[TRANSLATOR] ğŸ”§ Scaling DOWN normal workers: {self.normal_workers} â†’ {new_normal_workers}")
                await self._scale_normal_workers(new_normal_workers)
        
        # Ajuster les workers "any"
        if any_queue_size > 50 and any_utilization > 0.8 and self.any_workers < self.max_any_workers:
            new_any_workers = min(self.any_workers + 3, self.max_any_workers)
            if new_any_workers > self.any_workers:
                logger.info(f"[TRANSLATOR] ğŸ”§ Scaling UP any workers: {self.any_workers} â†’ {new_any_workers}")
                await self._scale_any_workers(new_any_workers)
        
        elif any_queue_size < 5 and any_utilization < 0.3 and self.any_workers > self.any_workers_min:
            new_any_workers = max(self.any_workers - 1, self.any_workers_min)
            if new_any_workers < self.any_workers:
                logger.info(f"[TRANSLATOR] ğŸ”§ Scaling DOWN any workers: {self.any_workers} â†’ {new_any_workers}")
                await self._scale_any_workers(new_any_workers)
    
    async def _scale_normal_workers(self, new_count: int):
        """Ajuste le nombre de workers normaux"""
        if new_count > self.normal_workers:
            # Ajouter des workers
            for i in range(self.normal_workers, new_count):
                task = asyncio.create_task(self._normal_worker_loop(f"normal_worker_{i}"))
                self.normal_worker_tasks.append(task)
        else:
            # RÃ©duire les workers (ils s'arrÃªteront naturellement)
            pass
        
        self.normal_workers = new_count
        self.stats['dynamic_scaling_events'] += 1
    
    async def _scale_any_workers(self, new_count: int):
        """Ajuste le nombre de workers any"""
        if new_count > self.any_workers:
            # Ajouter des workers
            for i in range(self.any_workers, new_count):
                task = asyncio.create_task(self._any_worker_loop(f"any_worker_{i}"))
                self.any_worker_tasks.append(task)
        else:
            # RÃ©duire les workers (ils s'arrÃªteront naturellement)
            pass
        
        self.any_workers = new_count
        self.stats['dynamic_scaling_events'] += 1
    
    async def _normal_worker_loop(self, worker_name: str):
        """Boucle de travail pour les workers de la pool normale avec scaling dynamique"""
        logger.info(f"Worker {worker_name} dÃ©marrÃ©")

        while self.normal_workers_running:
            try:
                # VÃ©rifier le scaling dynamique
                await self._dynamic_scaling_check()

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # OPTIMISATION: VÃ©rifier fast_pool d'abord (textes courts prioritaires)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                task = None

                if self.enable_priority_queue and not self.fast_pool.empty():
                    try:
                        task = self.fast_pool.get_nowait()
                        logger.debug(f"âš¡ Worker {worker_name} traite tÃ¢che fast_pool")
                    except asyncio.QueueEmpty:
                        pass

                # Si pas de tÃ¢che fast, attendre la pool normale
                if task is None:
                    try:
                        task = await asyncio.wait_for(self.normal_pool.get(), timeout=1.0)
                    except asyncio.TimeoutError:
                        continue

                self.stats['normal_workers_active'] += 1
                self.stats['normal_pool_size'] = self.normal_pool.qsize()
                
                logger.debug(f"Worker {worker_name} traite la tÃ¢che {task.task_id} ({len(task.target_languages)} langues)")
                
                # Traiter la tÃ¢che
                start_time = time.time()
                await self._process_translation_task(task, worker_name)
                processing_time = time.time() - start_time
                
                # Mettre Ã  jour les stats de performance
                self.stats['avg_processing_time'] = (
                    (self.stats['avg_processing_time'] * (self.stats['tasks_processed']) + processing_time) 
                    / (self.stats['tasks_processed'] + 1)
                )
                
                self.stats['normal_workers_active'] -= 1
                self.stats['tasks_processed'] += 1
                
            except Exception as e:
                logger.error(f"Erreur dans le worker {worker_name}: {e}")
                self.stats['tasks_failed'] += 1
                if self.stats['normal_workers_active'] > 0:
                    self.stats['normal_workers_active'] -= 1
        
        logger.info(f"Worker {worker_name} arrÃªtÃ©")
    
    async def _any_worker_loop(self, worker_name: str):
        """Boucle de travail pour les workers de la pool 'any' avec scaling dynamique"""
        logger.info(f"Worker {worker_name} dÃ©marrÃ©")

        while self.any_workers_running:
            try:
                # VÃ©rifier le scaling dynamique
                await self._dynamic_scaling_check()

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # OPTIMISATION: VÃ©rifier fast_pool d'abord (textes courts prioritaires)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                task = None

                if self.enable_priority_queue and not self.fast_pool.empty():
                    try:
                        task = self.fast_pool.get_nowait()
                        logger.debug(f"âš¡ Worker {worker_name} traite tÃ¢che fast_pool")
                    except asyncio.QueueEmpty:
                        pass

                # Si pas de tÃ¢che fast, attendre la pool "any"
                if task is None:
                    try:
                        task = await asyncio.wait_for(self.any_pool.get(), timeout=1.0)
                    except asyncio.TimeoutError:
                        continue

                self.stats['any_workers_active'] += 1
                self.stats['any_pool_size'] = self.any_pool.qsize()
                
                logger.debug(f"Worker {worker_name} traite la tÃ¢che {task.task_id} ({len(task.target_languages)} langues)")
                
                # Traiter la tÃ¢che
                start_time = time.time()
                await self._process_translation_task(task, worker_name)
                processing_time = time.time() - start_time
                
                # Mettre Ã  jour les stats de performance
                self.stats['avg_processing_time'] = (
                    (self.stats['avg_processing_time'] * (self.stats['tasks_processed']) + processing_time) 
                    / (self.stats['tasks_processed'] + 1)
                )
                
                self.stats['any_workers_active'] -= 1
                self.stats['tasks_processed'] += 1
                
            except Exception as e:
                logger.error(f"Erreur dans le worker {worker_name}: {e}")
                self.stats['tasks_failed'] += 1
                if self.stats['any_workers_active'] > 0:
                    self.stats['any_workers_active'] -= 1
        
        logger.info(f"Worker {worker_name} arrÃªtÃ©")
    
    async def _process_translation_task(self, task: TranslationTask, worker_name: str):
        """Traite une tÃ¢che de traduction avec traduction parallÃ¨le"""
        try:
            # Lancer les traductions en parallÃ¨le
            translation_tasks = []
            
            for target_language in task.target_languages:
                translation_task = asyncio.create_task(
                    self._translate_single_language(task, target_language, worker_name)
                )
                translation_tasks.append((target_language, translation_task))
            
            # Attendre toutes les traductions
            for target_language, translation_task in translation_tasks:
                try:
                    result = await translation_task
                    # Ajouter le type de pool au rÃ©sultat
                    result['poolType'] = 'any' if task.conversation_id == 'any' else 'normal'
                    result['created_at'] = task.created_at
                    # Publier le rÃ©sultat via PUB
                    await self._publish_translation_result(task.task_id, result, target_language)
                    self.stats['translations_completed'] += 1
                    
                except Exception as e:
                    logger.error(f"Erreur de traduction pour {target_language} dans {task.task_id}: {e}")
                    # Publier un rÃ©sultat d'erreur
                    error_result = self._create_error_result(task, target_language, str(e))
                    await self._publish_translation_result(task.task_id, error_result, target_language)
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement de la tÃ¢che {task.task_id}: {e}")
            self.stats['tasks_failed'] += 1
    
    async def _translate_single_language(self, task: TranslationTask, target_language: str, worker_name: str):
        """Traduit un texte vers une langue cible spÃ©cifique (avec cache Redis)"""
        start_time = time.time()

        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Ã‰TAPE 1: VÃ©rifier le cache (basÃ© sur hash du texte)
            # Le hash change automatiquement si le texte est modifiÃ©
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.translation_cache:
                cached = await self.translation_cache.get_translation(
                    text=task.text,
                    source_lang=task.source_language,
                    target_lang=target_language,
                    model_type=task.model_type
                )

                if cached:
                    processing_time = time.time() - start_time
                    logger.debug(f"âš¡ [CACHE] Hit traduction: {task.source_language}â†’{target_language} (msg={task.message_id})")

                    return {
                        'messageId': task.message_id,
                        'translatedText': cached.get('translated_text', ''),
                        'sourceLanguage': cached.get('source_lang', task.source_language),
                        'targetLanguage': target_language,
                        'confidenceScore': 0.99,  # Cache = haute confiance
                        'processingTime': processing_time,
                        'modelType': cached.get('model_type', task.model_type),
                        'workerName': worker_name,
                        'fromCache': True,
                        'segmentsCount': 0,
                        'emojisCount': 0
                    }

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Ã‰TAPE 2: Traduire si pas en cache
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.translation_service:
                # Effectuer la vraie traduction avec prÃ©servation de structure
                result = await self.translation_service.translate_with_structure(
                    text=task.text,
                    source_language=task.source_language,
                    target_language=target_language,
                    model_type=task.model_type,
                    source_channel='zmq'
                )

                processing_time = time.time() - start_time

                # VÃ©rifier si le rÃ©sultat est None ou invalide
                if result is None:
                    logger.error(f"âŒ [TRANSLATOR] Service ML a retournÃ© None pour {worker_name}")
                    raise Exception("Service de traduction a retournÃ© None")

                # VÃ©rifier que le rÃ©sultat contient les clÃ©s attendues
                if not isinstance(result, dict) or 'translated_text' not in result:
                    logger.error(f"âŒ [TRANSLATOR] RÃ©sultat invalide pour {worker_name}: {result}")
                    raise Exception(f"RÃ©sultat de traduction invalide: {result}")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # Ã‰TAPE 3: Mettre en cache la nouvelle traduction (TTL 1 mois)
                # Le hash du texte sert de clÃ© - rÃ©utilisable cross-message
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                if self.translation_cache:
                    await self.translation_cache.set_translation(
                        text=task.text,
                        source_lang=task.source_language,
                        target_lang=target_language,
                        translated_text=result['translated_text'],
                        model_type=task.model_type
                    )

                return {
                    'messageId': task.message_id,
                    'translatedText': result['translated_text'],
                    'sourceLanguage': result.get('detected_language', task.source_language),
                    'targetLanguage': target_language,
                    'confidenceScore': result.get('confidence', 0.95),
                    'processingTime': processing_time,
                    'modelType': task.model_type,
                    'workerName': worker_name,
                    'fromCache': False,
                    # MÃ©triques de prÃ©servation de structure
                    'segmentsCount': result.get('segments_count', 0),
                    'emojisCount': result.get('emojis_count', 0)
                }
            else:
                # Fallback si pas de service de traduction
                translated_text = f"[{target_language.upper()}] {task.text}"
                processing_time = time.time() - start_time
                
                return {
                    'messageId': task.message_id,
                    'translatedText': translated_text,
                    'sourceLanguage': task.source_language,
                    'targetLanguage': target_language,
                    'confidenceScore': 0.1,
                    'processingTime': processing_time,
                    'modelType': 'fallback',
                    'workerName': worker_name,
                    'error': 'No translation service available'
                }
            
        except Exception as e:
            logger.error(f"Erreur de traduction dans {worker_name}: {e}")
            # Fallback en cas d'erreur
            translated_text = f"[{target_language.upper()}] {task.text}"
            processing_time = time.time() - start_time
            
            return {
                'messageId': task.message_id,
                'translatedText': translated_text,
                'sourceLanguage': task.source_language,
                'targetLanguage': target_language,
                'confidenceScore': 0.1,
                'processingTime': processing_time,
                'modelType': 'fallback',
                'workerName': worker_name,
                'error': str(e)
            }
    
    def _create_error_result(self, task: TranslationTask, target_language: str, error_message: str):
        """CrÃ©e un rÃ©sultat d'erreur pour une traduction Ã©chouÃ©e"""
        return {
            'messageId': task.message_id,
            'translatedText': f"[ERREUR: {error_message}]",
            'sourceLanguage': task.source_language,
            'targetLanguage': target_language,
            'confidenceScore': 0.0,
            'processingTime': 0.0,
            'modelType': task.model_type,
            'error': error_message
        }
    
    async def _publish_translation_result(self, task_id: str, result: dict, target_language: str):
        """Publie un rÃ©sultat de traduction via PUB"""
        try:
            # Cette mÃ©thode sera appelÃ©e par le serveur ZMQ principal
            # Le rÃ©sultat sera publiÃ© via le socket PUB
            # Note: Cette mÃ©thode sera remplacÃ©e par le serveur ZMQ principal
            pass
        except Exception as e:
            logger.error(f"Erreur lors de la publication du rÃ©sultat {task_id}: {e}")
    
    def get_stats(self) -> dict:
        """Retourne les statistiques actuelles"""
        return {
            **self.stats,
            'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
            'uptime_seconds': time.time() - getattr(self, '_start_time', time.time())
        }

class ZMQTranslationServer:
    """
    Serveur ZMQ pour la traduction avec architecture PUB/SUB

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ARCHITECTURE: SÃ‰PARATION DES RESPONSABILITÃ‰S
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    TRANSLATOR (ce service):
      âœ… Traduit les textes (modÃ¨les ML NLLB)
      âœ… Transcrit les audios (Whisper)
      âœ… Clone les voix et gÃ©nÃ¨re TTS
      âœ… Cache les rÃ©sultats dans Redis (TTL 1 mois)
      âœ… Renvoie les rÃ©sultats Ã  Gateway via ZMQ PUB
      âŒ NE SAUVEGARDE PAS en base de donnÃ©es (sauf profils vocaux)

    GATEWAY:
      âœ… ReÃ§oit les rÃ©sultats via ZMQ SUB
      âœ… Persiste en base de donnÃ©es (MongoDB/Prisma)
      âœ… GÃ¨re l'encryption des traductions si nÃ©cessaire
      âœ… ContrÃ´le la logique mÃ©tier de persistance

    Avantages:
      - Translator peut fonctionner sans base de donnÃ©es
      - Meilleure scalabilitÃ© (Translator stateless)
      - Gateway contrÃ´le la logique de persistance
      - SÃ©paration claire des responsabilitÃ©s
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """

    def __init__(self,
                 host: str = "0.0.0.0",
                 gateway_push_port: int = 5555,  # Port oÃ¹ Translator PULL bind (Gateway PUSH connect ici)
                 gateway_sub_port: int = 5558,   # Port oÃ¹ Translator PUB bind (Gateway SUB connect ici)
                 normal_pool_size: int = 10000,
                 any_pool_size: int = 10000,
                 normal_workers: int = 3,
                 any_workers: int = 2,
                 translation_service=None,
                 database_url: str = None):
        
        self.host = host
        self.gateway_push_port = gateway_push_port  # Port pour PULL (recevoir commandes)
        self.gateway_sub_port = gateway_sub_port    # Port pour PUB (envoyer rÃ©ponses)
        self.context = zmq.asyncio.Context()
        
        # Sockets
        self.pull_socket = None  # PULL pour recevoir les commandes de traduction
        self.pub_socket = None   # PUB pour publier les rÃ©sultats (inchangÃ©)
        
        # Pool manager
        self.pool_manager = TranslationPoolManager(
            normal_pool_size=normal_pool_size,
            any_pool_size=any_pool_size,
            normal_workers=normal_workers,
            any_workers=any_workers,
            translation_service=translation_service
        )
        
        # Remplacer la mÃ©thode de publication du pool manager
        self.pool_manager._publish_translation_result = self._publish_translation_result
        
        # Service de base de donnÃ©es
        self.database_service = DatabaseService(database_url)

        # Voice API handler
        self.voice_api_handler = None
        if VOICE_API_AVAILABLE:
            self.voice_api_handler = get_voice_api_handler()
            logger.info("âœ… [ZMQ] VoiceAPIHandler initialisÃ©")

        # Voice Profile handler (internal ZMQ processing for Gateway)
        self.voice_profile_handler = None
        if VOICE_PROFILE_HANDLER_AVAILABLE:
            self.voice_profile_handler = get_voice_profile_handler()
            logger.info("âœ… [ZMQ] VoiceProfileHandler initialisÃ©")

        # Ã‰tat du serveur
        self.running = False
        self.worker_tasks = []

        logger.info(f"ZMQTranslationServer initialisÃ©: Gateway PUSH {host}:{gateway_push_port} (PULL bind)")
        logger.info(f"ZMQTranslationServer initialisÃ©: Gateway SUB {host}:{gateway_sub_port} (PUB bind)")

    async def _connect_database_background(self):
        """Connecte Ã  la base de donnÃ©es en arriÃ¨re-plan sans bloquer le dÃ©marrage"""
        try:
            logger.info("[TRANSLATOR-DB] ğŸ”— Tentative de connexion Ã  MongoDB...")
            db_connected = await self.database_service.connect()
            if db_connected:
                logger.info("[TRANSLATOR-DB] âœ… Connexion Ã  la base de donnÃ©es Ã©tablie")
            else:
                logger.warning("[TRANSLATOR-DB] âš ï¸ Connexion Ã  la base de donnÃ©es Ã©chouÃ©e, sauvegarde dÃ©sactivÃ©e")
        except Exception as e:
            logger.error(f"[TRANSLATOR-DB] âŒ Erreur lors de la connexion Ã  la base de donnÃ©es: {e}")
    
    async def initialize(self):
        """Initialise les sockets ZMQ avec architecture PUSH/PULL + PUB/SUB"""
        try:
            # Connexion Ã  la base de donnÃ©es en arriÃ¨re-plan (non-bloquante)
            logger.info("[TRANSLATOR] ğŸ”— Lancement de la connexion Ã  la base de donnÃ©es en arriÃ¨re-plan...")
            # CrÃ©er une tÃ¢che asynchrone pour la connexion DB sans bloquer
            asyncio.create_task(self._connect_database_background())
            logger.info("[TRANSLATOR] âœ… Connexion DB lancÃ©e en arriÃ¨re-plan, le serveur continue son dÃ©marrage...")
            
            # Socket PULL pour recevoir les commandes du Gateway (remplace SUB)
            self.pull_socket = self.context.socket(zmq.PULL)
            self.pull_socket.bind(f"tcp://{self.host}:{self.gateway_push_port}")
            
            # Socket PUB pour publier les rÃ©sultats vers le Gateway (inchangÃ©)
            self.pub_socket = self.context.socket(zmq.PUB)
            self.pub_socket.bind(f"tcp://{self.host}:{self.gateway_sub_port}")
            
            # Petit dÃ©lai pour Ã©tablir les connexions ZMQ
            await asyncio.sleep(0.1)
            logger.info("[TRANSLATOR] âœ… Sockets ZMQ crÃ©Ã©s, dÃ©marrage des workers...")
            
            # DÃ©marrer les workers
            self.worker_tasks = await self.pool_manager.start_workers()
            logger.info(f"[TRANSLATOR] âœ… Workers dÃ©marrÃ©s: {len(self.worker_tasks)} tÃ¢ches")
            
            logger.info("ZMQTranslationServer initialisÃ© avec succÃ¨s")
            logger.info(f"ğŸ”Œ Socket PULL liÃ© au port: {self.host}:{self.gateway_push_port}")
            logger.info(f"ğŸ”Œ Socket PUB liÃ© au port: {self.host}:{self.gateway_sub_port}")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation: {e}")
            raise
    
    async def start(self):
        """DÃ©marre le serveur"""
        if not self.pull_socket or not self.pub_socket:
            await self.initialize()
        
        self.running = True
        logger.info("ZMQTranslationServer dÃ©marrÃ©")
        
        try:
            while self.running:
                try:
                    # Recevoir une commande de traduction via PULL
                    message = await self.pull_socket.recv()
                    await self._handle_translation_request(message)
                    
                except zmq.ZMQError as e:
                    if self.running:
                        logger.error(f"Erreur ZMQ: {e}")
                    break
                except Exception as e:
                    logger.error(f"Erreur inattendue: {e}")
                    import traceback
                    traceback.print_exc()
                    
        except KeyboardInterrupt:
            logger.info("ArrÃªt demandÃ© par l'utilisateur")
        finally:
            await self.stop()
    
    async def _handle_translation_request(self, message: bytes):
        """
        Traite une requÃªte de traduction reÃ§ue via SUB
        
        XXX: PARALLÃ‰LISATION OPPORTUNITÃ‰ #3 - Traduction multi-langues simultanÃ©e
        TODO: Actuellement, si targetLanguages = ['en', 'es', 'de', 'it', 'pt']
              chaque langue est traduite SÃ‰QUENTIELLEMENT par le worker
        TODO: Optimisation possible:
              - CrÃ©er UNE tÃ¢che par langue cible (5 tÃ¢ches au lieu d'1)
              - Les workers traitent en parallÃ¨le (si plusieurs workers disponibles)
              - OU: Batch translation dans le worker (traduire toutes les langues en 1 passe)
        TODO: ImplÃ©mentation suggÃ©rÃ©e:
              # Option A: Multiple tasks (simple, utilise workers existants)
              for target_lang in target_languages:
                  task = TranslationTask(
                      target_languages=[target_lang],  # UNE langue par tÃ¢che
                      ...
                  )
                  await self.pool_manager.enqueue_task(task)
              
              # Option B: Batch API dans ML service (plus efficace)
              results = await ml_service.translate_batch_multilingual(
                  text=text,
                  source_lang=source_lang,
                  target_langs=['en', 'es', 'de', 'it', 'pt'],  # Toutes ensemble
                  model_type=model_type
              )
        TODO: Gains attendus:
              - Option A: N workers Ã— vitesse (si N workers disponibles)
              - Option B: 2-3x plus rapide (overhead rÃ©duit, batch processing)
        """
        try:
            request_data = json.loads(message.decode('utf-8'))
            
            # Dispatcher selon le type de message
            message_type = request_data.get('type')

            # === PING ===
            if message_type == 'ping':
                logger.info(f"ğŸ“ [TRANSLATOR] Ping reÃ§u, timestamp: {request_data.get('timestamp')}")
                # RÃ©pondre au ping via PUB
                ping_response = {
                    'type': 'pong',
                    'timestamp': time.time(),
                    'translator_status': 'alive',
                    'translator_port_pub': self.gateway_sub_port,
                    'translator_port_pull': self.gateway_push_port,
                    'audio_pipeline_available': AUDIO_PIPELINE_AVAILABLE
                }
                if self.pub_socket:
                    await self.pub_socket.send(json.dumps(ping_response).encode('utf-8'))
                    logger.info(f"ğŸ“ [TRANSLATOR] Pong envoyÃ© via port {self.gateway_sub_port}")
                else:
                    logger.error(f"âŒ [TRANSLATOR] Socket PUB non disponible pour pong (port {self.gateway_sub_port})")
                return

            # === AUDIO PROCESSING ===
            if message_type == 'audio_process':
                await self._handle_audio_process_request(request_data)
                return

            # === VOICE API ===
            if VOICE_API_AVAILABLE and self.voice_api_handler and self.voice_api_handler.is_voice_api_request(message_type):
                await self._handle_voice_api_request(request_data)
                return

            # === VOICE PROFILE (internal processing for Gateway) ===
            if VOICE_PROFILE_HANDLER_AVAILABLE and self.voice_profile_handler and self.voice_profile_handler.is_voice_profile_request(message_type):
                await self._handle_voice_profile_request(request_data)
                return

            # VÃ©rifier que c'est une requÃªte de traduction valide
            if not request_data.get('text') or not request_data.get('targetLanguages'):
                logger.warning(f"âš ï¸ [TRANSLATOR] RequÃªte invalide reÃ§ue: {request_data}")
                return
            
            # VÃ©rifier la longueur du message pour la traduction
            message_text = request_data.get('text', '')
            if not can_translate_message(message_text):
                logger.warning(f"âš ï¸ [TRANSLATOR] Message too long to be translated: {len(message_text)} caractÃ¨res (max: {MessageLimits.MAX_TRANSLATION_LENGTH})")
                # Ne pas traiter ce message, retourner un rÃ©sultat vide ou le texte original
                # On pourrait aussi envoyer une notification Ã  la gateway ici si nÃ©cessaire
                no_translation_message = {
                    'type': 'translation_skipped',
                    'messageId': request_data.get('messageId'),
                    'reason': 'message_too_long',
                    'length': len(message_text),
                    'max_length': MessageLimits.MAX_TRANSLATION_LENGTH,
                    'conversationId': request_data.get('conversationId', 'unknown')
                }
                if self.pub_socket:
                    await self.pub_socket.send(json.dumps(no_translation_message).encode('utf-8'))
                    logger.info(f"[TRANSLATOR] translation message ignored for message {request_data.get('messageId')}")
                return
            
            # CrÃ©er la tÃ¢che de traduction
            task = TranslationTask(
                task_id=str(uuid.uuid4()),
                message_id=request_data.get('messageId'),
                text=message_text,
                source_language=request_data.get('sourceLanguage', 'fr'),
                target_languages=request_data.get('targetLanguages', []),
                conversation_id=request_data.get('conversationId', 'unknown'),
                model_type=request_data.get('modelType', 'basic')
            )
            
            logger.info(f"ğŸ”§ [TRANSLATOR] TÃ¢che crÃ©Ã©e: {task.task_id} pour {task.conversation_id} ({len(task.target_languages)} langues)")
            logger.info(f"ğŸ“ [TRANSLATOR] DÃ©tails: texte='{task.text[:50]}...', source={task.source_language}, target={task.target_languages}, modÃ¨le={task.model_type}")
            
            # Enfiler la tÃ¢che dans la pool appropriÃ©e
            success = await self.pool_manager.enqueue_task(task)
            
            if not success:
                # Pool pleine, publier un message d'erreur vers la gateway
                error_message = {
                    'type': 'translation_error',
                    'taskId': task.task_id,
                    'messageId': task.message_id,
                    'error': 'translation pool full',
                    'conversationId': task.conversation_id
                }
                # Utiliser le socket PUB configurÃ© pour envoyer l'erreur Ã  la gateway
                if self.pub_socket:
                    await self.pub_socket.send(json.dumps(error_message).encode('utf-8'))
                    logger.warning(f"Pool pleine, rejet de la tÃ¢che {task.task_id}")
                else:
                    logger.error("âŒ Socket PUB non initialisÃ© pour envoyer l'erreur")
            
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de dÃ©codage JSON: {e}")
        except Exception as e:
            logger.error(f"Erreur lors du traitement de la requÃªte: {e}")

    async def _handle_audio_process_request(self, request_data: dict):
        """
        Traite une requÃªte de processing audio.

        Pipeline complet:
        1. Transcription (mobile ou Whisper)
        2. Traduction vers les langues cibles
        3. Clonage vocal
        4. GÃ©nÃ©ration TTS

        Format attendu:
        {
            "type": "audio_process",
            "messageId": str,
            "attachmentId": str,
            "conversationId": str,
            "senderId": str,
            "audioUrl": str,
            "audioPath": str,
            "audioDurationMs": int,
            "mobileTranscription": {
                "text": str,
                "language": str,
                "confidence": float,
                "source": str
            },
            "targetLanguages": [str],
            "generateVoiceClone": bool,
            "modelType": str
        }
        """
        task_id = str(uuid.uuid4())
        start_time = time.time()

        logger.info(f"ğŸ¤ [TRANSLATOR] Audio process request reÃ§u: {request_data.get('messageId')}")

        if not AUDIO_PIPELINE_AVAILABLE:
            logger.error("[TRANSLATOR] Audio pipeline non disponible")
            await self._publish_audio_error(
                task_id=task_id,
                message_id=request_data.get('messageId', ''),
                attachment_id=request_data.get('attachmentId', ''),
                error="Audio pipeline not available",
                error_code="pipeline_unavailable"
            )
            return

        try:
            # Valider les donnÃ©es requises
            required_fields = ['messageId', 'attachmentId', 'audioPath', 'senderId']
            for field in required_fields:
                if not request_data.get(field):
                    raise ValueError(f"Champ requis manquant: {field}")

            # PrÃ©parer les mÃ©tadonnÃ©es mobiles
            metadata = None
            mobile_trans = request_data.get('mobileTranscription')
            if mobile_trans and mobile_trans.get('text'):
                metadata = AudioMessageMetadata(
                    transcription=mobile_trans.get('text'),
                    language=mobile_trans.get('language'),
                    confidence=mobile_trans.get('confidence'),
                    source=mobile_trans.get('source'),
                    segments=mobile_trans.get('segments')
                )

            # Obtenir le pipeline et l'initialiser
            pipeline = get_audio_pipeline()

            # Injecter les services si pas encore fait
            if pipeline.translation_service is None and hasattr(self, 'pool_manager') and self.pool_manager.translation_service:
                pipeline.set_translation_service(self.pool_manager.translation_service)

            if pipeline.database_service is None and hasattr(self, 'database_service'):
                pipeline.set_database_service(self.database_service)

            # ExÃ©cuter le pipeline audio
            result = await pipeline.process_audio_message(
                audio_path=request_data.get('audioPath'),
                audio_url=request_data.get('audioUrl', ''),
                sender_id=request_data.get('senderId'),
                conversation_id=request_data.get('conversationId', ''),
                message_id=request_data.get('messageId'),
                attachment_id=request_data.get('attachmentId'),
                audio_duration_ms=request_data.get('audioDurationMs', 0),
                metadata=metadata,
                target_languages=request_data.get('targetLanguages'),
                generate_voice_clone=request_data.get('generateVoiceClone', True),
                model_type=request_data.get('modelType', 'medium'),
                # Voice profile options (pour messages transfÃ©rÃ©s - voix de l'Ã©metteur original)
                original_sender_id=request_data.get('originalSenderId'),
                existing_voice_profile=request_data.get('existingVoiceProfile'),
                use_original_voice=request_data.get('useOriginalVoice', True)
            )

            processing_time = int((time.time() - start_time) * 1000)

            # Publier le rÃ©sultat
            await self._publish_audio_result(task_id, result, processing_time)

            logger.info(
                f"âœ… [TRANSLATOR] Audio process terminÃ©: "
                f"msg={result.message_id}, "
                f"translations={len(result.translations)}, "
                f"time={processing_time}ms"
            )

        except Exception as e:
            logger.error(f"âŒ [TRANSLATOR] Erreur audio process: {e}")
            import traceback
            traceback.print_exc()

            await self._publish_audio_error(
                task_id=task_id,
                message_id=request_data.get('messageId', ''),
                attachment_id=request_data.get('attachmentId', ''),
                error=str(e),
                error_code="processing_failed"
            )

    async def _publish_audio_result(self, task_id: str, result, processing_time: int):
        """Publie le rÃ©sultat du processing audio via PUB"""
        try:
            # Construire le message de rÃ©sultat
            message = {
                'type': 'audio_process_completed',
                'taskId': task_id,
                'messageId': result.message_id,
                'attachmentId': result.attachment_id,
                'transcription': {
                    'text': result.original.transcription,
                    'language': result.original.language,
                    'confidence': result.original.confidence,
                    'source': result.original.source,
                    'segments': result.original.segments
                },
                'translatedAudios': [
                    {
                        'targetLanguage': t.language,
                        'translatedText': t.translated_text,
                        'audioUrl': t.audio_url,
                        'audioPath': t.audio_path,
                        'durationMs': t.duration_ms,
                        'voiceCloned': t.voice_cloned,
                        'voiceQuality': t.voice_quality
                    }
                    for t in result.translations.values()
                ],
                'voiceModelUserId': result.voice_model_user_id,
                'voiceModelQuality': result.voice_model_quality,
                'processingTimeMs': processing_time,
                'timestamp': time.time()
            }

            # Inclure le nouveau profil vocal si crÃ©Ã© par Translator
            # Gateway doit le sauvegarder dans MongoDB pour rÃ©utilisation future
            if hasattr(result, 'new_voice_profile') and result.new_voice_profile:
                nvp = result.new_voice_profile
                message['newVoiceProfile'] = {
                    'userId': nvp.user_id,
                    'profileId': nvp.profile_id,
                    'embedding': nvp.embedding_base64,
                    'qualityScore': nvp.quality_score,
                    'audioCount': nvp.audio_count,
                    'totalDurationMs': nvp.total_duration_ms,
                    'version': nvp.version,
                    'fingerprint': nvp.fingerprint,
                    'voiceCharacteristics': nvp.voice_characteristics
                }
                logger.info(f"ğŸ“¦ [TRANSLATOR] Nouveau profil vocal inclus pour Gateway: {nvp.user_id}")

            if self.pub_socket:
                await self.pub_socket.send(json.dumps(message).encode('utf-8'))
                logger.info(f"âœ… [TRANSLATOR] Audio result publiÃ©: {result.message_id}")
            else:
                logger.error("âŒ [TRANSLATOR] Socket PUB non disponible pour audio result")

        except Exception as e:
            logger.error(f"âŒ [TRANSLATOR] Erreur publication audio result: {e}")

    async def _publish_audio_error(
        self,
        task_id: str,
        message_id: str,
        attachment_id: str,
        error: str,
        error_code: str
    ):
        """Publie une erreur de processing audio via PUB"""
        try:
            message = {
                'type': 'audio_process_error',
                'taskId': task_id,
                'messageId': message_id,
                'attachmentId': attachment_id,
                'error': error,
                'errorCode': error_code,
                'timestamp': time.time()
            }

            if self.pub_socket:
                await self.pub_socket.send(json.dumps(message).encode('utf-8'))
                logger.warning(f"âš ï¸ [TRANSLATOR] Audio error publiÃ©: {message_id} - {error_code}")
            else:
                logger.error("âŒ [TRANSLATOR] Socket PUB non disponible pour audio error")

        except Exception as e:
            logger.error(f"âŒ [TRANSLATOR] Erreur publication audio error: {e}")

    async def _handle_voice_api_request(self, request_data: dict):
        """
        Traite une requÃªte Voice API.
        DÃ©lÃ¨gue au VoiceAPIHandler et publie le rÃ©sultat via PUB.
        """
        try:
            if not self.voice_api_handler:
                logger.error("[TRANSLATOR] Voice API handler non disponible")
                return

            # DÃ©lÃ©guer au handler
            response = await self.voice_api_handler.handle_request(request_data)

            # Publier la rÃ©ponse via PUB
            if self.pub_socket:
                await self.pub_socket.send(json.dumps(response).encode('utf-8'))
                logger.info(f"ğŸ“¤ [TRANSLATOR] Voice API response publiÃ©e: {response.get('taskId')} ({response.get('type')})")
            else:
                logger.error("âŒ [TRANSLATOR] Socket PUB non disponible pour Voice API response")

        except Exception as e:
            logger.error(f"âŒ [TRANSLATOR] Erreur Voice API: {e}")
            import traceback
            traceback.print_exc()

            # Publier une erreur
            error_response = {
                'type': 'voice_api_error',
                'taskId': request_data.get('taskId', ''),
                'requestType': request_data.get('type', ''),
                'error': str(e),
                'errorCode': 'INTERNAL_ERROR',
                'timestamp': time.time()
            }

            if self.pub_socket:
                await self.pub_socket.send(json.dumps(error_response).encode('utf-8'))

    async def _handle_voice_profile_request(self, request_data: dict):
        """
        Traite une requÃªte Voice Profile (internal processing for Gateway).

        Gateway sends audio via ZMQ, Translator processes and returns:
        - Fingerprint
        - Voice characteristics
        - Quality score
        - Embedding path

        Gateway then persists the results in database.
        """
        try:
            if not self.voice_profile_handler:
                logger.error("[TRANSLATOR] Voice Profile handler non disponible")
                return

            # DÃ©lÃ©guer au handler
            response = await self.voice_profile_handler.handle_request(request_data)

            # Publier la rÃ©ponse via PUB
            if self.pub_socket:
                await self.pub_socket.send(json.dumps(response).encode('utf-8'))
                logger.info(f"ğŸ“¤ [TRANSLATOR] Voice Profile response publiÃ©e: {response.get('request_id')} ({response.get('type')})")
            else:
                logger.error("âŒ [TRANSLATOR] Socket PUB non disponible pour Voice Profile response")

        except Exception as e:
            logger.error(f"âŒ [TRANSLATOR] Erreur Voice Profile: {e}")
            import traceback
            traceback.print_exc()

            # Publier une erreur
            error_response = {
                'type': 'voice_profile_error',
                'request_id': request_data.get('request_id', ''),
                'user_id': request_data.get('user_id', ''),
                'error': str(e),
                'success': False,
                'timestamp': time.time()
            }

            if self.pub_socket:
                await self.pub_socket.send(json.dumps(error_response).encode('utf-8'))

    def set_voice_api_services(
        self,
        transcription_service=None,
        translation_service=None,
        voice_clone_service=None,
        tts_service=None,
        voice_analyzer=None,
        translation_pipeline=None,
        analytics_service=None
    ):
        """
        Configure les services pour le Voice API handler et Voice Profile handler.
        AppelÃ© par main.py aprÃ¨s initialisation des services.
        """
        if self.voice_api_handler:
            self.voice_api_handler.transcription_service = transcription_service
            self.voice_api_handler.translation_service = translation_service
            self.voice_api_handler.voice_clone_service = voice_clone_service
            self.voice_api_handler.tts_service = tts_service
            self.voice_api_handler.voice_analyzer = voice_analyzer
            self.voice_api_handler.translation_pipeline = translation_pipeline
            self.voice_api_handler.analytics_service = analytics_service
            logger.info("âœ… [ZMQ] Voice API services configurÃ©s")

        # Also configure voice profile handler
        if self.voice_profile_handler:
            self.voice_profile_handler.voice_clone_service = voice_clone_service
            logger.info("âœ… [ZMQ] Voice Profile handler services configurÃ©s")

    async def _publish_translation_result(self, task_id: str, result: dict, target_language: str):
        """Publie un rÃ©sultat de traduction via PUB vers la gateway avec informations techniques complÃ¨tes"""
        try:
            # DEBUG: Logs rÃ©duits de 60% - Suppression des vÃ©rifications dÃ©taillÃ©es
            
            # RÃ©cupÃ©rer les informations techniques du systÃ¨me
            import socket
            import uuid
            
            # Calculer le temps d'attente en queue
            queue_time = time.time() - result.get('created_at', time.time())
            
            # RÃ©cupÃ©rer les mÃ©triques systÃ¨me
            memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            cpu_usage = psutil.Process().cpu_percent()
            # Attendre un peu pour avoir une mesure CPU valide
            await asyncio.sleep(0.1)
            cpu_usage = psutil.Process().cpu_percent()
            
            # Enrichir le rÃ©sultat avec toutes les informations techniques
            enriched_result = {
                # Informations applicatives existantes
                'messageId': result.get('messageId'),
                'translatedText': result.get('translatedText'),
                'sourceLanguage': result.get('sourceLanguage'),
                'targetLanguage': result.get('targetLanguage'),
                'confidenceScore': result.get('confidenceScore', 0.0),
                'processingTime': result.get('processingTime', 0.0),
                'modelType': result.get('modelType', 'basic'),
                'workerName': result.get('workerName', 'unknown'),
                
                # NOUVELLES INFORMATIONS TECHNIQUES
                'translatorModel': result.get('modelType', 'basic'),  # ModÃ¨le ML utilisÃ©
                'workerId': result.get('workerName', 'unknown'),      # Worker qui a traitÃ©
                'poolType': result.get('poolType', 'normal'),         # Pool utilisÃ©e (normal/any)
                'translationTime': result.get('processingTime', 0.0), # Temps de traduction
                'queueTime': queue_time,                              # Temps d'attente en queue
                'memoryUsage': memory_usage,                          # Usage mÃ©moire (MB)
                'cpuUsage': cpu_usage,                                # Usage CPU (%)
                'timestamp': time.time(),
                'version': '1.0.0'  # Version du Translator
            }
            
            # CrÃ©er le message enrichi
            message = {
                'type': 'translation_completed',
                'taskId': task_id,
                'result': enriched_result,
                'targetLanguage': target_language,
                'timestamp': time.time(),
                # MÃ‰TADONNÃ‰ES TECHNIQUES
                'metadata': {
                    'translatorVersion': '1.0.0',
                    'modelVersion': result.get('modelType', 'basic'),
                    'processingNode': socket.gethostname(),
                    'sessionId': str(uuid.uuid4()),
                    'requestId': task_id,
                    'protocol': 'ZMQ_PUB_SUB',
                    'encoding': 'UTF-8'
                }
            }
            
            # DEBUG: Logs rÃ©duits de 60% - Suppression des dÃ©tails techniques
            
            # VÃ‰RIFICATION DE LA QUALITÃ‰ DE LA TRADUCTION
            translated_text = result.get('translatedText', '')
            is_valid_translation = self._is_valid_translation(translated_text, result)
            
            if not is_valid_translation:
                # Traduction invalide - NE PAS ENVOYER Ã  la Gateway
                logger.error(f"âŒ [TRANSLATOR] Traduction invalide dÃ©tectÃ©e - PAS D'ENVOI Ã  la Gateway:")
                logger.error(f"   ğŸ“‹ Task ID: {task_id}")
                logger.error(f"   ğŸ“‹ Message ID: {result.get('messageId')}")
                logger.error(f"   ğŸ“‹ Source: {result.get('sourceLanguage')} -> Target: {target_language}")
                logger.error(f"   ğŸ“‹ Texte original: {result.get('originalText', 'N/A')}")
                logger.error(f"   ğŸ“‹ Texte traduit: '{translated_text}'")
                logger.error(f"   ğŸ“‹ ModÃ¨le utilisÃ©: {result.get('modelType', 'unknown')}")
                logger.error(f"   ğŸ“‹ Worker: {result.get('workerName', 'unknown')}")
                logger.error(f"   ğŸ“‹ Raison: {self._get_translation_error_reason(translated_text)}")
                return  # Sortir sans envoyer Ã  la Gateway
            
            # Traduction valide - SAUVEGARDE ET ENVOI
            try:
                # PrÃ©parer les donnÃ©es pour la sauvegarde
                save_data = {
                    'messageId': result.get('messageId'),
                    'sourceLanguage': result.get('sourceLanguage'),
                    'targetLanguage': result.get('targetLanguage'),
                    'translatedText': result.get('translatedText'),
                    'translatorModel': result.get('translatorModel', result.get('modelType', 'basic')),
                    'confidenceScore': result.get('confidenceScore', 0.9),
                    'processingTime': result.get('processingTime', 0.0),
                    'workerName': result.get('workerName', 'unknown'),
                    'poolType': result.get('poolType', 'normal')
                }
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ARCHITECTURE: PAS DE SAUVEGARDE EN BASE ICI
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # La persistance des traductions est la RESPONSABILITÃ‰ DE LA GATEWAY.
                # Translator ne fait que:
                #   1. Traduire (avec cache Redis pour Ã©viter les retraductions)
                #   2. Renvoyer les rÃ©sultats Ã  Gateway via ZMQ PUB
                # Gateway reÃ§oit les rÃ©sultats et persiste en base de donnÃ©es.
                #
                # Avantages:
                #   - SÃ©paration claire des responsabilitÃ©s
                #   - Translator peut fonctionner sans base de donnÃ©es
                #   - Gateway contrÃ´le la logique de persistance (encryption, etc.)
                #   - Meilleure scalabilitÃ© (Translator stateless)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # CODE DÃ‰SACTIVÃ‰ - ConservÃ© pour rÃ©fÃ©rence uniquement:
                # if self.database_service.is_db_connected():
                #     save_success = await self.database_service.save_translation(save_data)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    
            except Exception as e:
                logger.error(f"âŒ [TRANSLATOR] Erreur sauvegarde base de donnÃ©es: {e}")
            
            # ENVOI Ã€ LA GATEWAY (seulement si traduction valide)
            if self.pub_socket:
                await self.pub_socket.send(json.dumps(message).encode('utf-8'))
                logger.info(f"ğŸ“¤ [TRANSLATOR] RÃ©sultat envoyÃ© Ã  la Gateway: {task_id} -> {target_language}")
            else:
                logger.error("âŒ Socket PUB non initialisÃ©")
            
        except Exception as e:
            logger.error(f"Erreur lors de la publication du rÃ©sultat enrichi: {e}")
            import traceback
            traceback.print_exc()
    
    def _is_valid_translation(self, translated_text: str, result: dict) -> bool:
        """
        VÃ©rifie si une traduction est valide et peut Ãªtre envoyÃ©e Ã  la Gateway
        
        Args:
            translated_text: Le texte traduit
            result: Le rÃ©sultat complet de la traduction
        
        Returns:
            bool: True si la traduction est valide, False sinon
        """
        # VÃ©rifier que le texte traduit existe et n'est pas vide
        if not translated_text or translated_text.strip() == '':
            return False
        
        # VÃ©rifier que ce n'est pas un message d'erreur
        error_patterns = [
            r'^\[.*Error.*\]',
            r'^\[.*Failed.*\]',
            r'^\[.*No.*Result.*\]',
            r'^\[.*Fallback.*\]',
            r'^\[.*ML.*Error.*\]',
            r'^\[.*Ã‰CHEC.*\]',
            r'^\[.*MODÃˆLES.*NON.*\]',
            r'^\[.*MODÃˆLES.*NON.*CHARGÃ‰S.*\]',
            r'^\[.*NLLB.*No.*Result.*\]',
            r'^\[.*NLLB.*Fallback.*\]',
            r'^\[.*ERREUR.*\]',
            r'^\[.*FAILED.*\]',
            r'^\[.*TIMEOUT.*\]',
            r'^\[.*META.*TENSOR.*\]'
        ]
        
        for pattern in error_patterns:
            if re.search(pattern, translated_text, re.IGNORECASE):
                return False
        
        # VÃ©rifier que le texte traduit n'est pas identique au texte source
        original_text = result.get('originalText', '')
        if original_text and translated_text.strip().lower() == original_text.strip().lower():
            return False
        
        # VÃ©rifier que le score de confiance est acceptable
        confidence_score = result.get('confidenceScore', 1.0)
        if confidence_score < 0.1:
            return False
        
        # VÃ©rifier qu'il n'y a pas d'erreur dans le rÃ©sultat
        if result.get('error'):
            return False
        
        return True
    
    def _get_translation_error_reason(self, translated_text: str) -> str:
        """
        Retourne la raison de l'Ã©chec de traduction
        
        Args:
            translated_text: Le texte traduit
        
        Returns:
            str: La raison de l'Ã©chec
        """
        if not translated_text or translated_text.strip() == '':
            return "Texte traduit vide"
        
        error_patterns = [
            (r'^\[.*Error.*\]', "Message d'erreur dÃ©tectÃ©"),
            (r'^\[.*Failed.*\]', "Ã‰chec de traduction dÃ©tectÃ©"),
            (r'^\[.*No.*Result.*\]', "Aucun rÃ©sultat de traduction"),
            (r'^\[.*Fallback.*\]', "Fallback de traduction dÃ©tectÃ©"),
            (r'^\[.*ML.*Error.*\]', "Erreur ML dÃ©tectÃ©e"),
            (r'^\[.*Ã‰CHEC.*\]', "Ã‰chec de traduction"),
            (r'^\[.*MODÃˆLES.*NON.*\]', "ModÃ¨les non disponibles"),
            (r'^\[.*MODÃˆLES.*NON.*CHARGÃ‰S.*\]', "ModÃ¨les non chargÃ©s"),
            (r'^\[.*NLLB.*No.*Result.*\]', "NLLB: Aucun rÃ©sultat"),
            (r'^\[.*NLLB.*Fallback.*\]', "NLLB: Fallback"),
            (r'^\[.*ERREUR.*\]', "Erreur gÃ©nÃ©rale"),
            (r'^\[.*FAILED.*\]', "Ã‰chec gÃ©nÃ©ral"),
            (r'^\[.*TIMEOUT.*\]', "Timeout de traduction"),
            (r'^\[.*META.*TENSOR.*\]', "Erreur meta tensor")
        ]
        
        for pattern, reason in error_patterns:
            if re.search(pattern, translated_text, re.IGNORECASE):
                return reason
        
        return "Erreur de validation inconnue"
    
    async def stop(self):
        """ArrÃªte le serveur"""
        self.running = False
        
        # ArrÃªter les workers
        await self.pool_manager.stop_workers()
        
        # Attendre que tous les workers se terminent
        if self.worker_tasks:
            await asyncio.gather(*self.worker_tasks, return_exceptions=True)
        
        # Fermer la connexion Ã  la base de donnÃ©es
        await self.database_service.disconnect()
        
        # Fermer les sockets
        if self.pull_socket:
            self.pull_socket.close()
        if self.pub_socket:
            self.pub_socket.close()
        
        logger.info("ZMQTranslationServer arrÃªtÃ©")
    
    def get_stats(self) -> dict:
        """Retourne les statistiques du serveur"""
        pool_stats = self.pool_manager.get_stats()
        
        return {
            'server_status': 'running' if self.running else 'stopped',
            'gateway_push_port': self.gateway_push_port,
            'gateway_sub_port': self.gateway_sub_port,
            'normal_workers': self.pool_manager.normal_workers,
            'any_workers': self.pool_manager.any_workers,
            **pool_stats
        }
    
    async def health_check(self) -> dict:
        """VÃ©rification de santÃ© du serveur"""
        try:
            stats = self.get_stats()
            return {
                'status': 'healthy',
                'running': self.running,
                'stats': stats
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }
