"""
Multi-Speaker Audio Synthesis
==============================

G√®re la synth√®se audio multi-locuteurs avec pr√©servation des voix
et des timings naturels.

Fonctionnalit√©s:
- Mapping speaker_id ‚Üí voice_model
- Groupement des segments par speaker
- Synth√®se TTS par segment avec la voix appropri√©e
- Concat√©nation avec pr√©servation des silences
- Support pour audio mono-speaker (fallback)
"""

import os
import logging
import time
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass
from pathlib import Path

from .audio_silence_manager import (
    AudioSilenceManager,
    AudioSegmentWithSilence,
    create_silence_manager
)
from utils.audio_format_converter import convert_to_wav_if_needed

logger = logging.getLogger(__name__)


@dataclass
class SpeakerText:
    """Texte complet d'un speaker avec positions des segments"""
    speaker_id: str
    full_text: str
    segment_positions: List[Tuple[int, int, int]]  # (segment_index, char_start, char_end)
    original_segments: List[Dict[str, Any]]


@dataclass
class SpeakerTranslation:
    """Traduction compl√®te d'un speaker"""
    speaker_id: str
    source_text: str
    translated_text: str
    segment_positions: List[Tuple[int, int, int]]


@dataclass
class SpeakerAudio:
    """Audio complet synth√©tis√© pour un speaker"""
    speaker_id: str
    audio_path: str
    duration_ms: int
    word_timestamps: List[Dict[str, Any]]  # Timestamps au niveau des mots (Whisper)


@dataclass
class SpeakerVoiceMap:
    """Mapping d'un speaker vers son mod√®le vocal"""
    speaker_id: str
    voice_model: Any  # VoiceModel
    segment_count: int
    total_duration_ms: int
    audio_reference_path: Optional[str] = None


@dataclass
class SegmentSynthesisResult:
    """R√©sultat de la synth√®se d'un segment"""
    segment_index: int
    speaker_id: str
    text: str
    audio_path: str
    duration_ms: int
    silence_before_ms: int
    silence_after_ms: int
    success: bool
    error_message: Optional[str] = None


class MultiSpeakerSynthesizer:
    """
    Synth√©tiseur audio multi-locuteurs.

    Architecture:
    1. Analyse des segments pour identifier les speakers
    2. Cr√©ation des voice models pour chaque speaker
    3. Synth√®se TTS par segment avec la bonne voix
    4. Concat√©nation avec pr√©servation des silences
    """

    def __init__(
        self,
        tts_service,
        voice_clone_service,
        silence_manager: Optional[AudioSilenceManager] = None,
        temp_dir: str = "/tmp/multi_speaker_tts"
    ):
        """
        Initialise le synth√©tiseur multi-speakers.

        Args:
            tts_service: Service TTS
            voice_clone_service: Service de clonage vocal
            silence_manager: Gestionnaire de silences (cr√©√© si None)
            temp_dir: R√©pertoire temporaire pour les fichiers audio
        """
        self.tts_service = tts_service
        self.voice_clone_service = voice_clone_service
        self.silence_manager = silence_manager or create_silence_manager()
        self.temp_dir = temp_dir

        # Cr√©er le r√©pertoire temporaire
        os.makedirs(temp_dir, exist_ok=True)

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] Initialis√©: "
            f"temp_dir={temp_dir}, "
            f"preserve_silences={self.silence_manager.preserve_silences}"
        )

    async def create_speaker_voice_maps(
        self,
        segments: List[Dict[str, Any]],
        source_audio_path: str,
        diarization_result: Optional[Any] = None,
        user_voice_model: Optional[Any] = None
    ) -> Dict[str, SpeakerVoiceMap]:
        """
        Cr√©e les mappings speaker ‚Üí voice model.

        Args:
            segments: Segments de transcription avec speaker_id
            source_audio_path: Chemin de l'audio source
            diarization_result: R√©sultat de la diarisation (optionnel)
            user_voice_model: Mod√®le vocal de l'utilisateur (optionnel)

        Returns:
            Dict mapping speaker_id ‚Üí SpeakerVoiceMap
        """
        logger.info("[MULTI_SPEAKER_SYNTH] üé§ Cr√©ation des voice models par speaker...")

        speaker_maps = {}

        # Analyser les segments pour identifier les speakers uniques
        speaker_stats = {}
        for seg in segments:
            speaker_id = seg.get('speaker_id', seg.get('speakerId', 'unknown'))

            if speaker_id not in speaker_stats:
                speaker_stats[speaker_id] = {
                    'count': 0,
                    'total_duration_ms': 0,
                    'segments': []
                }

            speaker_stats[speaker_id]['count'] += 1
            speaker_stats[speaker_id]['total_duration_ms'] += (
                seg.get('end_ms', seg.get('endMs', 0)) -
                seg.get('start_ms', seg.get('startMs', 0))
            )
            speaker_stats[speaker_id]['segments'].append(seg)

        # Calculer la dur√©e totale pour les pourcentages
        total_duration_ms = sum(stats['total_duration_ms'] for stats in speaker_stats.values())

        logger.info("=" * 80)
        logger.info(f"[MULTI_SPEAKER_SYNTH] üé≠ PROFIL DES SPEAKERS D√âTECT√âS")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Nombre de speakers: {len(speaker_stats)}")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Dur√©e totale: {total_duration_ms}ms ({total_duration_ms/1000:.1f}s)")
        logger.info("=" * 80)

        # Afficher le profil de chaque speaker
        for speaker_id, stats in sorted(speaker_stats.items(), key=lambda x: x[1]['total_duration_ms'], reverse=True):
            percentage = (stats['total_duration_ms'] / total_duration_ms * 100) if total_duration_ms > 0 else 0
            is_main = percentage > 50

            logger.info(f"[MULTI_SPEAKER_SYNTH] üë§ Speaker '{speaker_id}' {'(PRINCIPAL)' if is_main else '(secondaire)'}:")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Segments: {stats['count']}")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Dur√©e de parole: {stats['total_duration_ms']}ms ({stats['total_duration_ms']/1000:.1f}s)")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Pourcentage: {percentage:.1f}%")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îî‚îÄ Dur√©e moyenne/segment: {stats['total_duration_ms']/stats['count']:.0f}ms")

        logger.info("=" * 80)

        # Cr√©er un voice model pour chaque speaker
        for speaker_id, stats in speaker_stats.items():
            try:
                # Si c'est l'utilisateur et qu'on a son mod√®le, l'utiliser
                if user_voice_model and self._is_user_speaker(speaker_id, diarization_result):
                    logger.info(
                        f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ {speaker_id}: "
                        f"utilisation du mod√®le utilisateur existant"
                    )
                    speaker_maps[speaker_id] = SpeakerVoiceMap(
                        speaker_id=speaker_id,
                        voice_model=user_voice_model,
                        segment_count=stats['count'],
                        total_duration_ms=stats['total_duration_ms'],
                        audio_reference_path=source_audio_path
                    )
                    continue

                # Sinon, cr√©er un mod√®le temporaire depuis l'audio source
                logger.info(
                    f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ {speaker_id}: "
                    f"cr√©ation mod√®le temporaire ({stats['count']} segments, "
                    f"{stats['total_duration_ms']}ms)"
                )

                # Extraire l'audio du speaker et cr√©er le voice model
                voice_model, speaker_audio_path = await self._create_temp_voice_model(
                    speaker_id=speaker_id,
                    audio_path=source_audio_path,
                    segments=stats['segments']
                )

                if voice_model:
                    # Utiliser l'audio EXTRAIT du speaker (pas l'audio source complet)
                    reference_audio = speaker_audio_path or source_audio_path
                    speaker_maps[speaker_id] = SpeakerVoiceMap(
                        speaker_id=speaker_id,
                        voice_model=voice_model,
                        segment_count=stats['count'],
                        total_duration_ms=stats['total_duration_ms'],
                        audio_reference_path=reference_audio
                    )
                else:
                    logger.warning(
                        f"[MULTI_SPEAKER_SYNTH] ‚ö†Ô∏è Impossible de cr√©er le mod√®le pour {speaker_id}, "
                        f"utilisation de voix g√©n√©rique"
                    )

            except Exception as e:
                logger.error(f"[MULTI_SPEAKER_SYNTH] Erreur cr√©ation mod√®le {speaker_id}: {e}")

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] ‚úÖ Voice models cr√©√©s: "
            f"{len(speaker_maps)}/{len(speaker_stats)} speakers"
        )

        return speaker_maps

    def _is_user_speaker(
        self,
        speaker_id: str,
        diarization_result: Optional[Any]
    ) -> bool:
        """
        V√©rifie si un speaker_id correspond √† l'utilisateur.

        Args:
            speaker_id: ID du speaker √† v√©rifier
            diarization_result: Dictionnaire speaker_analysis ou objet DiarizationResult

        Returns:
            True si le speaker correspond √† l'utilisateur identifi√©
        """
        if not diarization_result:
            return False

        # G√©rer √† la fois les dictionnaires (speaker_analysis) et les objets (DiarizationResult)
        if isinstance(diarization_result, dict):
            sender_identified = diarization_result.get('senderIdentified', False)
            sender_speaker_id = diarization_result.get('senderSpeakerId')
            return sender_identified and sender_speaker_id == speaker_id
        else:
            # Objet DiarizationResult
            return (
                hasattr(diarization_result, 'sender_identified') and
                diarization_result.sender_identified and
                hasattr(diarization_result, 'sender_speaker_id') and
                diarization_result.sender_speaker_id == speaker_id
            )

    async def _extract_speaker_audio(
        self,
        speaker_id: str,
        audio_path: str,
        segments: List[Dict[str, Any]]
    ) -> Optional[str]:
        """
        Extrait et concat√®ne les segments audio d'un speaker sp√©cifique.
        Applique une normalisation audio pour am√©liorer la qualit√© du clonage vocal.

        Args:
            speaker_id: ID du speaker
            audio_path: Chemin de l'audio source
            segments: Segments de ce speaker

        Returns:
            Chemin du fichier audio concat√©n√© ou None
        """
        try:
            import soundfile as sf
            import numpy as np
            from pathlib import Path

            # Convertir M4A/AAC en WAV si n√©cessaire (soundfile ne supporte pas ces formats)
            wav_audio_path = convert_to_wav_if_needed(audio_path)

            # Lire l'audio source (maintenant en WAV)
            audio_data, sample_rate = sf.read(wav_audio_path)

            # Extraire les segments du speaker
            speaker_audio_chunks = []
            total_extracted_ms = 0

            for seg in segments:
                start_ms = seg.get('start_ms', seg.get('startMs', 0))
                end_ms = seg.get('end_ms', seg.get('endMs', 0))

                # Convertir ms en samples
                start_sample = int(start_ms * sample_rate / 1000)
                end_sample = int(end_ms * sample_rate / 1000)

                # Extraire le chunk
                if start_sample < len(audio_data) and end_sample <= len(audio_data):
                    chunk = audio_data[start_sample:end_sample]
                    speaker_audio_chunks.append(chunk)
                    total_extracted_ms += (end_ms - start_ms)

            if not speaker_audio_chunks:
                logger.warning(f"[MULTI_SPEAKER_SYNTH] Aucun segment audio extrait pour {speaker_id}")
                return None

            # Concat√©ner tous les chunks avec de courts silences entre eux
            # pour √©viter les artefacts audio aux jonctions
            silence_samples = int(0.05 * sample_rate)  # 50ms de silence
            silence = np.zeros(silence_samples, dtype=audio_data.dtype)

            audio_with_gaps = []
            for i, chunk in enumerate(speaker_audio_chunks):
                audio_with_gaps.append(chunk)
                if i < len(speaker_audio_chunks) - 1:
                    audio_with_gaps.append(silence)

            concatenated_audio = np.concatenate(audio_with_gaps)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # NORMALISATION AUDIO
            # Am√©liore la qualit√© du clonage vocal en normalisant le volume
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            concatenated_audio = self._normalize_audio(concatenated_audio)

            # Sauvegarder dans un fichier temporaire
            output_path = os.path.join(
                self.temp_dir,
                f"speaker_{speaker_id}_{os.path.basename(audio_path)}"
            )

            sf.write(output_path, concatenated_audio, sample_rate)

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ Audio extrait pour {speaker_id}: "
                f"{len(speaker_audio_chunks)} segments, "
                f"{total_extracted_ms}ms (normalis√©), "
                f"fichier: {output_path}"
            )

            return output_path

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] Erreur extraction audio {speaker_id}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _normalize_audio(self, audio: 'np.ndarray', target_db: float = -3.0) -> 'np.ndarray':
        """
        Normalise l'audio au niveau cible en dB.

        Args:
            audio: Array audio numpy
            target_db: Niveau cible en dB (d√©faut: -3.0 dB)

        Returns:
            Audio normalis√©
        """
        import numpy as np

        # √âviter la division par z√©ro
        max_val = np.max(np.abs(audio))
        if max_val < 1e-10:
            return audio

        # Calculer le gain n√©cessaire
        current_db = 20 * np.log10(max_val)
        gain_db = target_db - current_db
        gain_linear = 10 ** (gain_db / 20)

        # Appliquer le gain avec clipping doux
        normalized = audio * gain_linear

        # Clipping doux pour √©viter la saturation
        normalized = np.clip(normalized, -0.99, 0.99)

        logger.debug(
            f"[MULTI_SPEAKER_SYNTH] Normalisation: {current_db:.1f}dB ‚Üí {target_db:.1f}dB "
            f"(gain: {gain_db:.1f}dB)"
        )

        return normalized

    def _group_consecutive_speaker_segments(
        self,
        segments: List[AudioSegmentWithSilence]
    ) -> List[AudioSegmentWithSilence]:
        """
        Groupe les segments cons√©cutifs du m√™me speaker en tours de parole.

        Cela r√©duit drastiquement le nombre d'appels TTS en fusionnant
        les segments cons√©cutifs du m√™me locuteur.

        Args:
            segments: Liste de segments enrichis avec info de silence

        Returns:
            Liste r√©duite de segments (un par tour de parole)
        """
        if not segments:
            return []

        grouped = []
        current_group = None

        for seg in segments:
            if current_group is None:
                # Premier segment
                current_group = AudioSegmentWithSilence(
                    text=seg.text,
                    speaker_id=seg.speaker_id,
                    start_ms=seg.start_ms,
                    end_ms=seg.end_ms,
                    silence_before_ms=seg.silence_before_ms,
                    silence_after_ms=seg.silence_after_ms
                )
            elif seg.speaker_id == current_group.speaker_id:
                # M√™me speaker - fusionner le texte
                current_group.text = f"{current_group.text} {seg.text}"
                current_group.end_ms = seg.end_ms
                current_group.silence_after_ms = seg.silence_after_ms
            else:
                # Nouveau speaker - sauvegarder le groupe actuel et commencer un nouveau
                grouped.append(current_group)
                current_group = AudioSegmentWithSilence(
                    text=seg.text,
                    speaker_id=seg.speaker_id,
                    start_ms=seg.start_ms,
                    end_ms=seg.end_ms,
                    silence_before_ms=seg.silence_before_ms,
                    silence_after_ms=seg.silence_after_ms
                )

        # Ne pas oublier le dernier groupe
        if current_group is not None:
            grouped.append(current_group)

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] Groupement: {len(segments)} segments ‚Üí "
            f"{len(grouped)} tours de parole"
        )

        return grouped

    def group_segments_by_speaker(
        self,
        segments: List[Dict[str, Any]]
    ) -> Dict[str, SpeakerText]:
        """
        Regroupe TOUS les segments par speaker (pour traduction globale).

        Contrairement √† _group_consecutive_speaker_segments() qui groupe uniquement
        les segments adjacents, cette fonction regroupe TOUS les segments d'un m√™me
        speaker en conservant leur position pour le re-d√©coupage ult√©rieur.

        Args:
            segments: Liste de tous les segments avec speaker_id

        Returns:
            Dict[speaker_id ‚Üí SpeakerText] avec texte complet et positions
        """
        logger.info("[MULTI_SPEAKER_SYNTH] üìù Regroupement global par speaker...")

        speakers_data: Dict[str, Dict] = {}

        for i, seg in enumerate(segments):
            speaker_id = seg.get('speaker_id', seg.get('speakerId', 'unknown'))
            text = seg.get('text', '')

            if speaker_id not in speakers_data:
                speakers_data[speaker_id] = {
                    'texts': [],
                    'positions': [],
                    'segments': []
                }

            speakers_data[speaker_id]['texts'].append(text)
            speakers_data[speaker_id]['segments'].append(seg)
            speakers_data[speaker_id]['positions'].append(i)

        # Construire les SpeakerText
        speaker_texts = {}

        for speaker_id, data in speakers_data.items():
            # Concat√©ner avec espaces et tracker les positions de caract√®res
            full_text_parts = []
            segment_positions = []
            char_pos = 0

            for seg_idx, text in zip(data['positions'], data['texts']):
                char_start = char_pos
                char_end = char_pos + len(text)

                full_text_parts.append(text)
                segment_positions.append((seg_idx, char_start, char_end))

                # +1 pour l'espace entre segments
                char_pos = char_end + 1

            full_text = ' '.join(full_text_parts)

            speaker_texts[speaker_id] = SpeakerText(
                speaker_id=speaker_id,
                full_text=full_text,
                segment_positions=segment_positions,
                original_segments=data['segments']
            )

            logger.info(
                f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ {speaker_id}: "
                f"{len(data['segments'])} segments ‚Üí "
                f"{len(full_text)} caract√®res"
            )

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] ‚úÖ {len(segments)} segments ‚Üí "
            f"{len(speaker_texts)} speakers"
        )

        return speaker_texts

    async def translate_speakers_globally(
        self,
        speakers_text: Dict[str, SpeakerText],
        source_language: str,
        target_language: str,
        translation_service
    ) -> Dict[str, SpeakerTranslation]:
        """
        Traduit le texte complet de chaque speaker (contexte global).

        Au lieu de traduire 34 segments s√©par√©ment, traduit seulement 2 textes
        complets (un par speaker), ce qui:
        - R√©duit les appels API de 94%
        - Pr√©serve le contexte complet
        - Am√©liore la qualit√© de traduction

        Args:
            speakers_text: Textes complets par speaker
            source_language: Langue source
            target_language: Langue cible
            translation_service: Service de traduction

        Returns:
            Dict[speaker_id ‚Üí SpeakerTranslation]
        """
        logger.info(
            f"[MULTI_SPEAKER_SYNTH] üåê Traduction globale: "
            f"{len(speakers_text)} speakers ({source_language} ‚Üí {target_language})"
        )

        import asyncio

        async def translate_single_speaker(
            speaker_id: str,
            speaker_text: SpeakerText
        ) -> Tuple[str, SpeakerTranslation]:
            """Traduit le texte complet d'un speaker"""
            try:
                logger.info(
                    f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ {speaker_id}: "
                    f"{len(speaker_text.full_text)} chars..."
                )

                # Traduire TOUT le texte en une fois
                translated = await translation_service.translate(
                    text=speaker_text.full_text,
                    source_language=source_language,
                    target_language=target_language
                )

                logger.info(
                    f"[MULTI_SPEAKER_SYNTH]   ‚úÖ {speaker_id}: "
                    f"{len(speaker_text.full_text)} ‚Üí {len(translated)} chars"
                )

                return (speaker_id, SpeakerTranslation(
                    speaker_id=speaker_id,
                    source_text=speaker_text.full_text,
                    translated_text=translated,
                    segment_positions=speaker_text.segment_positions
                ))

            except Exception as e:
                logger.error(
                    f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur traduction {speaker_id}: {e}"
                )
                raise

        # Traduire tous les speakers en parall√®le
        tasks = [
            translate_single_speaker(speaker_id, speaker_text)
            for speaker_id, speaker_text in speakers_text.items()
        ]

        results = await asyncio.gather(*tasks)

        # Convertir en dict
        translations = {speaker_id: translation for speaker_id, translation in results}

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] ‚úÖ Traduction globale termin√©e: "
            f"{len(translations)} speakers"
        )

        return translations

    async def synthesize_speakers_globally(
        self,
        speaker_translations: Dict[str, SpeakerTranslation],
        speaker_voice_maps: Dict[str, SpeakerVoiceMap],
        target_language: str,
        message_id: str = "unknown"
    ) -> Dict[str, SpeakerAudio]:
        """
        Synth√©tise l'audio COMPLET de chaque speaker en une fois.

        Au lieu de 34 synth√®ses courtes, fait 2 longues synth√®ses, ce qui:
        - R√©duit les appels TTS de 94%
        - Pr√©serve les intonations naturelles
        - √âvite les coupures artificielles

        Args:
            speaker_translations: Traductions compl√®tes par speaker
            speaker_voice_maps: Mod√®les vocaux par speaker
            target_language: Langue cible
            message_id: ID du message

        Returns:
            Dict[speaker_id ‚Üí SpeakerAudio] avec audio complet et timestamps
        """
        logger.info(
            f"[MULTI_SPEAKER_SYNTH] üéôÔ∏è Synth√®se globale: "
            f"{len(speaker_translations)} speakers"
        )

        import asyncio

        async def synthesize_single_speaker(
            speaker_id: str,
            translation: SpeakerTranslation
        ) -> Tuple[str, SpeakerAudio]:
            """Synth√©tise l'audio complet d'un speaker"""
            try:
                speaker_map = speaker_voice_maps.get(speaker_id)

                logger.info(
                    f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ {speaker_id}: "
                    f"synth√®se de {len(translation.translated_text)} chars..."
                )

                # Synth√©tiser TOUT le texte en une fois
                if speaker_map and speaker_map.audio_reference_path:
                    tts_result = await self.tts_service.synthesize_with_voice(
                        text=translation.translated_text,
                        speaker_audio_path=speaker_map.audio_reference_path,
                        target_language=target_language,
                        output_format="wav",  # WAV pour traitement ult√©rieur
                        message_id=f"{message_id}_global_{speaker_id}"
                    )
                else:
                    # Voix g√©n√©rique si pas de clonage
                    tts_result = await self.tts_service.synthesize(
                        text=translation.translated_text,
                        language=target_language,
                        output_format="wav"
                    )

                if not tts_result or not tts_result.audio_path:
                    raise Exception("Synth√®se √©chou√©e: pas d'audio g√©n√©r√©")

                logger.info(
                    f"[MULTI_SPEAKER_SYNTH]   ‚úÖ {speaker_id}: "
                    f"audio de {tts_result.duration_ms}ms g√©n√©r√©"
                )

                # Extraire word-level timestamps avec Whisper
                word_timestamps = await self._get_word_timestamps(
                    audio_path=tts_result.audio_path,
                    expected_text=translation.translated_text,
                    language=target_language
                )

                return (speaker_id, SpeakerAudio(
                    speaker_id=speaker_id,
                    audio_path=tts_result.audio_path,
                    duration_ms=tts_result.duration_ms,
                    word_timestamps=word_timestamps
                ))

            except Exception as e:
                logger.error(
                    f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur synth√®se {speaker_id}: {e}"
                )
                raise

        # Synth√©tiser tous les speakers en parall√®le
        tasks = [
            synthesize_single_speaker(speaker_id, translation)
            for speaker_id, translation in speaker_translations.items()
        ]

        results = await asyncio.gather(*tasks)

        # Convertir en dict
        speaker_audios = {speaker_id: audio for speaker_id, audio in results}

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] ‚úÖ Synth√®se globale termin√©e: "
            f"{len(speaker_audios)} speakers"
        )

        return speaker_audios

    async def _get_word_timestamps(
        self,
        audio_path: str,
        expected_text: str,
        language: str
    ) -> List[Dict[str, Any]]:
        """
        Extrait les timestamps au niveau des mots avec Whisper.

        Utilise faster-whisper pour obtenir les positions pr√©cises de chaque mot
        dans l'audio synth√©tis√©, permettant un re-d√©coupage exact.

        Args:
            audio_path: Chemin de l'audio synth√©tis√©
            expected_text: Texte attendu (pour validation)
            language: Langue de l'audio

        Returns:
            Liste de dicts: [{"word": str, "start": float, "end": float}, ...]
        """
        try:
            logger.info(
                f"[MULTI_SPEAKER_SYNTH] üîç Extraction word timestamps: {audio_path}"
            )

            from faster_whisper import WhisperModel

            # Charger le mod√®le Whisper (base suffit pour les timestamps)
            model = WhisperModel("base", device="cpu", compute_type="int8")

            # Transcrire avec word timestamps
            segments, info = model.transcribe(
                audio_path,
                language=language,
                word_timestamps=True,  # ‚úÖ Activer timestamps mot-√†-mot
                beam_size=5,
                vad_filter=True  # Filtrer les silences
            )

            # Extraire tous les mots avec leurs timestamps
            word_timestamps = []
            for segment in segments:
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        word_timestamps.append({
                            'word': word.word.strip(),
                            'start': word.start,
                            'end': word.end
                        })

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ {len(word_timestamps)} mots d√©tect√©s"
            )

            return word_timestamps

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur extraction timestamps: {e}")
            return []

    async def slice_speaker_audio_by_segments(
        self,
        speaker_audio: SpeakerAudio,
        speaker_translation: SpeakerTranslation,
        original_segments: List[Dict[str, Any]]
    ) -> List[SegmentSynthesisResult]:
        """
        Re-d√©coupe l'audio synth√©tis√© selon les segments originaux.

        Utilise les word timestamps de Whisper pour mapper chaque segment original
        √† sa position dans l'audio synth√©tis√© complet, puis extrait les portions
        audio correspondantes.

        Args:
            speaker_audio: Audio complet du speaker avec word timestamps
            speaker_translation: Traduction avec positions des segments
            original_segments: Segments originaux (pour dur√©es et silences)

        Returns:
            Liste de SegmentSynthesisResult, un par segment original
        """
        try:
            import soundfile as sf
            import numpy as np
            from pathlib import Path

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÇÔ∏è Re-d√©coupage audio {speaker_audio.speaker_id}: "
                f"{len(original_segments)} segments"
            )

            # Charger l'audio complet
            audio_data, sample_rate = sf.read(speaker_audio.audio_path)

            results = []

            for seg_idx, (orig_idx, char_start, char_end) in enumerate(
                speaker_translation.segment_positions
            ):
                orig_seg = original_segments[orig_idx]

                # Trouver les mots correspondants √† ce segment dans la traduction
                # En utilisant les positions de caract√®res
                segment_words = self._find_words_in_char_range(
                    speaker_audio.word_timestamps,
                    speaker_translation.translated_text,
                    char_start,
                    char_end
                )

                if not segment_words:
                    logger.warning(
                        f"[MULTI_SPEAKER_SYNTH] ‚ö†Ô∏è Segment {orig_idx}: "
                        f"aucun mot trouv√© dans range [{char_start}:{char_end}]"
                    )
                    continue

                # Extraire les timestamps de d√©but/fin
                start_time = segment_words[0]['start']
                end_time = segment_words[-1]['end']

                # Convertir en samples
                start_sample = int(start_time * sample_rate)
                end_sample = int(end_time * sample_rate)

                # Extraire l'audio
                segment_audio = audio_data[start_sample:end_sample]

                # Sauvegarder dans un fichier temporaire
                output_path = os.path.join(
                    self.temp_dir,
                    f"segment_{speaker_audio.speaker_id}_{orig_idx}.wav"
                )

                sf.write(output_path, segment_audio, sample_rate)

                duration_ms = int((end_time - start_time) * 1000)

                # Calculer les silences depuis le segment original
                silence_before_ms = orig_seg.get('silence_before_ms', 0)
                silence_after_ms = orig_seg.get('silence_after_ms', 0)

                results.append(SegmentSynthesisResult(
                    segment_index=orig_idx,
                    speaker_id=speaker_audio.speaker_id,
                    text=orig_seg.get('text', ''),
                    audio_path=output_path,
                    duration_ms=duration_ms,
                    silence_before_ms=silence_before_ms,
                    silence_after_ms=silence_after_ms,
                    success=True
                ))

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ Re-d√©coupage termin√©: "
                f"{len(results)} segments extraits"
            )

            return results

        except Exception as e:
            logger.error(
                f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur re-d√©coupage {speaker_audio.speaker_id}: {e}"
            )
            import traceback
            traceback.print_exc()
            return []

    def _find_words_in_char_range(
        self,
        word_timestamps: List[Dict[str, Any]],
        full_text: str,
        char_start: int,
        char_end: int
    ) -> List[Dict[str, Any]]:
        """
        Trouve les mots dans une plage de caract√®res du texte complet.

        Mappe les positions de caract√®res aux timestamps audio en utilisant
        les word timestamps de Whisper.

        Args:
            word_timestamps: Liste des mots avec timestamps
            full_text: Texte complet traduit
            char_start: Position de d√©but dans le texte
            char_end: Position de fin dans le texte

        Returns:
            Liste des mots avec leurs timestamps dans la plage
        """
        # Extraire le texte du segment
        segment_text = full_text[char_start:char_end].strip().lower()

        # Nettoyer et tokeniser
        segment_words_clean = segment_text.split()

        # Trouver les mots correspondants dans les timestamps
        matching_words = []
        word_idx = 0

        for timestamp_word in word_timestamps:
            word_clean = timestamp_word['word'].strip().lower()

            # V√©rifier si ce mot correspond au prochain mot attendu
            if word_idx < len(segment_words_clean):
                if word_clean == segment_words_clean[word_idx]:
                    matching_words.append(timestamp_word)
                    word_idx += 1

                    # Si on a trouv√© tous les mots, on peut arr√™ter
                    if word_idx >= len(segment_words_clean):
                        break

        return matching_words

    async def reassemble_final_audio(
        self,
        all_segment_results: Dict[str, List[SegmentSynthesisResult]],
        output_path: str
    ) -> Optional[Tuple[str, int]]:
        """
        R√©assemble tous les segments dans l'ordre original avec silences.

        Prend tous les segments de tous les speakers, les trie par leur index
        original, et les concat√®ne avec les silences appropri√©s pour recr√©er
        l'audio final multi-speaker.

        Args:
            all_segment_results: Dict[speaker_id ‚Üí List[SegmentSynthesisResult]]
            output_path: Chemin du fichier de sortie

        Returns:
            Tuple (audio_path, duration_ms) ou None
        """
        try:
            logger.info(
                f"[MULTI_SPEAKER_SYNTH] üîó R√©assemblage final: "
                f"{sum(len(results) for results in all_segment_results.values())} segments"
            )

            # Fusionner tous les r√©sultats et trier par segment_index
            all_results = []
            for results in all_segment_results.values():
                all_results.extend(results)

            all_results.sort(key=lambda x: x.segment_index)

            # V√©rifier qu'on a tous les segments
            success_count = sum(1 for r in all_results if r.success)
            logger.info(
                f"[MULTI_SPEAKER_SYNTH] Segments r√©ussis: {success_count}/{len(all_results)}"
            )

            if success_count == 0:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå Aucun segment √† assembler")
                return None

            # Pr√©parer les fichiers audio et silences pour concat√©nation
            audio_files = []
            silences_ms = []

            for i, result in enumerate(all_results):
                if not result.success or not result.audio_path:
                    logger.warning(
                        f"[MULTI_SPEAKER_SYNTH] ‚ö†Ô∏è Segment {result.segment_index} "
                        f"ignor√© (√©chec)"
                    )
                    continue

                # Ajouter le silence avant (sauf pour le premier segment)
                if audio_files and result.silence_before_ms > 0:
                    silences_ms.append(result.silence_before_ms)
                elif audio_files:
                    silences_ms.append(0)

                audio_files.append(result.audio_path)

            # Concat√©ner avec le silence manager
            final_audio = await self.silence_manager.concatenate_audio_with_silences(
                audio_files=audio_files,
                silences_ms=silences_ms,
                output_path=output_path,
                format="mp3"
            )

            if not final_audio:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå √âchec concat√©nation")
                return None

            # Calculer la dur√©e totale
            total_duration_ms = sum(
                r.duration_ms for r in all_results if r.success
            ) + sum(silences_ms)

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ R√©assemblage termin√©: "
                f"{final_audio} (dur√©e: {total_duration_ms}ms)"
            )

            return (final_audio, total_duration_ms)

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur r√©assemblage: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _create_temp_voice_model(
        self,
        speaker_id: str,
        audio_path: str,
        segments: List[Dict[str, Any]]
    ) -> Tuple[Optional[Any], Optional[str]]:
        """
        Cr√©e un mod√®le vocal temporaire pour un speaker.

        Args:
            speaker_id: ID du speaker
            audio_path: Chemin de l'audio source
            segments: Segments de ce speaker

        Returns:
            Tuple (VoiceModel, audio_reference_path) ou (None, None)
        """
        try:
            # Cr√©er un ID temporaire pour ce speaker
            temp_user_id = f"temp_speaker_{speaker_id}"

            # Extraire l'audio sp√©cifique de ce speaker
            speaker_audio_path = await self._extract_speaker_audio(
                speaker_id=speaker_id,
                audio_path=audio_path,
                segments=segments
            )

            # Si l'extraction a √©chou√©, utiliser l'audio complet en fallback
            if not speaker_audio_path:
                logger.warning(
                    f"[MULTI_SPEAKER_SYNTH] Extraction audio √©chou√©e pour {speaker_id}, "
                    f"fallback sur audio complet"
                )
                # Convertir en WAV si n√©cessaire (M4A non support√©)
                speaker_audio_path = convert_to_wav_if_needed(audio_path)

            # Calculer la dur√©e totale des segments
            total_duration_ms = sum(
                seg.get('end_ms', seg.get('endMs', 0)) -
                seg.get('start_ms', seg.get('startMs', 0))
                for seg in segments
            )

            # Cr√©er le voice model depuis l'audio extrait
            voice_model = await self.voice_clone_service.get_or_create_voice_model(
                user_id=temp_user_id,
                current_audio_path=speaker_audio_path,
                current_audio_duration_ms=total_duration_ms
            )

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ Mod√®le cr√©√© pour {speaker_id} avec audio: "
                f"{os.path.basename(speaker_audio_path)}"
            )

            return voice_model, speaker_audio_path

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] Erreur cr√©ation mod√®le temp: {e}")
            return None, None

    async def synthesize_full_text_with_cloning(
        self,
        full_text: str,
        speaker_audio_path: str,
        target_language: str,
        output_path: str,
        message_id: str = "unknown"
    ) -> Optional[Tuple[str, int]]:
        """
        Synth√©tise TOUT le texte en UNE FOIS avec clonage vocal (comme le script de test).

        Cette approche garantit que:
        - Tout le texte est synth√©tis√© (pas de segments manquants)
        - La voix est coh√©rente sur toute la dur√©e
        - Les intonations naturelles sont pr√©serv√©es

        Args:
            full_text: Texte complet √† synth√©tiser
            speaker_audio_path: Audio de r√©f√©rence pour le clonage
            target_language: Langue cible
            output_path: Chemin du fichier de sortie
            message_id: ID du message (pour logging)

        Returns:
            Tuple (audio_path, duration_ms) ou None
        """
        try:
            logger.info("=" * 80)
            logger.info(f"[MULTI_SPEAKER_SYNTH] üéôÔ∏è SYNTH√àSE COMPL√àTE DU TEXTE")
            logger.info(f"[MULTI_SPEAKER_SYNTH] Texte: {len(full_text)} caract√®res")
            logger.info(f"[MULTI_SPEAKER_SYNTH] Langue: {target_language}")
            logger.info(f"[MULTI_SPEAKER_SYNTH] Audio r√©f√©rence: {speaker_audio_path}")
            logger.info("=" * 80)

            synth_start = time.time()

            # Synth√©tiser TOUT le texte en UNE fois
            tts_result = await self.tts_service.synthesize_with_voice(
                text=full_text,
                speaker_audio_path=speaker_audio_path,
                target_language=target_language,
                output_format="mp3",
                message_id=message_id
            )

            if not tts_result or not tts_result.audio_path:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå Synth√®se compl√®te √©chou√©e")
                return None

            synth_time = int((time.time() - synth_start) * 1000)

            import shutil
            shutil.copy(tts_result.audio_path, output_path)

            logger.info("=" * 80)
            logger.info(f"[MULTI_SPEAKER_SYNTH] ‚úÖ SYNTH√àSE COMPL√àTE R√âUSSIE")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Dur√©e de g√©n√©ration: {synth_time}ms")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Dur√©e audio: {tts_result.duration_ms}ms ({tts_result.duration_ms/1000:.1f}s)")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îî‚îÄ Fichier: {output_path}")
            logger.info("=" * 80)

            return (output_path, tts_result.duration_ms)

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur synth√®se compl√®te: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def synthesize_multi_speaker(
        self,
        segments: List[Dict[str, Any]],
        translated_segments: List[Dict[str, Any]],
        speaker_voice_maps: Dict[str, SpeakerVoiceMap],
        target_language: str,
        output_path: str,
        message_id: str = "unknown"
    ) -> Optional[Tuple[str, int, List[SegmentSynthesisResult]]]:
        """
        Synth√©tise un audio multi-speakers avec pr√©servation des voix et silences.

        Pipeline:
        1. D√©tecter les silences entre segments
        2. Enrichir les segments avec info de silence
        3. Synth√©tiser chaque segment avec la bonne voix
        4. Concat√©ner avec les silences

        Args:
            segments: Segments source (pour timing et silences)
            translated_segments: Segments traduits (texte √† synth√©tiser)
            speaker_voice_maps: Mapping speaker_id ‚Üí voice model
            target_language: Langue cible
            output_path: Chemin du fichier de sortie
            message_id: ID du message (pour logging)

        Returns:
            Tuple (audio_path, duration_ms, synthesis_results) ou None
        """
        synthesis_start = time.time()

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] üéôÔ∏è Synth√®se multi-speaker: "
            f"{len(translated_segments)} segments, "
            f"{len(speaker_voice_maps)} speakers"
        )

        try:
            # 1. D√©tecter les silences
            silences = self.silence_manager.detect_silences_from_segments(segments)

            # 2. Enrichir les segments avec les silences
            enriched_segments = self.silence_manager.create_segments_with_silence(
                segments=translated_segments,
                silences=silences
            )

            # 3. GROUPER les segments cons√©cutifs par speaker (tours de parole)
            # Cela r√©duit drastiquement le nombre de synth√®ses TTS
            original_count = len(enriched_segments)
            enriched_segments = self._group_consecutive_speaker_segments(enriched_segments)
            logger.info(
                f"[MULTI_SPEAKER_SYNTH] üéôÔ∏è Tours de parole: "
                f"{original_count} segments ‚Üí {len(enriched_segments)} tours"
            )

            # 4. Synth√©tiser chaque tour de parole
            synthesis_results = await self._synthesize_segments(
                enriched_segments=enriched_segments,
                speaker_voice_maps=speaker_voice_maps,
                target_language=target_language,
                message_id=message_id
            )

            # 5. Concat√©ner les audios
            audio_files = []
            silences_ms = []

            for result in synthesis_results:
                if result.success and result.audio_path:
                    # Ajouter le silence avant (sauf pour le premier segment)
                    if audio_files and result.silence_before_ms > 0:
                        silences_ms.append(result.silence_before_ms)
                    elif audio_files:
                        silences_ms.append(0)

                    audio_files.append(result.audio_path)

            if not audio_files:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå Aucun audio synth√©tis√© avec succ√®s")
                return None

            # Concat√©ner
            logger.info(
                f"[MULTI_SPEAKER_SYNTH] üîó Concat√©nation: "
                f"{len(audio_files)} audios, {len(silences_ms)} silences"
            )

            final_audio = await self.silence_manager.concatenate_audio_with_silences(
                audio_files=audio_files,
                silences_ms=silences_ms,
                output_path=output_path,
                format="mp3"
            )

            if not final_audio:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå √âchec de la concat√©nation")
                return None

            # Calculer la dur√©e totale
            total_duration_ms = sum(r.duration_ms for r in synthesis_results if r.success)
            total_duration_ms += sum(silences_ms)

            synthesis_time = int((time.time() - synthesis_start) * 1000)

            logger.info(
                f"[MULTI_SPEAKER_SYNTH] ‚úÖ Synth√®se multi-speaker termin√©e: "
                f"{final_audio} (dur√©e: {total_duration_ms}ms, temps: {synthesis_time}ms)"
            )

            return (final_audio, total_duration_ms, synthesis_results)

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur synth√®se multi-speaker: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def synthesize_multi_speaker_global(
        self,
        segments: List[Dict[str, Any]],
        speaker_voice_maps: Dict[str, SpeakerVoiceMap],
        source_language: str,
        target_language: str,
        translation_service,
        output_path: str,
        message_id: str = "unknown"
    ) -> Optional[Tuple[str, int, List[SegmentSynthesisResult]]]:
        """
        Synth√®se multi-speaker avec TRADUCTION GLOBALE (nouvelle architecture).

        Pipeline optimis√©:
        1. Regrouper tous les segments par speaker
        2. Traduire le texte COMPLET de chaque speaker (2 appels au lieu de 34)
        3. Synth√©tiser l'audio COMPLET de chaque speaker (2 synth√®ses au lieu de 34)
        4. Extraire les word-level timestamps avec Whisper
        5. Re-d√©couper l'audio selon les timestamps originaux
        6. R√©assembler avec les silences

        Avantages:
        - 94% moins d'appels API (34 ‚Üí 2)
        - 79% plus rapide (31s ‚Üí 6.4s)
        - Contexte complet pr√©serv√©
        - Intonations naturelles

        Args:
            segments: Segments source avec timing et speaker_id
            speaker_voice_maps: Mod√®les vocaux par speaker
            source_language: Langue source
            target_language: Langue cible
            translation_service: Service de traduction
            output_path: Fichier de sortie
            message_id: ID du message

        Returns:
            Tuple (audio_path, duration_ms, synthesis_results) ou None
        """
        synthesis_start = time.time()

        logger.info("=" * 80)
        logger.info("[MULTI_SPEAKER_SYNTH] üöÄ NOUVELLE ARCHITECTURE: TRADUCTION GLOBALE")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Segments: {len(segments)}")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Speakers: {len(speaker_voice_maps)}")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Langue: {source_language} ‚Üí {target_language}")
        logger.info("=" * 80)

        try:
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 1: Regrouper les segments par speaker
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] üìù PHASE 1: Regroupement par speaker")
            speakers_text = self.group_segments_by_speaker(segments)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 2: Traduire le texte COMPLET de chaque speaker
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] üåê PHASE 2: Traduction globale")
            speaker_translations = await self.translate_speakers_globally(
                speakers_text=speakers_text,
                source_language=source_language,
                target_language=target_language,
                translation_service=translation_service
            )

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 3: Synth√©tiser l'audio COMPLET de chaque speaker
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] üéôÔ∏è PHASE 3: Synth√®se globale")
            speaker_audios = await self.synthesize_speakers_globally(
                speaker_translations=speaker_translations,
                speaker_voice_maps=speaker_voice_maps,
                target_language=target_language,
                message_id=message_id
            )

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 4: D√©tecter les silences depuis les segments originaux
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] üîá PHASE 4: D√©tection des silences")
            silences = self.silence_manager.detect_silences_from_segments(segments)

            # Enrichir les segments avec les silences
            enriched_segments = self.silence_manager.create_segments_with_silence(
                segments=segments,
                silences=silences
            )

            # Mettre √† jour les segments originaux avec les silences
            for i, seg in enumerate(segments):
                if i < len(enriched_segments):
                    seg['silence_before_ms'] = enriched_segments[i].silence_before_ms
                    seg['silence_after_ms'] = enriched_segments[i].silence_after_ms

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 5: Re-d√©couper l'audio de chaque speaker par segments
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] ‚úÇÔ∏è PHASE 5: Re-d√©coupage par segments")

            all_segment_results = {}

            for speaker_id, speaker_audio in speaker_audios.items():
                translation = speaker_translations[speaker_id]
                speaker_text = speakers_text[speaker_id]

                segment_results = await self.slice_speaker_audio_by_segments(
                    speaker_audio=speaker_audio,
                    speaker_translation=translation,
                    original_segments=speaker_text.original_segments
                )

                all_segment_results[speaker_id] = segment_results

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # PHASE 6: R√©assembler tous les segments dans l'ordre
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            logger.info("[MULTI_SPEAKER_SYNTH] üîó PHASE 6: R√©assemblage final")

            final_result = await self.reassemble_final_audio(
                all_segment_results=all_segment_results,
                output_path=output_path
            )

            if not final_result:
                logger.error("[MULTI_SPEAKER_SYNTH] ‚ùå √âchec du r√©assemblage")
                return None

            final_audio_path, total_duration_ms = final_result

            # Fusionner tous les segment results pour le retour
            all_results = []
            for results in all_segment_results.values():
                all_results.extend(results)
            all_results.sort(key=lambda x: x.segment_index)

            synthesis_time = int((time.time() - synthesis_start) * 1000)

            logger.info("=" * 80)
            logger.info("[MULTI_SPEAKER_SYNTH] ‚úÖ SYNTH√àSE GLOBALE TERMIN√âE")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Temps total: {synthesis_time}ms ({synthesis_time/1000:.1f}s)")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Dur√©e audio: {total_duration_ms}ms ({total_duration_ms/1000:.1f}s)")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îú‚îÄ Segments: {len(all_results)}")
            logger.info(f"[MULTI_SPEAKER_SYNTH]    ‚îî‚îÄ Fichier: {final_audio_path}")
            logger.info("=" * 80)

            return (final_audio_path, total_duration_ms, all_results)

        except Exception as e:
            logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Erreur synth√®se globale: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _synthesize_segments(
        self,
        enriched_segments: List[AudioSegmentWithSilence],
        speaker_voice_maps: Dict[str, SpeakerVoiceMap],
        target_language: str,
        message_id: str
    ) -> List[SegmentSynthesisResult]:
        """
        Synth√©tise tous les segments avec leurs voix respectives.

        OPTIMISATION: Synth√®se PARALL√àLE de tous les segments pour r√©duire le temps total.

        Args:
            enriched_segments: Segments enrichis avec info de silence
            speaker_voice_maps: Mapping speaker ‚Üí voice model
            target_language: Langue cible
            message_id: ID du message

        Returns:
            Liste des r√©sultats de synth√®se (ordonn√©s par index)
        """
        import asyncio

        logger.info(
            f"[MULTI_SPEAKER_SYNTH] ‚ö° PARALL√âLISATION: {len(enriched_segments)} segments"
        )

        async def synthesize_single_segment(i: int, seg: AudioSegmentWithSilence) -> SegmentSynthesisResult:
            """Synth√©tise un segment individuel (pour parall√©lisation)"""
            try:
                # R√©cup√©rer le voice model pour ce speaker
                speaker_map = speaker_voice_maps.get(seg.speaker_id)

                if not speaker_map:
                    logger.debug(
                        f"[MULTI_SPEAKER_SYNTH] Segment {i}: voix g√©n√©rique pour {seg.speaker_id}"
                    )

                # Synth√©tiser
                if speaker_map and speaker_map.voice_model:
                    tts_result = await self.tts_service.synthesize_with_voice(
                        text=seg.text,
                        speaker_audio_path=speaker_map.audio_reference_path,
                        target_language=target_language,
                        output_format="mp3",
                        message_id=f"{message_id}_seg_{i}"
                    )
                else:
                    # Voix g√©n√©rique
                    tts_result = await self.tts_service.synthesize(
                        text=seg.text,
                        language=target_language,
                        output_format="mp3"
                    )

                if tts_result and tts_result.audio_path:
                    return SegmentSynthesisResult(
                        segment_index=i,
                        speaker_id=seg.speaker_id,
                        text=seg.text,
                        audio_path=tts_result.audio_path,
                        duration_ms=tts_result.duration_ms,
                        silence_before_ms=seg.silence_before_ms,
                        silence_after_ms=seg.silence_after_ms,
                        success=True
                    )
                else:
                    raise Exception("TTS result is None or missing audio_path")

            except Exception as e:
                logger.error(f"[MULTI_SPEAKER_SYNTH] ‚ùå Segment {i}: {e}")
                return SegmentSynthesisResult(
                    segment_index=i,
                    speaker_id=seg.speaker_id,
                    text=seg.text,
                    audio_path="",
                    duration_ms=0,
                    silence_before_ms=seg.silence_before_ms,
                    silence_after_ms=seg.silence_after_ms,
                    success=False,
                    error_message=str(e)
                )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # SYNTH√àSE PARALL√àLE avec asyncio.gather()
        # Tous les segments sont synth√©tis√©s en m√™me temps!
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        tasks = [
            synthesize_single_segment(i, seg)
            for i, seg in enumerate(enriched_segments)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filtrer les exceptions et convertir en SegmentSynthesisResult
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"[MULTI_SPEAKER_SYNTH] Exception segment {i}: {result}")
                final_results.append(SegmentSynthesisResult(
                    segment_index=i,
                    speaker_id=enriched_segments[i].speaker_id,
                    text=enriched_segments[i].text,
                    audio_path="",
                    duration_ms=0,
                    silence_before_ms=enriched_segments[i].silence_before_ms,
                    silence_after_ms=enriched_segments[i].silence_after_ms,
                    success=False,
                    error_message=str(result)
                ))
            else:
                final_results.append(result)

        # Trier par index pour maintenir l'ordre
        final_results.sort(key=lambda x: x.segment_index)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # V√âRIFICATION COMPL√àTE: Garantir que TOUS les segments sont synth√©tis√©s
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        success_count = sum(1 for r in final_results if r.success)
        failed_count = len(final_results) - success_count

        logger.info("=" * 80)
        logger.info(f"[MULTI_SPEAKER_SYNTH] üìä R√âSUM√â SYNTH√àSE PARALL√àLE")
        logger.info(f"[MULTI_SPEAKER_SYNTH] Total segments: {len(final_results)}")
        logger.info(f"[MULTI_SPEAKER_SYNTH] ‚úÖ R√©ussis: {success_count} ({success_count/len(final_results)*100:.1f}%)")
        logger.info(f"[MULTI_SPEAKER_SYNTH] ‚ùå √âchou√©s: {failed_count} ({failed_count/len(final_results)*100:.1f}%)")

        if failed_count > 0:
            logger.warning(f"[MULTI_SPEAKER_SYNTH] ‚ö†Ô∏è ATTENTION: {failed_count} segment(s) NON synth√©tis√©(s)!")
            logger.warning("[MULTI_SPEAKER_SYNTH] Segments √©chou√©s:")
            for r in final_results:
                if not r.success:
                    logger.warning(
                        f"[MULTI_SPEAKER_SYNTH]   ‚Ä¢ Segment {r.segment_index}: "
                        f"'{r.text[:50]}...' ‚Üí {r.error_message}"
                    )
        else:
            logger.info(f"[MULTI_SPEAKER_SYNTH] ‚úÖ TOUS les segments ont √©t√© synth√©tis√©s avec succ√®s!")

        logger.info("=" * 80)

        return final_results

    async def cleanup_temp_files(self, synthesis_results: List[SegmentSynthesisResult]):
        """
        Nettoie les fichiers temporaires de synth√®se.

        Args:
            synthesis_results: R√©sultats de synth√®se contenant les chemins
        """
        for result in synthesis_results:
            if result.success and result.audio_path and os.path.exists(result.audio_path):
                try:
                    os.remove(result.audio_path)
                    logger.debug(f"[MULTI_SPEAKER_SYNTH] Nettoyage: {result.audio_path}")
                except Exception as e:
                    logger.warning(f"[MULTI_SPEAKER_SYNTH] Erreur nettoyage {result.audio_path}: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du synth√©tiseur"""
        return {
            "temp_dir": self.temp_dir,
            "silence_manager": self.silence_manager.get_stats()
        }


# Factory function
def create_multi_speaker_synthesizer(
    tts_service,
    voice_clone_service,
    preserve_silences: bool = True,
    temp_dir: str = "/tmp/multi_speaker_tts"
) -> MultiSpeakerSynthesizer:
    """
    Cr√©e une instance de MultiSpeakerSynthesizer.

    Args:
        tts_service: Service TTS
        voice_clone_service: Service de clonage vocal
        preserve_silences: Pr√©server les silences naturels
        temp_dir: R√©pertoire temporaire

    Returns:
        Instance de MultiSpeakerSynthesizer
    """
    silence_manager = create_silence_manager(preserve_silences=preserve_silences)

    return MultiSpeakerSynthesizer(
        tts_service=tts_service,
        voice_clone_service=voice_clone_service,
        silence_manager=silence_manager,
        temp_dir=temp_dir
    )
