"""
Multi-Speaker Audio Processor
==============================

Traite les audios multi-speakers en cr√©ant des TOURS DE PAROLE.

Architecture:
1. Extraire l'audio de chaque speaker (un fichier par speaker)
2. Cr√©er un voice_model par speaker (en utilisant leur audio extrait)
3. Comparer les voice_models et fusionner les speakers similaires
4. Cr√©er des "tours de parole" (segments cons√©cutifs du m√™me speaker)
5. Traiter chaque TOUR DE PAROLE avec la cha√Æne MONO-LOCUTEUR
   - Chaque tour utilise le voice_model de son speaker (r√©utilis√©)
6. Concat√©ner les tours traduits dans l'ordre original

Cette approche optimise le nombre d'appels TTS et pr√©serve l'ordre de la conversation.

Cache WAV:
- Les conversions M4A ‚Üí WAV sont mises en cache pendant 7 jours
- R√©pertoire: models/wav_cache
- Nettoyage automatique des fichiers expir√©s

"""

import os
import logging
import numpy as np
import time
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# R√©pertoire de cache WAV
WAV_CACHE_DIR = "/Users/smpceo/Documents/v2_meeshy/services/translator/models/wav_cache"
WAV_CACHE_MAX_AGE_DAYS = 7


def cleanup_wav_cache(cache_dir: str = WAV_CACHE_DIR, max_age_days: int = WAV_CACHE_MAX_AGE_DAYS) -> int:
    """
    Nettoie le cache WAV en supprimant les fichiers plus vieux que max_age_days.

    Args:
        cache_dir: R√©pertoire du cache
        max_age_days: √Çge maximum des fichiers en jours

    Returns:
        Nombre de fichiers supprim√©s
    """
    if not os.path.exists(cache_dir):
        return 0

    max_age_seconds = max_age_days * 86400
    current_time = time.time()
    removed_count = 0

    try:
        for filename in os.listdir(cache_dir):
            if not filename.endswith('.wav'):
                continue

            file_path = os.path.join(cache_dir, filename)
            file_age = current_time - os.path.getmtime(file_path)

            if file_age > max_age_seconds:
                try:
                    os.remove(file_path)
                    removed_count += 1
                    logger.info(
                        f"[WAV_CACHE] üóëÔ∏è  Supprim√©: {filename} "
                        f"(√¢ge: {file_age/86400:.1f}j > {max_age_days}j)"
                    )
                except Exception as e:
                    logger.warning(f"[WAV_CACHE] ‚ö†Ô∏è  Impossible de supprimer {filename}: {e}")

        if removed_count > 0:
            logger.info(f"[WAV_CACHE] ‚úÖ Nettoyage termin√©: {removed_count} fichier(s) supprim√©(s)")

    except Exception as e:
        logger.error(f"[WAV_CACHE] ‚ùå Erreur nettoyage cache: {e}")

    return removed_count


def get_wav_cache_stats(cache_dir: str = WAV_CACHE_DIR) -> Dict[str, Any]:
    """
    Retourne les statistiques du cache WAV.

    Returns:
        Dict avec: total_files, total_size_mb, oldest_file_age_days
    """
    if not os.path.exists(cache_dir):
        return {"total_files": 0, "total_size_mb": 0, "oldest_file_age_days": 0}

    total_files = 0
    total_size = 0
    oldest_age = 0
    current_time = time.time()

    try:
        for filename in os.listdir(cache_dir):
            if not filename.endswith('.wav'):
                continue

            file_path = os.path.join(cache_dir, filename)
            total_files += 1
            total_size += os.path.getsize(file_path)
            file_age = (current_time - os.path.getmtime(file_path)) / 86400
            oldest_age = max(oldest_age, file_age)

    except Exception as e:
        logger.error(f"[WAV_CACHE] ‚ùå Erreur stats cache: {e}")

    return {
        "total_files": total_files,
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "oldest_file_age_days": round(oldest_age, 1)
    }


@dataclass
class SpeakerData:
    """Donn√©es d'un speaker"""
    speaker_id: str
    segments: List[Dict[str, Any]]  # Segments de ce speaker
    full_text: str                   # Texte concat√©n√©
    audio_path: str                  # Audio extrait du speaker
    voice_model: Any                 # Voice model cr√©√©
    segment_positions: List[int]     # Positions globales des segments


@dataclass
class TurnOfSpeech:
    """Tour de parole : segments cons√©cutifs du m√™me speaker"""
    speaker_id: str
    text: str                        # Texte complet du tour
    segments: List[Dict[str, Any]]   # Segments constituant ce tour
    start_position: int              # Position du premier segment
    end_position: int                # Position du dernier segment


async def process_multi_speaker_audio(
    translation_stage,
    voice_clone_service,
    segments: List[Dict[str, Any]],
    source_audio_path: str,
    target_languages: List[str],
    source_language: str,
    message_id: str,
    attachment_id: str,
    user_voice_model: Optional[Any] = None,
    sender_speaker_id: Optional[str] = None,
    model_type: str = "premium",
    on_translation_ready: Optional[Any] = None
) -> Dict[str, Any]:
    """
    Traite un audio multi-speaker en cr√©ant des tours de parole.

    Processus:
    1. Groupe les segments par speaker
    2. Extrait l'audio de chaque speaker dans un fichier s√©par√©
    3. Cr√©e un voice_model par speaker (UNE SEULE FOIS, r√©utilis√© ensuite)
    4. Compare les voice_models et fusionne les speakers trop similaires
    5. Cr√©e des "tours de parole" (segments cons√©cutifs du m√™me speaker)
    6. Traite chaque TOUR DE PAROLE avec la cha√Æne mono-locuteur:
       - Traduit le texte complet du tour
       - Synth√©tise avec le voice_model du speaker (d√©j√† calcul√©)
    7. Concat√®ne tous les tours dans l'ordre original de la conversation
    8. **Re-transcription l√©g√®re** pour obtenir des segments fins:
       - Transcrit l'audio traduit avec Whisper (sans diarisation)
       - Mappe les speakers en utilisant les timestamps des tours
       - Remplace les segments grossiers par des segments fins pr√©cis
    9. **Callback imm√©diat** apr√®s chaque langue:
       - Remonte la traduction √† la gateway d√®s qu'elle est pr√™te
       - Permet une mise √† jour progressive de l'UI

    Avantages:
    - Voice_model cr√©√© une seule fois par speaker (pas recalcul√©)
    - Speakers similaires fusionn√©s automatiquement
    - Moins d'appels TTS (un par tour au lieu d'un par segment)
    - Meilleure continuit√© audio dans les tours de parole
    - Segments fins avec timestamps exacts (+30% overhead vs +80% avec diarisation)
    - Remont√©e progressive des r√©sultats (UX r√©active)

    Args:
        translation_stage: TranslationStage avec la cha√Æne mono-locuteur
        voice_clone_service: Service de clonage vocal
        segments: Segments avec speaker_id
        source_audio_path: Chemin audio source
        target_languages: Langues cibles
        source_language: Langue source
        message_id: ID message
        attachment_id: ID attachment
        user_voice_model: Voice model utilisateur (optionnel)
        sender_speaker_id: Speaker identifi√© comme exp√©diteur
        model_type: Type de mod√®le de traduction
        on_translation_ready: Callback appel√© apr√®s chaque traduction de langue
                             (permet remont√©e progressive √† la gateway)

    Returns:
        Dict avec les traductions par langue
    """
    logger.info(
        f"[MULTI_SPEAKER] üé≠ Traitement multi-speaker: "
        f"{len(segments)} segments, langues={target_languages}"
    )

    # Nettoyage automatique du cache WAV (fichiers > 7 jours)
    cleanup_wav_cache()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 1: GROUPER PAR SPEAKER
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    speakers_data = await _group_segments_by_speaker(segments)

    logger.info(f"[MULTI_SPEAKER] Speakers d√©tect√©s: {len(speakers_data)}")
    for speaker_id, data in speakers_data.items():
        logger.info(
            f"[MULTI_SPEAKER]   ‚Ä¢ {speaker_id}: "
            f"{len(data['segments'])} segments, "
            f"{len(data['full_text'])} chars"
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 2: EXTRAIRE L'AUDIO DE CHAQUE SPEAKER
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    for speaker_id, data in speakers_data.items():
        logger.info(f"[MULTI_SPEAKER] üé§ Extraction audio pour {speaker_id}...")

        audio_path = await _extract_speaker_audio(
            speaker_id=speaker_id,
            source_audio_path=source_audio_path,
            segments=data['segments']
        )

        if not audio_path:
            logger.error(f"[MULTI_SPEAKER] ‚ùå √âchec extraction audio pour {speaker_id}")
            return {}

        data['audio_path'] = audio_path
        logger.info(f"[MULTI_SPEAKER] ‚úÖ Audio extrait: {audio_path}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 3: CR√âER VOICE MODEL POUR CHAQUE SPEAKER
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    for speaker_id, data in speakers_data.items():
        logger.info(f"[MULTI_SPEAKER] üéôÔ∏è Cr√©ation voice model pour {speaker_id}...")

        # Si c'est l'exp√©diteur et qu'on a son mod√®le, l'utiliser
        if sender_speaker_id == speaker_id and user_voice_model:
            logger.info(f"[MULTI_SPEAKER] ‚úÖ Utilisation du mod√®le utilisateur pour {speaker_id}")
            data['voice_model'] = user_voice_model
            continue

        # Sinon cr√©er un mod√®le temporaire
        temp_user_id = f"temp_speaker_{speaker_id}"
        voice_model = await voice_clone_service.get_or_create_voice_model(
            user_id=temp_user_id,
            current_audio_path=data['audio_path'],
            current_audio_duration_ms=data['total_duration_ms']
        )

        if not voice_model:
            logger.error(f"[MULTI_SPEAKER] ‚ùå √âchec cr√©ation voice model pour {speaker_id}")
            return {}

        data['voice_model'] = voice_model
        logger.info(f"[MULTI_SPEAKER] ‚úÖ Voice model cr√©√© pour {speaker_id}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 4: FUSIONNER LES SPEAKERS SIMILAIRES
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    logger.info("[MULTI_SPEAKER] üîç Comparaison des voice_models...")
    speaker_mapping = await _merge_similar_speakers(
        speakers_data=speakers_data,
        similarity_threshold=0.85
    )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 5: CR√âER LES TOURS DE PAROLE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    logger.info("[MULTI_SPEAKER] üìã Cr√©ation des tours de parole...")
    turns_of_speech = _create_turns_of_speech(
        segments=segments,
        speaker_mapping=speaker_mapping
    )

    if not turns_of_speech:
        logger.error("[MULTI_SPEAKER] ‚ùå Aucun tour de parole cr√©√©")
        return {}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 6: TRAITER CHAQUE TOUR DE PAROLE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    translations = {}

    for target_lang in target_languages:
        logger.info(f"[MULTI_SPEAKER] üåê Traitement langue: {target_lang}")

        turn_translations = []

        # Traiter chaque tour de parole
        for turn_idx, turn in enumerate(turns_of_speech):
            speaker_id = turn.speaker_id

            # R√©cup√©rer le voice_model du speaker (d√©j√† calcul√©)
            speaker_data = speakers_data.get(speaker_id)
            if not speaker_data:
                logger.error(f"[MULTI_SPEAKER] ‚ùå Speaker {speaker_id} introuvable")
                continue

            voice_model = speaker_data['voice_model']

            logger.info(
                f"[MULTI_SPEAKER]   ‚Üí Tour {turn_idx+1}/{len(turns_of_speech)}: "
                f"Speaker {speaker_id}, {len(turn.text)} chars, "
                f"{len(turn.segments)} segments"
            )

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # UTILISER LA CHA√éNE MONO-LOCUTEUR POUR CE TOUR DE PAROLE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            import time
            lang_start = time.time()

            _, translation_result = await translation_stage._process_single_language_async(
                target_lang=target_lang,
                source_text=turn.text,
                source_language=source_language,
                audio_hash=f"{speaker_id}_{message_id}_{turn_idx}",
                voice_model=voice_model,
                message_id=message_id,
                attachment_id=f"{attachment_id}_{speaker_id}_{turn_idx}",
                model_type=model_type,
                cloning_params=None,
                lang_start=lang_start,
                source_audio_path=speaker_data['audio_path']
            )

            if not translation_result:
                logger.error(
                    f"[MULTI_SPEAKER] ‚ùå √âchec traduction tour {turn_idx+1}, "
                    f"speaker {speaker_id}, langue {target_lang}"
                )
                continue

            turn_translations.append({
                'speaker_id': speaker_id,
                'translation': translation_result,
                'turn': turn
            })

            logger.info(
                f"[MULTI_SPEAKER] ‚úÖ Tour {turn_idx+1} trait√©: "
                f"{translation_result.duration_ms}ms"
            )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # √âTAPE 7: CONCAT√âNER LES TOURS DE PAROLE DANS L'ORDRE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if turn_translations:
            logger.info(
                f"[MULTI_SPEAKER] üîó Concat√©nation de {len(turn_translations)} tours "
                f"pour {target_lang}..."
            )

            final_audio = await _concatenate_turns_in_order(
                turn_translations=turn_translations,
                target_lang=target_lang,
                message_id=message_id,
                attachment_id=attachment_id
            )

            if final_audio:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # √âTAPE 8: RE-TRANSCRIPTION L√âG√àRE POUR SEGMENTS FINS
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # Essayer la re-transcription, mais ne pas bloquer si elle √©choue
                fine_segments = None
                try:
                    logger.info(f"[MULTI_SPEAKER] üéØ Re-transcription pour segments fins ({target_lang})...")

                    # Extraire les m√©tadonn√©es des tours depuis turn_translations
                    turns_metadata = []
                    current_time_ms = 0
                    for turn_data in turn_translations:
                        translation = turn_data['translation']
                        turn = turn_data.get('turn')
                        if translation and turn:
                            # R√©cup√©rer les m√©tadonn√©es compl√®tes du speaker
                            speaker_data = speakers_data.get(turn.speaker_id, {})

                            turns_metadata.append({
                                'start_ms': current_time_ms,
                                'end_ms': current_time_ms + translation.duration_ms,
                                'speaker_id': turn.speaker_id,
                                'voice_similarity_score': speaker_data.get('voice_similarity_score')
                            })
                            current_time_ms += translation.duration_ms

                    # Re-transcrire avec mapping speakers
                    from .retranscription_service import retranscribe_translated_audio

                    fine_segments = await retranscribe_translated_audio(
                        audio_path=final_audio.audio_path,
                        target_language=target_lang,
                        turns_metadata=turns_metadata
                    )

                    # Remplacer les segments grossiers par les segments fins
                    final_audio.segments = fine_segments

                    logger.info(
                        f"[MULTI_SPEAKER] ‚úÖ {len(fine_segments)} segments fins "
                        f"au lieu de {len(turns_metadata)} segments grossiers"
                    )
                except Exception as e:
                    logger.warning(
                        f"[MULTI_SPEAKER] ‚ö†Ô∏è √âchec re-transcription pour {target_lang}: {e}"
                    )
                    # Fallback: cr√©er des segments grossiers bas√©s sur les tours
                    fine_segments = []
                    current_time_ms = 0
                    for turn_data in turn_translations:
                        translation = turn_data['translation']
                        turn = turn_data.get('turn')
                        if translation and turn:
                            speaker_data = speakers_data.get(turn.speaker_id, {})
                            fine_segments.append({
                                'text': translation.translated_text,
                                'startMs': current_time_ms,
                                'endMs': current_time_ms + translation.duration_ms,
                                'speakerId': turn.speaker_id,
                                'confidence': 0.9,  # Confiance par d√©faut
                                'voiceSimilarityScore': speaker_data.get('voice_similarity_score'),
                                'language': target_lang
                            })
                            current_time_ms += translation.duration_ms

                    final_audio.segments = fine_segments
                    logger.info(
                        f"[MULTI_SPEAKER] üìù Utilisation segments grossiers (fallback): "
                        f"{len(fine_segments)} segments bas√©s sur les tours"
                    )

                translations[target_lang] = final_audio

                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # √âTAPE 9: CALLBACK POUR REMONT√âE IMM√âDIATE √Ä LA GATEWAY
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                if on_translation_ready:
                    try:
                        logger.info(f"[MULTI_SPEAKER] üì§ Remont√©e traduction {target_lang} √† la gateway...")

                        # D√©terminer le type d'√©v√©nement selon le nombre de langues
                        total_languages = len(target_languages)
                        is_single_language = total_languages == 1
                        current_index = list(target_languages).index(target_lang) + 1
                        is_last_language = current_index == total_languages

                        translation_data = {
                            'message_id': message_id,
                            'attachment_id': attachment_id,
                            'language': target_lang,
                            'translation': final_audio,
                            'segments': fine_segments,
                            # M√©tadonn√©es pour d√©terminer le type d'√©v√©nement
                            'is_single_language': is_single_language,
                            'is_last_language': is_last_language,
                            'current_index': current_index,
                            'total_languages': total_languages
                        }

                        # Appeler le callback (peut √™tre async)
                        if asyncio.iscoroutinefunction(on_translation_ready):
                            await on_translation_ready(translation_data)
                        else:
                            on_translation_ready(translation_data)

                        logger.info(
                            f"[MULTI_SPEAKER] ‚úÖ Traduction {target_lang} remont√©e "
                            f"({current_index}/{total_languages})"
                        )
                    except Exception as e:
                        logger.warning(f"[MULTI_SPEAKER] ‚ö†Ô∏è Erreur callback traduction: {e}")

                logger.info(f"[MULTI_SPEAKER] ‚úÖ Langue {target_lang} compl√®te")
            else:
                logger.error(f"[MULTI_SPEAKER] ‚ùå √âchec concat√©nation pour {target_lang}")

    logger.info(
        f"[MULTI_SPEAKER] ‚úÖ Traitement termin√©: "
        f"{len(translations)}/{len(target_languages)} langues"
    )

    return translations


async def _group_segments_by_speaker(
    segments: List[Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """Groupe les segments par speaker"""
    speakers = {}

    for i, seg in enumerate(segments):
        speaker_id = seg.get('speaker_id', seg.get('speakerId', 'unknown'))
        text = seg.get('text', '')

        if not text.strip():
            continue

        if speaker_id not in speakers:
            speakers[speaker_id] = {
                'segments': [],
                'full_text': '',
                'segment_positions': [],
                'total_duration_ms': 0,
                'voice_similarity_score': None  # Sera extrait du premier segment
            }

        speakers[speaker_id]['segments'].append(seg)
        speakers[speaker_id]['segment_positions'].append(i)
        speakers[speaker_id]['full_text'] += ' ' + text
        speakers[speaker_id]['total_duration_ms'] += (
            seg.get('end_ms', seg.get('endMs', 0)) -
            seg.get('start_ms', seg.get('startMs', 0))
        )

        # Extraire voiceSimilarityScore du premier segment de ce speaker
        if speakers[speaker_id]['voice_similarity_score'] is None:
            speakers[speaker_id]['voice_similarity_score'] = seg.get(
                'voice_similarity_score',
                seg.get('voiceSimilarityScore')
            )

    # Nettoyer les espaces
    for speaker_id in speakers:
        speakers[speaker_id]['full_text'] = speakers[speaker_id]['full_text'].strip()

    return speakers


def _compare_voice_models(voice_model_1: Any, voice_model_2: Any) -> float:
    """
    Compare deux voice_models et retourne un score de similarit√© (0.0 √† 1.0).

    Utilise les caract√©ristiques vocales pour comparer les models.
    """
    try:
        # Extraire les caract√©ristiques vocales
        chars_1 = getattr(voice_model_1, 'voice_characteristics', None)
        chars_2 = getattr(voice_model_2, 'voice_characteristics', None)

        if not chars_1 or not chars_2:
            logger.warning("[MULTI_SPEAKER] ‚ö†Ô∏è Voice characteristics manquantes")
            return 0.0

        # Comparer pitch (hauteur de voix)
        pitch_1 = getattr(chars_1, 'pitch_mean', 0)
        pitch_2 = getattr(chars_2, 'pitch_mean', 0)

        if pitch_1 == 0 or pitch_2 == 0:
            return 0.0

        pitch_diff = abs(pitch_1 - pitch_2) / max(pitch_1, pitch_2)
        pitch_similarity = 1.0 - min(pitch_diff, 1.0)

        # Comparer √©nergie vocale
        energy_1 = getattr(chars_1, 'rms_energy', 0)
        energy_2 = getattr(chars_2, 'rms_energy', 0)

        if energy_1 > 0 and energy_2 > 0:
            energy_diff = abs(energy_1 - energy_2) / max(energy_1, energy_2)
            energy_similarity = 1.0 - min(energy_diff, 1.0)
        else:
            energy_similarity = 0.5

        # Score final (moyenne pond√©r√©e)
        similarity = 0.7 * pitch_similarity + 0.3 * energy_similarity

        logger.info(
            f"[MULTI_SPEAKER] üîç Similarit√© voice_models: {similarity:.2f} "
            f"(pitch: {pitch_similarity:.2f}, energy: {energy_similarity:.2f})"
        )

        return similarity

    except Exception as e:
        logger.error(f"[MULTI_SPEAKER] ‚ùå Erreur comparaison voice_models: {e}")
        return 0.0


async def _merge_similar_speakers(
    speakers_data: Dict[str, Dict[str, Any]],
    similarity_threshold: float = 0.85
) -> Dict[str, str]:
    """
    Fusionne les speakers avec voice_models similaires.

    Args:
        speakers_data: Donn√©es des speakers avec voice_models
        similarity_threshold: Seuil de similarit√© (0.85 = 85% similaire)

    Returns:
        Dict de mapping: speaker_id_original ‚Üí speaker_id_final
    """
    speaker_ids = list(speakers_data.keys())
    merged_mapping = {}  # speaker_id ‚Üí speaker_id_merged

    # Initialiser : chaque speaker pointe vers lui-m√™me
    for speaker_id in speaker_ids:
        merged_mapping[speaker_id] = speaker_id

    # Comparer tous les speakers entre eux
    for i, speaker_1 in enumerate(speaker_ids):
        for speaker_2 in speaker_ids[i+1:]:
            voice_model_1 = speakers_data[speaker_1].get('voice_model')
            voice_model_2 = speakers_data[speaker_2].get('voice_model')

            if not voice_model_1 or not voice_model_2:
                continue

            similarity = _compare_voice_models(voice_model_1, voice_model_2)

            if similarity >= similarity_threshold:
                logger.info(
                    f"[MULTI_SPEAKER] üîó Fusion: {speaker_2} ‚Üí {speaker_1} "
                    f"(similarit√©: {similarity:.2%})"
                )

                # Fusionner speaker_2 dans speaker_1
                merged_mapping[speaker_2] = speaker_1

                # Fusionner les donn√©es
                speakers_data[speaker_1]['segments'].extend(
                    speakers_data[speaker_2]['segments']
                )
                speakers_data[speaker_1]['segment_positions'].extend(
                    speakers_data[speaker_2]['segment_positions']
                )

    # Nombre de speakers apr√®s fusion
    unique_speakers = len(set(merged_mapping.values()))
    if unique_speakers < len(speaker_ids):
        logger.info(
            f"[MULTI_SPEAKER] ‚úÖ Fusion termin√©e: {len(speaker_ids)} ‚Üí {unique_speakers} speakers"
        )

    return merged_mapping


def _create_turns_of_speech(
    segments: List[Dict[str, Any]],
    speaker_mapping: Dict[str, str]
) -> List[TurnOfSpeech]:
    """
    Cr√©e des tours de parole en regroupant les segments cons√©cutifs du m√™me speaker.

    Args:
        segments: Liste des segments originaux
        speaker_mapping: Mapping speaker_id_original ‚Üí speaker_id_final

    Returns:
        Liste de TurnOfSpeech dans l'ordre original
    """
    turns = []
    current_turn = None

    for i, seg in enumerate(segments):
        speaker_id = seg.get('speaker_id', seg.get('speakerId', 'unknown'))
        text = seg.get('text', '').strip()

        if not text:
            continue

        # Appliquer le mapping de fusion
        final_speaker_id = speaker_mapping.get(speaker_id, speaker_id)

        # Si c'est le m√™me speaker que le tour en cours, ajouter au tour
        if current_turn and current_turn.speaker_id == final_speaker_id:
            current_turn.text += ' ' + text
            current_turn.segments.append(seg)
            current_turn.end_position = i
        else:
            # Nouveau tour de parole
            if current_turn:
                turns.append(current_turn)

            current_turn = TurnOfSpeech(
                speaker_id=final_speaker_id,
                text=text,
                segments=[seg],
                start_position=i,
                end_position=i
            )

    # Ajouter le dernier tour
    if current_turn:
        turns.append(current_turn)

    logger.info(
        f"[MULTI_SPEAKER] üìã Tours de parole cr√©√©s: {len(turns)} tours "
        f"pour {len(segments)} segments"
    )

    for i, turn in enumerate(turns):
        logger.info(
            f"[MULTI_SPEAKER]   ‚Ä¢ Tour {i+1}: Speaker {turn.speaker_id}, "
            f"{len(turn.segments)} segments, {len(turn.text)} chars"
        )

    return turns


async def _extract_speaker_audio(
    speaker_id: str,
    source_audio_path: str,
    segments: List[Dict[str, Any]]
) -> Optional[str]:
    """
    Extrait l'audio de R√âF√âRENCE d'un speaker pour le clonage vocal.

    IMPORTANT: On utilise uniquement le SEGMENT LE PLUS LONG du speaker,
    car c'est le meilleur √©chantillon pour capturer sa voix naturelle.

    Args:
        speaker_id: ID du speaker
        source_audio_path: Chemin audio source
        segments: Segments de ce speaker

    Returns:
        Chemin vers l'audio de r√©f√©rence (segment le plus long)
    """
    try:
        import soundfile as sf
        import numpy as np
        import subprocess
        import hashlib
        import time
        from pathlib import Path

        logger.info(f"[MULTI_SPEAKER] üìÇ Lecture audio source: {source_audio_path}")

        # V√©rifier si le fichier est en M4A/AAC et le convertir en WAV si n√©cessaire
        audio_path_to_read = source_audio_path
        temp_wav_path = None
        cache_used = False

        if source_audio_path.lower().endswith(('.m4a', '.aac', '.mp4')):
            # Calculer hash du fichier source pour le cache
            with open(source_audio_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()[:16]

            # R√©pertoire de cache persistant (7 jours)
            os.makedirs(WAV_CACHE_DIR, exist_ok=True)

            # Nom du fichier en cache
            cached_wav_filename = f"{file_hash}_{Path(source_audio_path).stem}.wav"
            cached_wav_path = os.path.join(WAV_CACHE_DIR, cached_wav_filename)

            # V√©rifier si conversion d√©j√† en cache
            if os.path.exists(cached_wav_path):
                # V√©rifier l'√¢ge du cache (7 jours = 604800 secondes)
                file_age = time.time() - os.path.getmtime(cached_wav_path)
                if file_age < 604800:  # 7 jours
                    logger.info(
                        f"[MULTI_SPEAKER] ‚ôªÔ∏è  Utilisation cache WAV existant "
                        f"(√¢ge: {file_age/3600:.1f}h): {cached_wav_filename}"
                    )
                    audio_path_to_read = cached_wav_path
                    temp_wav_path = cached_wav_path  # Pour r√©f√©rence, mais ne sera pas supprim√©
                    cache_used = True
                else:
                    logger.info(f"[MULTI_SPEAKER] üóëÔ∏è  Cache WAV expir√© (>{file_age/86400:.1f}j), reconversion n√©cessaire")
                    os.remove(cached_wav_path)

            # Conversion n√©cessaire
            if not cache_used:
                logger.info(f"[MULTI_SPEAKER] üîÑ Conversion M4A/AAC vers WAV avec cache...")

                # Convertir avec ffmpeg
                cmd = [
                    'ffmpeg', '-i', source_audio_path,
                    '-ar', '16000',  # Sample rate 16kHz
                    '-ac', '1',      # Mono
                    '-y',            # Overwrite
                    cached_wav_path
                ]

                result = subprocess.run(cmd, capture_output=True, timeout=30)

                if result.returncode != 0:
                    error_msg = result.stderr.decode() if result.stderr else "Unknown error"
                    logger.error(f"[MULTI_SPEAKER] ‚ùå Erreur conversion ffmpeg: {error_msg}")
                    return None

                audio_path_to_read = cached_wav_path
                temp_wav_path = cached_wav_path
                logger.info(f"[MULTI_SPEAKER] ‚úÖ Conversion r√©ussie et mise en cache (7j): {cached_wav_filename}")

        audio_data, sample_rate = sf.read(audio_path_to_read)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # TROUVER LE SEGMENT LE PLUS LONG (meilleur √©chantillon vocal)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        longest_segment = None
        longest_duration = 0

        for seg in segments:
            start_ms = seg.get('start_ms', seg.get('startMs', 0))
            end_ms = seg.get('end_ms', seg.get('endMs', 0))
            duration = end_ms - start_ms

            if duration > longest_duration:
                longest_duration = duration
                longest_segment = seg

        if not longest_segment:
            logger.error(f"[MULTI_SPEAKER] ‚ùå Aucun segment trouv√© pour {speaker_id}")
            return None

        # Extraire UNIQUEMENT le segment le plus long
        start_ms = longest_segment.get('start_ms', longest_segment.get('startMs', 0))
        end_ms = longest_segment.get('end_ms', longest_segment.get('endMs', 0))

        start_sample = int((start_ms / 1000.0) * sample_rate)
        end_sample = int((end_ms / 1000.0) * sample_rate)

        if start_sample >= len(audio_data) or end_sample > len(audio_data):
            logger.error(
                f"[MULTI_SPEAKER] ‚ùå Segment hors limites pour {speaker_id}: "
                f"{start_ms}-{end_ms}ms (audio: {len(audio_data)/sample_rate*1000:.0f}ms)"
            )
            return None

        reference_audio = audio_data[start_sample:end_sample]

        logger.info(
            f"[MULTI_SPEAKER] üéØ Segment de r√©f√©rence pour {speaker_id}: "
            f"{longest_duration}ms (le plus long parmi {len(segments)} segments)"
        )

        # Normaliser l'audio de r√©f√©rence
        max_val = np.max(np.abs(reference_audio))
        if max_val > 0:
            target_db = -20.0
            current_db = 20 * np.log10(max_val)
            gain_db = target_db - current_db
            gain_linear = 10 ** (gain_db / 20)
            reference_audio = reference_audio * gain_linear

        # Sauvegarder en WAV
        output_dir = "/tmp/multi_speaker_tts"
        os.makedirs(output_dir, exist_ok=True)

        # Remplacer l'extension par .wav
        source_basename = Path(source_audio_path).stem  # Nom sans extension
        output_path = os.path.join(
            output_dir,
            f"speaker_{speaker_id}_{source_basename}_ref.wav"  # _ref pour "r√©f√©rence"
        )

        sf.write(output_path, reference_audio, sample_rate)

        logger.info(
            f"[MULTI_SPEAKER] ‚úÖ Audio de r√©f√©rence extrait pour {speaker_id}: "
            f"{longest_duration}ms ‚Üí {output_path}"
        )

        # Ne PAS supprimer le fichier WAV en cache (r√©utilisable pendant 7 jours)
        # Le nettoyage se fera automatiquement apr√®s 7 jours lors de la prochaine utilisation

        return output_path

    except Exception as e:
        logger.error(f"[MULTI_SPEAKER] ‚ùå Erreur extraction audio: {e}")
        import traceback
        traceback.print_exc()

        # En cas d'erreur, ne pas supprimer le cache WAV (peut √™tre utile pour debug)

        return None


async def _concatenate_turns_in_order(
    turn_translations: List[Dict[str, Any]],
    target_lang: str,
    message_id: str,
    attachment_id: str
) -> Optional[Any]:
    """
    Concat√®ne les tours de parole traduits dans l'ordre original de la conversation.

    IMPORTANT: Utilise le re-encodage pour √©viter les glitchs et assurer
    une transition fluide entre les tours de parole.

    Args:
        turn_translations: Liste de dicts avec 'speaker_id', 'translation', 'turn'
        target_lang: Langue cible
        message_id: ID message
        attachment_id: ID attachment

    Returns:
        TranslatedAudioVersion avec l'audio final concat√©n√©
    """
    try:
        # Collecter les audio paths dans l'ordre
        audio_paths = []
        translated_texts = []
        translated_segments = []  # Segments traduits pour synchronisation
        total_duration_ms = 0

        for turn_data in turn_translations:
            translation = turn_data['translation']
            turn = turn_data.get('turn')

            if translation and translation.audio_path:
                audio_paths.append(translation.audio_path)
                translated_texts.append(translation.translated_text)

                # Cr√©er un segment traduit pour ce tour
                if turn:
                    translated_segments.append({
                        'text': translation.translated_text,
                        'startMs': total_duration_ms,
                        'endMs': total_duration_ms + translation.duration_ms,
                        'speakerId': turn.speaker_id,
                        'confidence': 1.0
                    })

                total_duration_ms += translation.duration_ms

        if not audio_paths:
            logger.error("[MULTI_SPEAKER] ‚ùå Aucun audio √† concat√©ner")
            return None

        logger.info(
            f"[MULTI_SPEAKER] üìù Concat√©nation PROPRE de {len(audio_paths)} tours "
            f"({total_duration_ms}ms total)"
        )

        # Concat√©ner avec ffmpeg - RE-ENCODAGE pour √©viter les glitchs
        from .translation_stage import TranslatedAudioVersion
        import subprocess
        from pathlib import Path

        # Utiliser un chemin absolu pour √©viter les probl√®mes de r√©pertoire de travail
        output_path = os.path.abspath(f"generated/audios/translated/{message_id}_{attachment_id}_{target_lang}_multi.mp3")
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        # Cr√©er liste de concat√©nation
        concat_list_path = f"/tmp/concat_list_{message_id}_{target_lang}.txt"
        with open(concat_list_path, 'w') as f:
            for audio_path in audio_paths:
                f.write(f"file '{os.path.abspath(audio_path)}'\n")

        # RE-ENCODAGE avec param√®tres uniformes pour √©viter tout glitch
        # - Normalisation du sample rate (44100 Hz standard)
        # - Mono pour coh√©rence
        # - Bitrate constant pour qualit√© uniforme
        cmd = [
            'ffmpeg', '-f', 'concat', '-safe', '0',
            '-i', concat_list_path,
            '-ar', '44100',          # Sample rate uniforme
            '-ac', '1',              # Mono
            '-b:a', '128k',          # Bitrate constant
            '-af', 'aresample=async=1:min_hard_comp=0.100000:first_pts=0',  # Resample propre
            '-y', output_path
        ]

        result = subprocess.run(cmd, capture_output=True, timeout=60)

        if result.returncode == 0 and os.path.exists(output_path):
            logger.info(
                f"[MULTI_SPEAKER] ‚úÖ Audio final: {output_path} | "
                f"{len(translated_segments)} segments traduits | "
                f"Taille: {os.path.getsize(output_path) / 1024:.1f}KB"
            )

            # Joindre tous les textes traduits
            full_translated_text = ' '.join(translated_texts)

            # Nettoyer le fichier de liste temporaire
            try:
                os.remove(concat_list_path)
            except:
                pass

            # Le fichier sera lu et envoy√© en binaire multipart par zmq_audio_handler.py
            # (voir ligne 673-679: lecture depuis audio_path et envoi via send_multipart)
            # Donc: pas besoin d'encoder en base64, juste fournir le chemin correct

            return TranslatedAudioVersion(
                language=target_lang,
                translated_text=full_translated_text,
                audio_path=output_path,
                audio_url=f"/audio/{os.path.basename(output_path)}",  # URL utilis√©e seulement si binaire √©choue
                duration_ms=total_duration_ms,
                format="mp3",
                voice_cloned=True,
                voice_quality=0.85,
                processing_time_ms=0,
                audio_data_base64=None,  # Pas de base64, on utilise le binaire multipart
                audio_mime_type="audio/mpeg",
                segments=translated_segments  # ‚úÖ Segments traduits pour synchronisation
            )
        else:
            error_msg = result.stderr.decode() if result.stderr else "Unknown error"
            logger.error(f"[MULTI_SPEAKER] ‚ùå Erreur ffmpeg: {error_msg}")
            return None

    except Exception as e:
        logger.error(f"[MULTI_SPEAKER] ‚ùå Erreur concat√©nation: {e}")
        import traceback
        traceback.print_exc()
        return None
