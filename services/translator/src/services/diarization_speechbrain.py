"""
Diarisation avec SpeechBrain (SANS token HuggingFace)
Alternative Ã  pyannote.audio qui fonctionne comme NLLB - tÃ©lÃ©chargement automatique sans authentification

Avantages:
- âœ… Aucun token HuggingFace requis
- âœ… TÃ©lÃ©chargement automatique (comme NLLB)
- âœ… ModÃ¨les publics
- âœ… Bonne qualitÃ© (ECAPA-TDNN embeddings)
- âœ… PrÃ©cision: ~85% (vs ~95% pour pyannote gated models)

Utilisation:
    from diarization_speechbrain import SpeechBrainDiarization

    diarizer = SpeechBrainDiarization()
    result = await diarizer.diarize(audio_path)
"""

import os
import logging
import numpy as np
import librosa
from typing import List, Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass

# Import SpeechBrain
try:
    from speechbrain.inference.speaker import EncoderClassifier
    SPEECHBRAIN_AVAILABLE = True
except ImportError:
    SPEECHBRAIN_AVAILABLE = False

# Import clustering
try:
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class SpeakerSegment:
    """Segment d'un locuteur"""
    speaker_id: str
    start_ms: int
    end_ms: int
    duration_ms: int
    confidence: float = 1.0


@dataclass
class SpeakerInfo:
    """Information sur un locuteur"""
    speaker_id: str
    is_primary: bool
    speaking_time_ms: int
    speaking_ratio: float
    segments: List[SpeakerSegment]
    voice_similarity_score: Optional[float] = None


@dataclass
class DiarizationResult:
    """RÃ©sultat de diarisation"""
    speaker_count: int
    speakers: List[SpeakerInfo]
    primary_speaker_id: str
    total_duration_ms: int
    method: str = "speechbrain"
    sender_identified: bool = False
    sender_speaker_id: Optional[str] = None


class SpeechBrainDiarization:
    """
    Diarisation des locuteurs avec SpeechBrain
    Fonctionne SANS token HuggingFace (comme NLLB)
    """

    def __init__(self, models_dir: Optional[str] = None):
        """
        Args:
            models_dir: RÃ©pertoire pour stocker les modÃ¨les (optionnel)
        """
        self.models_dir = models_dir or str(
            Path(__file__).parent.parent.parent / "models" / "speechbrain"
        )
        self._encoder = None

    def _get_encoder(self) -> "EncoderClassifier":
        """Charge le modÃ¨le d'embedding (lazy loading)"""
        if not SPEECHBRAIN_AVAILABLE:
            raise RuntimeError("SpeechBrain non disponible - pip install speechbrain")

        if self._encoder is None:
            logger.info("[SPEECHBRAIN] ğŸ”„ Chargement du modÃ¨le d'embeddings...")
            logger.info("[SPEECHBRAIN]    TÃ©lÃ©chargement automatique SANS token (comme NLLB)")

            # Utiliser le nouveau module (speechbrain.inference)
            self._encoder = EncoderClassifier.from_hparams(
                source="speechbrain/spkrec-ecapa-voxceleb",
                savedir=self.models_dir,
                run_opts={"device": "cpu"}  # CPU par dÃ©faut
            )

            logger.info("[SPEECHBRAIN] âœ… ModÃ¨le chargÃ© (ECAPA-TDNN)")

        return self._encoder

    async def diarize(
        self,
        audio_path: str,
        window_size_ms: int = 1500,  # FenÃªtre de 1.5s
        hop_size_ms: int = 750,       # Hop de 0.75s (50% overlap)
        max_speakers: int = 5
    ) -> DiarizationResult:
        """
        Diarise un fichier audio avec SpeechBrain

        Args:
            audio_path: Chemin vers le fichier audio
            window_size_ms: Taille de la fenÃªtre en ms
            hop_size_ms: Hop entre fenÃªtres en ms
            max_speakers: Nombre max de speakers

        Returns:
            DiarizationResult avec les speakers dÃ©tectÃ©s
        """
        logger.info(f"[SPEECHBRAIN] ğŸ¯ Diarisation de {audio_path}")

        # Charger l'audio
        audio, sr = librosa.load(audio_path, sr=16000)  # 16kHz pour SpeechBrain
        duration_ms = int(len(audio) / sr * 1000)

        # DÃ©couper en fenÃªtres et extraire embeddings
        window_samples = int(window_size_ms * sr / 1000)
        hop_samples = int(hop_size_ms * sr / 1000)

        embeddings = []
        timestamps = []

        encoder = self._get_encoder()

        for start_sample in range(0, len(audio) - window_samples, hop_samples):
            end_sample = start_sample + window_samples
            window = audio[start_sample:end_sample]

            # Extraire embedding
            import torch
            with torch.no_grad():
                wav_tensor = torch.tensor(window).unsqueeze(0)
                embedding = encoder.encode_batch(wav_tensor)
                embeddings.append(embedding.squeeze().cpu().numpy())

            # Sauvegarder timestamps
            start_ms = int(start_sample / sr * 1000)
            end_ms = int(end_sample / sr * 1000)
            timestamps.append((start_ms, end_ms))

        embeddings = np.array(embeddings)
        logger.info(f"[SPEECHBRAIN]    Extrait {len(embeddings)} embeddings")

        # Clustering des embeddings
        if not SKLEARN_AVAILABLE:
            raise RuntimeError("scikit-learn requis pour le clustering")

        # Trouver le nombre optimal de clusters
        best_n_clusters = 1
        best_score = -1

        if len(embeddings) >= 4:  # Minimum pour clustering
            for n in range(2, min(max_speakers + 1, len(embeddings) // 2)):
                clustering = AgglomerativeClustering(
                    n_clusters=n,
                    metric='cosine',
                    linkage='average'
                )
                labels = clustering.fit_predict(embeddings)

                # Score de silhouette (qualitÃ© du clustering)
                score = silhouette_score(embeddings, labels, metric='cosine')

                if score > best_score and score > 0.3:  # Seuil de qualitÃ©
                    best_score = score
                    best_n_clusters = n

        # Appliquer le clustering final
        if best_n_clusters > 1:
            clustering = AgglomerativeClustering(
                n_clusters=best_n_clusters,
                metric='cosine',
                linkage='average'
            )
            labels = clustering.fit_predict(embeddings)
            logger.info(f"[SPEECHBRAIN]    DÃ©tectÃ© {best_n_clusters} speakers (score={best_score:.3f})")
        else:
            labels = np.zeros(len(embeddings), dtype=int)
            logger.info(f"[SPEECHBRAIN]    1 seul speaker dÃ©tectÃ©")

        # Construire les segments par speaker
        speakers_data = {}

        for idx, (label, (start_ms, end_ms)) in enumerate(zip(labels, timestamps)):
            speaker_id = f"s{label}"

            if speaker_id not in speakers_data:
                speakers_data[speaker_id] = {
                    'segments': [],
                }

            segment = SpeakerSegment(
                speaker_id=speaker_id,
                start_ms=start_ms,
                end_ms=end_ms,
                duration_ms=end_ms - start_ms,
                confidence=1.0
            )

            speakers_data[speaker_id]['segments'].append(segment)

        # Calculer les durÃ©es rÃ©elles en fusionnant les overlaps
        # mais garder les segments originaux pour le tagging de transcription
        for speaker_id, data in speakers_data.items():
            # Trier par start_ms
            segments_sorted = sorted(data['segments'], key=lambda s: s.start_ms)

            # Fusionner les segments chevauchants pour calculer la durÃ©e RÃ‰ELLE
            merged_intervals = []
            current_start = None
            current_end = None

            for seg in segments_sorted:
                if current_start is None:
                    # Premier segment
                    current_start = seg.start_ms
                    current_end = seg.end_ms
                elif seg.start_ms <= current_end:
                    # Chevauchement ou consÃ©cutif: Ã©tendre
                    current_end = max(current_end, seg.end_ms)
                else:
                    # Gap: sauvegarder l'intervalle fusionnÃ©
                    merged_intervals.append((current_start, current_end))
                    current_start = seg.start_ms
                    current_end = seg.end_ms

            # Ajouter le dernier intervalle
            if current_start is not None:
                merged_intervals.append((current_start, current_end))

            # Calculer la durÃ©e totale (sans overlap)
            total_duration = sum(end - start for start, end in merged_intervals)

            # Garder les segments originaux (pour tagging) mais avec durÃ©e corrigÃ©e
            data['segments'] = segments_sorted
            data['total_duration_ms'] = total_duration

        # Filtrer les faux positifs (speakers avec <5% du temps OU <2 segments)
        MIN_SPEAKING_RATIO = 0.05
        MIN_SEGMENTS = 2

        speakers_filtered = {}
        for speaker_id, data in speakers_data.items():
            speaking_ratio = data['total_duration_ms'] / duration_ms if duration_ms > 0 else 0

            if speaking_ratio >= MIN_SPEAKING_RATIO or len(data['segments']) >= MIN_SEGMENTS:
                speakers_filtered[speaker_id] = data
            else:
                logger.info(
                    f"[SPEECHBRAIN]    FiltrÃ© {speaker_id}: "
                    f"{speaking_ratio*100:.1f}% temps, {len(data['segments'])} segments"
                )

        speakers_data = speakers_filtered

        # CrÃ©er SpeakerInfo
        speakers = []
        for speaker_id, data in speakers_data.items():
            speaking_ratio = data['total_duration_ms'] / duration_ms if duration_ms > 0 else 0
            speakers.append(SpeakerInfo(
                speaker_id=speaker_id,
                is_primary=False,
                speaking_time_ms=data['total_duration_ms'],
                speaking_ratio=speaking_ratio,
                segments=data['segments']
            ))

        # Identifier le speaker principal (qui parle le plus)
        if speakers:
            primary = max(speakers, key=lambda s: s.speaking_time_ms)
            primary.is_primary = True
            primary_speaker_id = primary.speaker_id
        else:
            primary_speaker_id = "s0"

        result = DiarizationResult(
            speaker_count=len(speakers),
            speakers=speakers,
            primary_speaker_id=primary_speaker_id,
            total_duration_ms=duration_ms,
            method="speechbrain"
        )

        # Logs dÃ©taillÃ©s
        logger.info("=" * 80)
        logger.info(f"[SPEECHBRAIN] ğŸ­ RÃ‰SULTAT DIARISATION")
        logger.info(f"[SPEECHBRAIN] Speakers dÃ©tectÃ©s: {result.speaker_count}")
        logger.info(f"[SPEECHBRAIN] DurÃ©e totale: {result.total_duration_ms}ms")
        logger.info(f"[SPEECHBRAIN] Speaker principal: {result.primary_speaker_id}")
        logger.info("=" * 80)

        for speaker in result.speakers:
            logger.info(
                f"[SPEECHBRAIN] ğŸ‘¤ {speaker.speaker_id} "
                f"({'PRINCIPAL' if speaker.is_primary else 'secondaire'}): "
                f"{speaker.speaking_time_ms}ms ({speaker.speaking_ratio*100:.1f}%) | "
                f"{len(speaker.segments)} segments"
            )

        logger.info("=" * 80)

        return result


# Singleton global
_diarizer = None

def get_speechbrain_diarization() -> SpeechBrainDiarization:
    """Retourne l'instance singleton"""
    global _diarizer
    if _diarizer is None:
        _diarizer = SpeechBrainDiarization()
    return _diarizer
