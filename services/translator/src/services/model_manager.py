"""
ModelManager - Gestionnaire centralis√© des mod√®les ML pour multi-utilisateurs

Ce module r√©sout le probl√®me critique de gestion m√©moire GPU/CPU:
- Chaque service (TTS, Translation, etc.) chargeait les mod√®les ind√©pendamment
- Risque d'OOM (Out Of Memory) quand plusieurs mod√®les sont charg√©s
- Pas de strat√©gie d'√©viction des mod√®les peu utilis√©s

Solution:
- Registre centralis√© de TOUS les mod√®les (Translation, TTS, STT, VoiceClone)
- Tracking de l'utilisation m√©moire GPU/CPU
- √âviction LRU (Least Recently Used) quand la m√©moire est faible
- Limites configurables par type de mod√®le
- Gestion centralis√©e des chemins de stockage des mod√®les
"""

import os
import gc
import time
import logging
import threading
from pathlib import Path
from typing import Dict, Optional, Any, List, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import OrderedDict
from functools import wraps

logger = logging.getLogger(__name__)

# V√©rifier si PyTorch est disponible
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorch non disponible - ModelManager en mode limit√©")


class ModelType(Enum):
    """Types de mod√®les support√©s"""
    TRANSLATION = "translation"  # NLLB, M2M100, etc.
    TTS = "tts"  # Chatterbox, XTTS, MMS-TTS, VITS
    STT = "stt"  # Whisper (faster-whisper)
    VOICE_CLONE = "voice_clone"  # OpenVoice, etc.
    EMBEDDING = "embedding"  # Mod√®les d'embedding vocal
    VOCODER = "vocoder"  # HiFi-GAN, etc.


class TTSBackend(Enum):
    """Backends TTS sp√©cifiques"""
    CHATTERBOX = "chatterbox"
    CHATTERBOX_TURBO = "chatterbox_turbo"
    HIGGS_AUDIO = "higgs_audio"
    XTTS = "xtts"
    MMS = "mms"  # Meta MMS-TTS (1100+ langues)
    VITS = "vits"


class STTBackend(Enum):
    """Backends STT sp√©cifiques"""
    WHISPER = "whisper"
    WHISPER_LARGE = "whisper_large_v3"
    WHISPER_MEDIUM = "whisper_medium"


class TranslationBackend(Enum):
    """Backends de traduction sp√©cifiques"""
    NLLB_600M = "nllb_600m"
    NLLB_1_3B = "nllb_1_3b"
    M2M100 = "m2m100"


@dataclass
class ModelInfo:
    """Information sur un mod√®le charg√©"""
    model_id: str  # Identifiant unique (ex: "translation_basic", "tts_chatterbox")
    model_type: ModelType
    model_name: str  # Nom HuggingFace ou chemin local
    model_object: Any  # L'objet mod√®le PyTorch/transformers
    memory_bytes: int  # M√©moire estim√©e en bytes
    device: str  # "cuda", "cpu", "mps"
    loaded_at: float = field(default_factory=time.time)
    last_used_at: float = field(default_factory=time.time)
    use_count: int = 0
    priority: int = 1  # 1=haute priorit√© (ne pas d√©charger), 2=normale, 3=basse
    # M√©tadonn√©es additionnelles
    backend: Optional[str] = None  # Ex: "chatterbox", "mms", "whisper_large_v3"
    language: Optional[str] = None  # Pour les mod√®les sp√©cifiques √† une langue (MMS)
    model_path: Optional[str] = None  # Chemin local du mod√®le
    extra_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MemoryConfig:
    """Configuration des limites m√©moire"""
    max_gpu_memory_fraction: float = 0.85  # Utiliser max 85% du GPU
    max_cpu_memory_gb: float = 16.0  # Limite RAM pour les mod√®les
    eviction_threshold: float = 0.80  # Commencer l'√©viction √† 80%
    min_free_gpu_mb: int = 1024  # Garder au moins 1GB libre sur GPU
    min_free_cpu_gb: float = 2.0  # Garder au moins 2GB libre sur CPU


@dataclass
class ModelPathsConfig:
    """Configuration centralis√©e des chemins de mod√®les"""
    base_path: str = field(default_factory=lambda: os.getenv("MODELS_PATH", "models"))

    @property
    def huggingface(self) -> Path:
        """Mod√®les HuggingFace (NLLB, Chatterbox, etc.)"""
        return Path(self.base_path) / "huggingface"

    @property
    def translation(self) -> Path:
        """Mod√®les de traduction NLLB"""
        return self.huggingface / "facebook"

    @property
    def tts_chatterbox(self) -> Path:
        """Mod√®les Chatterbox TTS"""
        return self.huggingface / "ResembleAI"

    @property
    def tts_mms(self) -> Path:
        """Mod√®les Meta MMS-TTS (langues africaines, etc.)"""
        return Path(self.base_path) / "mms"

    @property
    def tts_xtts(self) -> Path:
        """Mod√®les XTTS (Coqui)"""
        return Path(self.base_path) / "xtts"

    @property
    def tts_vits(self) -> Path:
        """Mod√®les VITS custom"""
        return Path(self.base_path) / "vits"

    @property
    def stt_whisper(self) -> Path:
        """Mod√®les Whisper (faster-whisper)"""
        return Path(self.base_path) / "whisper"

    @property
    def voice_clone(self) -> Path:
        """Mod√®les de clonage vocal (OpenVoice)"""
        return Path(self.base_path) / "openvoice"

    @property
    def voice_cache(self) -> Path:
        """Cache des profils vocaux utilisateurs"""
        return Path(self.base_path) / "voice_cache"

    @property
    def embeddings(self) -> Path:
        """Mod√®les d'embedding (sentence-transformers, etc.)"""
        return Path(self.base_path) / "embeddings"

    def ensure_all_exist(self):
        """Cr√©e tous les dossiers s'ils n'existent pas"""
        for attr_name in dir(self):
            if not attr_name.startswith('_') and attr_name not in ['base_path', 'ensure_all_exist']:
                try:
                    path = getattr(self, attr_name)
                    if isinstance(path, Path):
                        path.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    logger.warning(f"Impossible de cr√©er {attr_name}: {e}")

    def get_path_for_model(self, model_type: ModelType, backend: Optional[str] = None) -> Path:
        """Retourne le chemin appropri√© pour un type de mod√®le"""
        if model_type == ModelType.TRANSLATION:
            return self.translation
        elif model_type == ModelType.TTS:
            if backend == "mms":
                return self.tts_mms
            elif backend == "xtts":
                return self.tts_xtts
            elif backend == "vits":
                return self.tts_vits
            else:
                return self.tts_chatterbox
        elif model_type == ModelType.STT:
            return self.stt_whisper
        elif model_type == ModelType.VOICE_CLONE:
            return self.voice_clone
        elif model_type == ModelType.EMBEDDING:
            return self.embeddings
        else:
            return Path(self.base_path)


# Instance globale des chemins
_model_paths: Optional[ModelPathsConfig] = None


def get_model_paths() -> ModelPathsConfig:
    """Retourne la configuration des chemins de mod√®les"""
    global _model_paths
    if _model_paths is None:
        _model_paths = ModelPathsConfig()
        _model_paths.ensure_all_exist()
    return _model_paths


class ModelManager:
    """
    Gestionnaire centralis√© des mod√®les ML - Singleton

    Responsabilit√©s:
    1. Registre de tous les mod√®les charg√©s
    2. Tracking de la m√©moire utilis√©e
    3. √âviction LRU quand m√©moire faible
    4. API unifi√©e pour charger/d√©charger les mod√®les
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self, config: Optional[MemoryConfig] = None):
        if self._initialized:
            return

        self.config = config or MemoryConfig()

        # Registre des mod√®les charg√©s (OrderedDict pour LRU)
        self._models: OrderedDict[str, ModelInfo] = OrderedDict()
        self._models_lock = threading.RLock()

        # Callbacks pour les √©v√©nements
        self._on_model_loaded: List[Callable] = []
        self._on_model_unloaded: List[Callable] = []
        self._on_memory_pressure: List[Callable] = []

        # Stats
        self._stats = {
            'models_loaded': 0,
            'models_unloaded': 0,
            'evictions_triggered': 0,
            'total_memory_freed_mb': 0
        }

        # D√©tection du device
        self._device = self._detect_device()

        self._initialized = True
        logger.info(f"‚úÖ ModelManager initialis√© (device: {self._device})")

    def _detect_device(self) -> str:
        """D√©tecte le meilleur device disponible"""
        if not TORCH_AVAILABLE:
            return "cpu"

        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    def register_model(
        self,
        model_id: str,
        model_type: ModelType,
        model_name: str,
        model_object: Any,
        memory_bytes: Optional[int] = None,
        priority: int = 2,
        backend: Optional[str] = None,
        language: Optional[str] = None,
        model_path: Optional[str] = None,
        extra_info: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Enregistre un mod√®le dans le gestionnaire centralis√©.

        Args:
            model_id: Identifiant unique du mod√®le
            model_type: Type de mod√®le (TRANSLATION, TTS, etc.)
            model_name: Nom du mod√®le (pour logging)
            model_object: L'objet mod√®le PyTorch
            memory_bytes: M√©moire estim√©e (auto-d√©tect√©e si None)
            priority: 1=haute (ne pas d√©charger), 2=normale, 3=basse
            backend: Sous-type (ex: "chatterbox", "mms", "whisper")
            language: Code langue pour mod√®les sp√©cifiques (ex: "sw" pour MMS Swahili)
            model_path: Chemin local du mod√®le
            extra_info: M√©tadonn√©es additionnelles

        Returns:
            True si enregistr√© avec succ√®s
        """
        with self._models_lock:
            # V√©rifier si d√©j√† enregistr√©
            if model_id in self._models:
                logger.debug(f"[ModelManager] Mod√®le {model_id} d√©j√† enregistr√©, mise √† jour LRU")
                self._models[model_id].last_used_at = time.time()
                self._models.move_to_end(model_id)
                return True

            # V√©rifier si on a assez de m√©moire
            if not self._check_memory_available(memory_bytes or 0):
                # Tenter une √©viction
                self._evict_models_if_needed(memory_bytes or 0)

            # Estimer la m√©moire si non fournie
            if memory_bytes is None:
                memory_bytes = self._estimate_model_memory(model_object)

            # Cr√©er l'info du mod√®le
            model_info = ModelInfo(
                model_id=model_id,
                model_type=model_type,
                model_name=model_name,
                model_object=model_object,
                memory_bytes=memory_bytes,
                device=self._device,
                priority=priority,
                backend=backend,
                language=language,
                model_path=model_path,
                extra_info=extra_info or {}
            )

            # Enregistrer
            self._models[model_id] = model_info
            self._stats['models_loaded'] += 1

            # Stats par type
            type_key = f"{model_type.value}_{backend}" if backend else model_type.value
            if 'by_backend' not in self._stats:
                self._stats['by_backend'] = {}
            self._stats['by_backend'][type_key] = self._stats['by_backend'].get(type_key, 0) + 1

            logger.info(
                f"‚úÖ [ModelManager] Mod√®le enregistr√©: {model_id} "
                f"({model_type.value}/{backend or 'default'}, {memory_bytes / 1024 / 1024:.0f}MB, priority={priority})"
            )

            # Callbacks
            for callback in self._on_model_loaded:
                try:
                    callback(model_info)
                except Exception as e:
                    logger.error(f"Erreur callback on_model_loaded: {e}")

            return True

    def get_model(self, model_id: str) -> Optional[Any]:
        """
        R√©cup√®re un mod√®le et met √† jour son timestamp LRU.

        Args:
            model_id: Identifiant du mod√®le

        Returns:
            L'objet mod√®le ou None si non trouv√©
        """
        with self._models_lock:
            if model_id not in self._models:
                return None

            model_info = self._models[model_id]
            model_info.last_used_at = time.time()
            model_info.use_count += 1

            # D√©placer √† la fin pour LRU (mod√®le r√©cemment utilis√©)
            self._models.move_to_end(model_id)

            return model_info.model_object

    def unload_model(self, model_id: str) -> bool:
        """
        D√©charge un mod√®le et lib√®re sa m√©moire.

        Args:
            model_id: Identifiant du mod√®le

        Returns:
            True si d√©charg√© avec succ√®s
        """
        with self._models_lock:
            if model_id not in self._models:
                logger.warning(f"[ModelManager] Mod√®le non trouv√©: {model_id}")
                return False

            model_info = self._models.pop(model_id)
            memory_mb = model_info.memory_bytes / 1024 / 1024

            # Lib√©rer la m√©moire
            try:
                del model_info.model_object
            except Exception:
                pass

            self._cleanup_memory()

            self._stats['models_unloaded'] += 1
            self._stats['total_memory_freed_mb'] += memory_mb

            logger.info(f"üóëÔ∏è [ModelManager] Mod√®le d√©charg√©: {model_id} ({memory_mb:.0f}MB lib√©r√©s)")

            # Callbacks
            for callback in self._on_model_unloaded:
                try:
                    callback(model_info)
                except Exception as e:
                    logger.error(f"Erreur callback on_model_unloaded: {e}")

            return True

    def _check_memory_available(self, required_bytes: int) -> bool:
        """V√©rifie si assez de m√©moire est disponible"""
        if not TORCH_AVAILABLE:
            return True  # Pas de v√©rification possible

        try:
            if self._device == "cuda":
                # V√©rifier la m√©moire GPU
                total = torch.cuda.get_device_properties(0).total_memory
                allocated = torch.cuda.memory_allocated(0)
                free = total - allocated
                threshold = total * self.config.max_gpu_memory_fraction

                return (allocated + required_bytes) < threshold

            else:
                # V√©rifier la m√©moire CPU (approximatif)
                import psutil
                mem = psutil.virtual_memory()
                free_gb = mem.available / (1024 ** 3)

                return free_gb > self.config.min_free_cpu_gb

        except Exception as e:
            logger.warning(f"Erreur v√©rification m√©moire: {e}")
            return True

    def _evict_models_if_needed(self, required_bytes: int) -> int:
        """
        √âvicte les mod√®les LRU pour lib√©rer de la m√©moire.

        Returns:
            Nombre de mod√®les √©vict√©s
        """
        evicted = 0

        with self._models_lock:
            # Trier par priorit√© puis par last_used_at (LRU)
            candidates = sorted(
                [m for m in self._models.values() if m.priority > 1],  # Pas les priorit√© 1
                key=lambda m: (m.priority, m.last_used_at)  # Priorit√© basse + vieux = premier
            )

            bytes_freed = 0

            for model_info in candidates:
                if bytes_freed >= required_bytes:
                    break

                if self._check_memory_available(required_bytes - bytes_freed):
                    break

                # √âviction
                model_id = model_info.model_id
                memory = model_info.memory_bytes

                if self.unload_model(model_id):
                    bytes_freed += memory
                    evicted += 1
                    logger.warning(
                        f"‚ö†Ô∏è [ModelManager] √âviction LRU: {model_id} "
                        f"(last_used: {time.time() - model_info.last_used_at:.0f}s ago)"
                    )

        if evicted > 0:
            self._stats['evictions_triggered'] += 1
            # Callbacks
            for callback in self._on_memory_pressure:
                try:
                    callback(evicted, bytes_freed)
                except Exception:
                    pass

        return evicted

    def _estimate_model_memory(self, model: Any) -> int:
        """Estime la m√©moire utilis√©e par un mod√®le"""
        if not TORCH_AVAILABLE:
            return 0

        try:
            if hasattr(model, 'parameters'):
                # Mod√®le PyTorch
                total_params = sum(p.numel() * p.element_size() for p in model.parameters())
                # Ajouter buffers
                total_buffers = sum(b.numel() * b.element_size() for b in model.buffers())
                return total_params + total_buffers

            elif hasattr(model, 'model') and hasattr(model.model, 'parameters'):
                # Pipeline Hugging Face
                return self._estimate_model_memory(model.model)

            else:
                # Fallback: estimer √† 1GB
                return 1024 * 1024 * 1024

        except Exception:
            return 1024 * 1024 * 1024  # 1GB par d√©faut

    def _cleanup_memory(self):
        """Nettoie la m√©moire GPU/CPU"""
        gc.collect()

        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            except Exception:
                pass

    def get_memory_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques m√©moire"""
        stats = {
            'device': self._device,
            'models_count': len(self._models),
            'stats': self._stats.copy()
        }

        if TORCH_AVAILABLE and self._device == "cuda":
            try:
                stats['gpu'] = {
                    'total_mb': torch.cuda.get_device_properties(0).total_memory / 1024 / 1024,
                    'allocated_mb': torch.cuda.memory_allocated(0) / 1024 / 1024,
                    'cached_mb': torch.cuda.memory_reserved(0) / 1024 / 1024,
                    'free_mb': (torch.cuda.get_device_properties(0).total_memory -
                               torch.cuda.memory_allocated(0)) / 1024 / 1024
                }
            except Exception:
                pass

        # Stats par type de mod√®le
        with self._models_lock:
            by_type = {}
            for model_info in self._models.values():
                type_name = model_info.model_type.value
                if type_name not in by_type:
                    by_type[type_name] = {'count': 0, 'memory_mb': 0}
                by_type[type_name]['count'] += 1
                by_type[type_name]['memory_mb'] += model_info.memory_bytes / 1024 / 1024
            stats['by_type'] = by_type

        return stats

    def get_loaded_models(self) -> List[Dict[str, Any]]:
        """Retourne la liste des mod√®les charg√©s"""
        with self._models_lock:
            return [
                {
                    'model_id': m.model_id,
                    'model_type': m.model_type.value,
                    'model_name': m.model_name,
                    'memory_mb': m.memory_bytes / 1024 / 1024,
                    'device': m.device,
                    'use_count': m.use_count,
                    'last_used_ago_s': time.time() - m.last_used_at,
                    'priority': m.priority
                }
                for m in self._models.values()
            ]

    def has_model(self, model_id: str) -> bool:
        """V√©rifie si un mod√®le est charg√©"""
        with self._models_lock:
            return model_id in self._models

    def get_model_info(self, model_id: str) -> Optional[ModelInfo]:
        """Retourne les infos d'un mod√®le"""
        with self._models_lock:
            return self._models.get(model_id)

    def on_model_loaded(self, callback: Callable[[ModelInfo], None]):
        """Enregistre un callback appel√© quand un mod√®le est charg√©"""
        self._on_model_loaded.append(callback)

    def on_model_unloaded(self, callback: Callable[[ModelInfo], None]):
        """Enregistre un callback appel√© quand un mod√®le est d√©charg√©"""
        self._on_model_unloaded.append(callback)

    def on_memory_pressure(self, callback: Callable[[int, int], None]):
        """Enregistre un callback appel√© lors d'une √©viction (evicted_count, bytes_freed)"""
        self._on_memory_pressure.append(callback)

    def force_cleanup(self):
        """Force un nettoyage m√©moire"""
        self._cleanup_memory()
        logger.info("üßπ [ModelManager] Nettoyage m√©moire forc√©")

    def shutdown(self):
        """Arr√™te le ModelManager et lib√®re tous les mod√®les"""
        logger.info("üõë [ModelManager] Arr√™t et lib√©ration des mod√®les...")

        with self._models_lock:
            model_ids = list(self._models.keys())
            for model_id in model_ids:
                try:
                    self.unload_model(model_id)
                except Exception as e:
                    logger.error(f"Erreur d√©chargement {model_id}: {e}")

        self._cleanup_memory()
        logger.info("‚úÖ [ModelManager] Arr√™t termin√©")


# Instance globale (Singleton)
_model_manager: Optional[ModelManager] = None


def get_model_manager(config: Optional[MemoryConfig] = None) -> ModelManager:
    """Retourne l'instance unique du ModelManager"""
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelManager(config)
    return _model_manager


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# M√âTHODES D'AIDE POUR L'INT√âGRATION DES SERVICES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def register_tts_model(
    model_id: str,
    model_object: Any,
    backend: str,
    model_name: Optional[str] = None,
    memory_bytes: Optional[int] = None,
    language: Optional[str] = None,
    priority: int = 2
) -> bool:
    """
    Enregistre un mod√®le TTS dans le ModelManager.

    Args:
        model_id: ID unique (ex: "tts_mms_sw" pour MMS Swahili)
        model_object: L'objet mod√®le TTS
        backend: Backend utilis√© (chatterbox, mms, xtts, vits)
        model_name: Nom du mod√®le
        memory_bytes: M√©moire estim√©e
        language: Code langue (pour MMS)
        priority: 1=haute, 2=normale, 3=basse

    Returns:
        True si enregistr√©
    """
    manager = get_model_manager()
    paths = get_model_paths()

    return manager.register_model(
        model_id=model_id,
        model_type=ModelType.TTS,
        model_name=model_name or f"TTS-{backend}",
        model_object=model_object,
        memory_bytes=memory_bytes,
        priority=priority,
        backend=backend,
        language=language,
        model_path=str(paths.get_path_for_model(ModelType.TTS, backend))
    )


def register_stt_model(
    model_id: str,
    model_object: Any,
    backend: str = "whisper",
    model_name: Optional[str] = None,
    memory_bytes: Optional[int] = None,
    priority: int = 1  # STT g√©n√©ralement haute priorit√©
) -> bool:
    """
    Enregistre un mod√®le STT (Whisper) dans le ModelManager.

    Args:
        model_id: ID unique (ex: "stt_whisper_large_v3")
        model_object: L'objet mod√®le Whisper
        backend: Backend utilis√© (whisper)
        model_name: Nom du mod√®le
        memory_bytes: M√©moire estim√©e
        priority: 1=haute par d√©faut car STT est critique

    Returns:
        True si enregistr√©
    """
    manager = get_model_manager()
    paths = get_model_paths()

    return manager.register_model(
        model_id=model_id,
        model_type=ModelType.STT,
        model_name=model_name or f"STT-{backend}",
        model_object=model_object,
        memory_bytes=memory_bytes,
        priority=priority,
        backend=backend,
        model_path=str(paths.stt_whisper)
    )


def register_translation_model(
    model_id: str,
    model_object: Any,
    backend: str,
    model_name: Optional[str] = None,
    memory_bytes: Optional[int] = None,
    priority: int = 1  # Translation haute priorit√©
) -> bool:
    """
    Enregistre un mod√®le de traduction dans le ModelManager.

    Args:
        model_id: ID unique (ex: "translation_nllb_600m")
        model_object: L'objet mod√®le de traduction
        backend: Backend utilis√© (nllb_600m, nllb_1_3b, m2m100)
        model_name: Nom du mod√®le
        memory_bytes: M√©moire estim√©e
        priority: 1=haute par d√©faut

    Returns:
        True si enregistr√©
    """
    manager = get_model_manager()
    paths = get_model_paths()

    return manager.register_model(
        model_id=model_id,
        model_type=ModelType.TRANSLATION,
        model_name=model_name or f"Translation-{backend}",
        model_object=model_object,
        memory_bytes=memory_bytes,
        priority=priority,
        backend=backend,
        model_path=str(paths.translation)
    )


def register_voice_clone_model(
    model_id: str,
    model_object: Any,
    backend: str = "openvoice",
    model_name: Optional[str] = None,
    memory_bytes: Optional[int] = None,
    priority: int = 2
) -> bool:
    """
    Enregistre un mod√®le de clonage vocal dans le ModelManager.

    Args:
        model_id: ID unique (ex: "voice_clone_openvoice_v2")
        model_object: L'objet mod√®le de clonage
        backend: Backend utilis√© (openvoice)
        model_name: Nom du mod√®le
        memory_bytes: M√©moire estim√©e
        priority: 2=normale par d√©faut

    Returns:
        True si enregistr√©
    """
    manager = get_model_manager()
    paths = get_model_paths()

    return manager.register_model(
        model_id=model_id,
        model_type=ModelType.VOICE_CLONE,
        model_name=model_name or f"VoiceClone-{backend}",
        model_object=model_object,
        memory_bytes=memory_bytes,
        priority=priority,
        backend=backend,
        model_path=str(paths.voice_clone)
    )


def get_tts_model(model_id: str) -> Optional[Any]:
    """R√©cup√®re un mod√®le TTS par son ID"""
    return get_model_manager().get_model(model_id)


def get_stt_model(model_id: str) -> Optional[Any]:
    """R√©cup√®re un mod√®le STT par son ID"""
    return get_model_manager().get_model(model_id)


def get_models_by_type(model_type: ModelType, backend: Optional[str] = None) -> List[ModelInfo]:
    """
    R√©cup√®re tous les mod√®les d'un type donn√©.

    Args:
        model_type: Type de mod√®le
        backend: Filtrer par backend (optionnel)

    Returns:
        Liste des ModelInfo correspondants
    """
    manager = get_model_manager()
    with manager._models_lock:
        models = [m for m in manager._models.values() if m.model_type == model_type]
        if backend:
            models = [m for m in models if m.backend == backend]
        return models


def get_mms_model_for_language(language_code: str) -> Optional[Any]:
    """
    R√©cup√®re le mod√®le MMS-TTS pour une langue sp√©cifique.

    Args:
        language_code: Code langue ISO (ex: "sw", "am", "ha")

    Returns:
        Mod√®le MMS ou None
    """
    model_id = f"tts_mms_{language_code}"
    return get_model_manager().get_model(model_id)


def unload_models_by_type(model_type: ModelType, keep_priority_1: bool = True) -> int:
    """
    D√©charge tous les mod√®les d'un type donn√©.

    Args:
        model_type: Type de mod√®le √† d√©charger
        keep_priority_1: Garder les mod√®les priorit√© 1

    Returns:
        Nombre de mod√®les d√©charg√©s
    """
    manager = get_model_manager()
    unloaded = 0

    with manager._models_lock:
        to_unload = [
            m.model_id for m in manager._models.values()
            if m.model_type == model_type and (not keep_priority_1 or m.priority > 1)
        ]

    for model_id in to_unload:
        if manager.unload_model(model_id):
            unloaded += 1

    return unloaded


def print_model_summary():
    """Affiche un r√©sum√© des mod√®les charg√©s"""
    manager = get_model_manager()
    stats = manager.get_memory_stats()

    logger.info("=" * 60)
    logger.info("üìä R√âSUM√â DES MOD√àLES CHARG√âS")
    logger.info("=" * 60)
    logger.info(f"Device: {stats['device']}")
    logger.info(f"Mod√®les charg√©s: {stats['models_count']}")

    if 'gpu' in stats:
        gpu = stats['gpu']
        logger.info(f"GPU: {gpu['allocated_mb']:.0f}MB / {gpu['total_mb']:.0f}MB utilis√©s")

    if 'by_type' in stats:
        logger.info("-" * 40)
        for type_name, info in stats['by_type'].items():
            logger.info(f"  {type_name}: {info['count']} mod√®les, {info['memory_mb']:.0f}MB")

    logger.info("=" * 60)
