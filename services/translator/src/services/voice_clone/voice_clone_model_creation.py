"""
Module de cr√©ation de mod√®les vocaux pour le clonage de voix.

Responsabilit√©s:
- R√©cup√©rer ou cr√©er un mod√®le vocal pour un utilisateur
- Cr√©er depuis un profil Gateway existant
- Logique centrale de cr√©ation d'embedding OpenVoice
- Validation et v√©rification de qualit√©

Architecture:
- D√©l√©gation vers VoiceCloneAudioProcessor pour traitement audio
- D√©l√©gation vers VoiceCloneCacheManager pour cache
- D√©l√©gation vers VoiceAnalyzer pour analyse vocale
"""

import os
import logging
import asyncio
# numpy remplace pickle pour la s√©curit√©
import time
import uuid as uuid_module
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np

from .voice_metadata import VoiceModel
from .voice_analyzer import get_voice_analyzer
from models.voice_models import VoiceCharacteristics
from .voice_fingerprint import VoiceFingerprint
from utils.audio_format_converter import convert_to_wav_if_needed

logger = logging.getLogger(__name__)


class VoiceCloneModelCreator:
    """
    Gestionnaire de cr√©ation de mod√®les vocaux.

    Responsable de:
    - Cr√©ation de nouveaux mod√®les vocaux depuis des audios
    - Conversion de profils Gateway en VoiceModel
    - Validation et v√©rification de qualit√©
    - Gestion du cycle de vie des mod√®les (cr√©ation, am√©lioration)
    """

    # Configuration
    MIN_AUDIO_DURATION_MS = 10_000  # 10 secondes minimum
    VOICE_MODEL_MAX_AGE_DAYS_DEFAULT = 7  # 7 jours par d√©faut (production hebdomadaire)

    def __init__(
        self,
        audio_processor,
        cache_manager,
        voice_cache_dir: Path,
        max_age_days: Optional[int] = None
    ):
        """
        Initialise le cr√©ateur de mod√®les vocaux.

        Args:
            audio_processor: Instance de VoiceCloneAudioProcessor
            cache_manager: Instance de VoiceCloneCacheManager
            voice_cache_dir: R√©pertoire de cache des mod√®les
            max_age_days: √Çge maximum d'un mod√®le avant recalibration (d√©faut: 7 jours)
        """
        self._audio_processor = audio_processor
        self._cache_manager = cache_manager
        self.voice_cache_dir = voice_cache_dir

        # Utiliser max_age_days si fourni, sinon lire depuis env, sinon 7 jours
        if max_age_days is not None:
            self.VOICE_MODEL_MAX_AGE_DAYS = max_age_days
        else:
            self.VOICE_MODEL_MAX_AGE_DAYS = int(
                os.getenv('VOICE_MODEL_MAX_AGE_DAYS', str(self.VOICE_MODEL_MAX_AGE_DAYS_DEFAULT))
            )

        logger.info(
            f"[MODEL_CREATOR] Initialis√©: cache_dir={voice_cache_dir}, "
            f"max_age={self.VOICE_MODEL_MAX_AGE_DAYS}j"
        )

    async def get_or_create_voice_model(
        self,
        user_id: str,
        current_audio_path: Optional[str] = None,
        current_audio_duration_ms: int = 0
    ) -> VoiceModel:
        """
        R√©cup√®re ou cr√©e un mod√®le de voix pour un utilisateur.

        Logique:
        1. Si mod√®le en cache et r√©cent ‚Üí utiliser
        2. Si mod√®le en cache mais ancien ‚Üí am√©liorer avec nouvel audio
        3. Si pas de mod√®le et audio trop court ‚Üí agr√©ger historique
        4. Cr√©er nouveau mod√®le

        Environnement:
        - Development: Recalcul forc√© si audio actuel > 20 secondes
        - Production: Recalcul hebdomadaire (tous les 7 jours)

        Args:
            user_id: ID de l'utilisateur
            current_audio_path: Audio actuel pour le clonage (optionnel)
            current_audio_duration_ms: Dur√©e de l'audio actuel

        Returns:
            VoiceModel pr√™t √† l'emploi
        """
        # D√©terminer l'environnement
        environment = os.getenv('ENVIRONMENT', os.getenv('NODE_ENV', 'production')).lower()
        is_development = environment in ['development', 'dev']

        # Dur√©e minimale pour recalcul en d√©veloppement (20 secondes)
        dev_recalc_threshold_ms = 20_000

        # 1. V√©rifier le cache
        cached_model = await self._cache_manager.load_cached_model(user_id)

        if cached_model:
            age_days = (datetime.now() - cached_model.updated_at).days

            # En d√©veloppement: forcer le recalcul si audio actuel > 20s
            if is_development and current_audio_path and current_audio_duration_ms >= dev_recalc_threshold_ms:
                logger.info(
                    f"[MODEL_CREATOR] üîÑ Mode DEV: Recalcul forc√© pour {user_id} "
                    f"(audio {current_audio_duration_ms}ms > {dev_recalc_threshold_ms}ms)"
                )
                return await self._improve_model(cached_model, current_audio_path)

            # Mod√®le r√©cent ‚Üí utiliser directement
            if age_days < self.VOICE_MODEL_MAX_AGE_DAYS:
                logger.info(
                    f"[MODEL_CREATOR] üì¶ Mod√®le en cache pour {user_id} "
                    f"(age: {age_days}j)"
                )

                # Charger l'embedding si pas en m√©moire
                if cached_model.embedding is None:
                    cached_model = await self._cache_manager.load_embedding(
                        cached_model
                    )

                return cached_model

            # Mod√®le ancien ‚Üí am√©liorer si on a un nouvel audio
            if current_audio_path:
                logger.info(
                    f"[MODEL_CREATOR] üîÑ Mod√®le obsol√®te pour {user_id}, "
                    f"am√©lioration..."
                )
                return await self._improve_model(cached_model, current_audio_path)

            # Sinon utiliser l'ancien mod√®le
            logger.info(
                f"[MODEL_CREATOR] ‚ö†Ô∏è Mod√®le obsol√®te pour {user_id} "
                f"mais pas de nouvel audio"
            )
            if cached_model.embedding is None:
                cached_model = await self._cache_manager.load_embedding(
                    cached_model
                )
            return cached_model

        # 2. Pas de mod√®le ‚Üí cr√©er
        if not current_audio_path:
            # Essayer de r√©cup√©rer l'historique audio
            audio_paths = await self._audio_processor.get_user_audio_history(
                user_id
            )
            if not audio_paths:
                raise ValueError(
                    f"Aucun audio disponible pour cr√©er le mod√®le de voix "
                    f"de {user_id}"
                )
            current_audio_path = audio_paths[0]
            current_audio_duration_ms = await self._audio_processor.get_audio_duration_ms(
                current_audio_path
            )

        audio_paths = [current_audio_path]
        total_duration = current_audio_duration_ms

        # Si audio trop court, chercher l'historique
        if total_duration < self.MIN_AUDIO_DURATION_MS:
            logger.info(
                f"[MODEL_CREATOR] ‚ö†Ô∏è Audio trop court ({total_duration}ms), "
                f"agr√©gation historique..."
            )
            historical_audios = await self._audio_processor.get_user_audio_history(
                user_id, exclude=[current_audio_path]
            )
            audio_paths.extend(historical_audios)
            total_duration = await self._audio_processor.calculate_total_duration(
                audio_paths
            )

            logger.info(
                f"[MODEL_CREATOR] üìö {len(audio_paths)} audios agr√©g√©s, "
                f"total: {total_duration}ms"
            )

        # Cr√©er le mod√®le avec ce qu'on a
        return await self._create_voice_model(user_id, audio_paths, total_duration)

    async def create_voice_model_from_gateway_profile(
        self,
        profile_data: Dict[str, Any],
        user_id: str
    ) -> Optional[VoiceModel]:
        """
        Cr√©e un VoiceModel √† partir du profil vocal re√ßu de Gateway.

        Cette m√©thode permet √† Gateway d'envoyer un profil vocal existant
        (par exemple celui de l'√©metteur original d'un message transf√©r√©)
        sans que Translator ait besoin d'acc√©der √† MongoDB.

        Args:
            profile_data: Donn√©es du profil vocal envoy√©es par Gateway:
                - profileId: str - ID unique du profil
                - userId: str - ID de l'utilisateur propri√©taire du profil
                - embedding: str - Embedding Base64 encoded (numpy array)
                - qualityScore: float - Score de qualit√© 0-1
                - fingerprint: Dict - Empreinte vocale (optionnel)
                - voiceCharacteristics: Dict - Caract√©ristiques vocales (optionnel)
                - version: int - Version du profil
                - audioCount: int - Nombre d'audios agr√©g√©s
                - totalDurationMs: int - Dur√©e totale des audios

            user_id: ID de l'utilisateur (pour logs)

        Returns:
            VoiceModel pr√™t √† l'emploi, ou None si √©chec
        """
        if not profile_data:
            logger.warning(
                f"[MODEL_CREATOR] ‚ö†Ô∏è Pas de profil fourni par Gateway "
                f"pour {user_id}"
            )
            return None

        try:
            logger.info(
                f"[MODEL_CREATOR] üì¶ Cr√©ation VoiceModel depuis profil Gateway "
                f"pour {user_id}"
            )

            # D√©coder l'embedding Base64
            import base64
            embedding_base64 = profile_data.get('embedding')
            if not embedding_base64:
                logger.error(
                    f"[MODEL_CREATOR] ‚ùå Embedding manquant dans le profil Gateway"
                )
                return None

            embedding_bytes = base64.b64decode(embedding_base64)
            embedding = np.frombuffer(embedding_bytes, dtype=np.float32)

            logger.info(
                f"[MODEL_CREATOR] ‚úÖ Embedding d√©cod√©: shape={embedding.shape}"
            )

            # Cr√©er les caract√©ristiques vocales si fournies
            voice_characteristics = None
            voice_chars_data = profile_data.get('voiceCharacteristics')
            if voice_chars_data:
                try:
                    voice_characteristics = VoiceCharacteristics(
                        pitch_mean_hz=voice_chars_data.get('pitch_mean_hz', 0),
                        pitch_std_hz=voice_chars_data.get('pitch_std_hz', 0),
                        pitch_range_hz=voice_chars_data.get('pitch_range_hz', (0, 0)),
                        estimated_gender=voice_chars_data.get('estimated_gender', 'unknown'),
                        speaking_rate_wpm=voice_chars_data.get('speaking_rate_wpm', 0),
                        spectral_centroid_hz=voice_chars_data.get('spectral_centroid_hz', 0),
                        spectral_bandwidth_hz=voice_chars_data.get('spectral_bandwidth_hz', 0),
                        energy_mean=voice_chars_data.get('energy_mean', 0),
                        energy_std=voice_chars_data.get('energy_std', 0),
                        mfcc_signature=voice_chars_data.get('mfcc_signature'),
                        formants_hz=voice_chars_data.get('formants_hz'),
                        jitter_percent=voice_chars_data.get('jitter_percent'),
                        shimmer_percent=voice_chars_data.get('shimmer_percent'),
                        confidence=voice_chars_data.get('confidence', 0.8)
                    )
                except Exception as e:
                    logger.warning(
                        f"[MODEL_CREATOR] ‚ö†Ô∏è Impossible de recr√©er "
                        f"VoiceCharacteristics: {e}"
                    )

            # Cr√©er l'empreinte vocale si fournie
            fingerprint = None
            fingerprint_data = profile_data.get('fingerprint')
            if fingerprint_data:
                try:
                    fingerprint = VoiceFingerprint(
                        fingerprint_id=fingerprint_data.get('fingerprint_id', ''),
                        signature=fingerprint_data.get('signature', ''),
                        signature_short=fingerprint_data.get('signature_short', ''),
                        audio_duration_ms=fingerprint_data.get('audio_duration_ms', 0),
                        created_at=datetime.fromisoformat(
                            fingerprint_data.get(
                                'created_at',
                                datetime.now().isoformat()
                            )
                        )
                    )
                except Exception as e:
                    logger.warning(
                        f"[MODEL_CREATOR] ‚ö†Ô∏è Impossible de recr√©er "
                        f"VoiceFingerprint: {e}"
                    )

            # Cr√©er un dossier temporaire pour l'embedding (n√©cessaire pour TTS)
            profile_user_id = profile_data.get('userId', user_id)
            user_dir = self.voice_cache_dir / profile_user_id
            user_dir.mkdir(parents=True, exist_ok=True)

            profile_id = profile_data.get('profileId', f"vfp_{profile_user_id[:8]}")
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            # S√âCURIT√â: .npy au lieu de .pkl (numpy safe vs pickle unsafe)
            embedding_filename = f"{profile_user_id}_{profile_id}_{timestamp}_gateway.npy"
            embedding_path = str(user_dir / embedding_filename)

            # Sauvegarder l'embedding de mani√®re s√©curis√©e avec NumPy
            # S√âCURIT√â: np.save est s√ªr, pickle.dump permet l'ex√©cution de code arbitraire
            np.save(embedding_path, embedding)

            logger.info(
                f"[MODEL_CREATOR] üíæ Embedding sauvegard√©: {embedding_path}"
            )

            # Cr√©er le VoiceModel
            model = VoiceModel(
                user_id=profile_user_id,
                embedding_path=embedding_path,
                audio_count=profile_data.get('audioCount', 1),
                total_duration_ms=profile_data.get('totalDurationMs', 0),
                quality_score=profile_data.get('qualityScore', 0.8),
                profile_id=profile_id,
                version=profile_data.get('version', 1),
                created_at=datetime.now(),
                updated_at=datetime.now(),
                embedding=embedding,
                voice_characteristics=voice_characteristics,
                fingerprint=fingerprint
            )

            logger.info(
                f"[MODEL_CREATOR] ‚úÖ VoiceModel cr√©√© depuis Gateway: "
                f"user={profile_user_id}, quality={model.quality_score:.2f}, "
                f"profile_id={profile_id}"
            )

            return model

        except Exception as e:
            logger.error(
                f"[MODEL_CREATOR] ‚ùå Erreur cr√©ation VoiceModel depuis Gateway: {e}"
            )
            import traceback
            traceback.print_exc()
            return None

    async def _validate_audio_quality_for_cloning(
        self,
        audio_path: str
    ) -> Dict[str, Any]:
        """
        Valide la qualit√© audio avant clonage vocal.

        V√©rifie:
        - SNR (Signal-to-Noise Ratio) estim√©
        - Pr√©sence de clipping
        - Ratio de silence vs parole
        - √ânergie moyenne

        Args:
            audio_path: Chemin vers le fichier audio

        Returns:
            Dict avec:
            - valid: bool - True si qualit√© suffisante pour clonage
            - snr_db: float - SNR estim√© en dB
            - clipping_ratio: float - Ratio de samples √©cr√™t√©s
            - silence_ratio: float - Ratio de silence dans l'audio
            - energy_db: float - √ânergie moyenne en dB
            - warnings: List[str] - Avertissements de qualit√©
            - can_clone: bool - True si clonage recommand√©
        """
        import soundfile as sf

        warnings = []
        result = {
            "valid": True,
            "snr_db": 0.0,
            "clipping_ratio": 0.0,
            "silence_ratio": 0.0,
            "energy_db": -60.0,
            "warnings": warnings,
            "can_clone": True
        }

        try:
            # Convertir en WAV si n√©cessaire (M4A, AAC non support√©s par soundfile)
            wav_path = convert_to_wav_if_needed(audio_path)

            # Charger l'audio
            audio, sr = sf.read(wav_path)
            if len(audio.shape) > 1:
                audio = audio.mean(axis=1)  # Convertir en mono

            # 1. Calcul de l'√©nergie RMS
            rms = np.sqrt(np.mean(audio ** 2))
            energy_db = 20 * np.log10(max(rms, 1e-10))
            result["energy_db"] = float(energy_db)

            if energy_db < -40:
                warnings.append(f"Audio tr√®s faible ({energy_db:.1f}dB)")
                result["can_clone"] = False

            # 2. D√©tection de clipping (samples > 0.99)
            clipping_samples = np.sum(np.abs(audio) > 0.99)
            clipping_ratio = clipping_samples / len(audio)
            result["clipping_ratio"] = float(clipping_ratio)

            if clipping_ratio > 0.05:  # Plus de 5% de clipping
                warnings.append(f"Clipping excessif ({clipping_ratio*100:.1f}%)")
                result["can_clone"] = False
            elif clipping_ratio > 0.01:
                warnings.append(f"Clipping mod√©r√© ({clipping_ratio*100:.1f}%)")

            # 3. D√©tection des silences (RMS < -40dB par fen√™tre)
            frame_length = int(0.025 * sr)  # 25ms
            hop_length = int(0.010 * sr)   # 10ms

            frames = []
            for i in range(0, len(audio) - frame_length, hop_length):
                frame_rms = np.sqrt(np.mean(audio[i:i+frame_length] ** 2))
                frames.append(frame_rms)

            if frames:
                frames = np.array(frames)
                silence_threshold = 10 ** (-40 / 20)  # -40dB
                silence_frames = np.sum(frames < silence_threshold)
                silence_ratio = silence_frames / len(frames)
                result["silence_ratio"] = float(silence_ratio)

                if silence_ratio > 0.7:  # Plus de 70% de silence
                    warnings.append(f"Trop de silence ({silence_ratio*100:.1f}%)")
                    result["can_clone"] = False

            # 4. Estimation du SNR (approximatif)
            # On utilise les percentiles d'√©nergie comme proxy
            if frames is not None and len(frames) > 0:
                speech_level = np.percentile(frames, 90)  # Niveau parole
                noise_level = np.percentile(frames, 10)   # Niveau bruit de fond

                if noise_level > 1e-10:
                    snr_db = 20 * np.log10(speech_level / noise_level)
                    result["snr_db"] = float(snr_db)

                    if snr_db < 10:  # SNR < 10dB est probl√©matique
                        warnings.append(f"SNR faible ({snr_db:.1f}dB)")
                        result["can_clone"] = False
                    elif snr_db < 15:
                        warnings.append(f"SNR mod√©r√© ({snr_db:.1f}dB)")

            # 5. Verdict final
            result["valid"] = len([w for w in warnings if "excessif" in w or "Trop" in w or "SNR faible" in w]) == 0

            if warnings:
                logger.warning(
                    f"[MODEL_CREATOR] ‚ö†Ô∏è Qualit√© audio: {', '.join(warnings)}"
                )
            else:
                logger.info(
                    f"[MODEL_CREATOR] ‚úÖ Qualit√© audio OK: "
                    f"SNR={result['snr_db']:.1f}dB, energy={result['energy_db']:.1f}dB"
                )

        except Exception as e:
            logger.warning(f"[MODEL_CREATOR] Validation audio √©chou√©e: {e}")
            result["warnings"].append(f"Erreur validation: {e}")
            result["valid"] = False
            result["can_clone"] = False

        return result

    async def _create_voice_model(
        self,
        user_id: str,
        audio_paths: List[str],
        total_duration_ms: int
    ) -> VoiceModel:
        """
        Cr√©e un nouveau mod√®le de voix √† partir des audios.

        IMPORTANT: Extrait uniquement les segments du locuteur principal
        pour garantir que le clonage ne concerne que sa voix.

        Args:
            user_id: ID de l'utilisateur
            audio_paths: Liste des chemins vers les fichiers audio
            total_duration_ms: Dur√©e totale des audios

        Returns:
            VoiceModel nouvellement cr√©√©
        """
        start_time = time.time()
        logger.info(
            f"[MODEL_CREATOR] üé§ Cr√©ation mod√®le pour {user_id} "
            f"({len(audio_paths)} audios)"
        )

        # Filtrer les audios valides
        valid_paths = [p for p in audio_paths if os.path.exists(p)]
        if not valid_paths:
            raise ValueError("Aucun fichier audio valide trouv√©")

        # =====================================================================
        # VALIDATION QUALIT√â AUDIO AVANT CLONAGE
        # V√©rifie SNR, clipping, silences pour garantir un clonage de qualit√©
        # =====================================================================
        audio_quality_issues = []
        for audio_path in valid_paths:
            quality_result = await self._validate_audio_quality_for_cloning(audio_path)
            if not quality_result["can_clone"]:
                audio_quality_issues.extend(quality_result["warnings"])

        if audio_quality_issues:
            logger.warning(
                f"[MODEL_CREATOR] ‚ö†Ô∏è Probl√®mes qualit√© d√©tect√©s pour {user_id}: "
                f"{', '.join(set(audio_quality_issues))} - clonage peut √™tre d√©grad√©"
            )

        # Cr√©er le dossier utilisateur: {voice_cache_dir}/{user_id}/
        user_dir = self.voice_cache_dir / user_id
        user_dir.mkdir(parents=True, exist_ok=True)

        # =====================================================================
        # EXTRACTION DU LOCUTEUR PRINCIPAL UNIQUEMENT
        # Pour chaque audio, extraire uniquement les segments du locuteur principal
        # =====================================================================
        voice_analyzer = get_voice_analyzer()
        extracted_paths = []
        primary_voice_chars = None
        recording_metadata = None

        for audio_path in valid_paths:
            try:
                # Extraire uniquement les segments du locuteur principal
                extracted_path, metadata = await voice_analyzer.extract_primary_speaker_audio(
                    audio_path,
                    output_path=str(user_dir / f"primary_{os.path.basename(audio_path)}"),
                    min_segment_duration_ms=100
                )
                extracted_paths.append(extracted_path)

                # Conserver les caract√©ristiques vocales du premier locuteur principal
                if primary_voice_chars is None and metadata.primary_speaker:
                    primary_voice_chars = metadata.primary_speaker.voice_characteristics
                    recording_metadata = metadata
                    logger.info(
                        f"[MODEL_CREATOR] Locuteur principal d√©tect√©: "
                        f"gender={primary_voice_chars.estimated_gender}, "
                        f"pitch={primary_voice_chars.pitch_mean_hz:.1f}Hz"
                    )

            except Exception as e:
                logger.warning(
                    f"[MODEL_CREATOR] Erreur extraction locuteur principal: {e}"
                )
                # Fallback: utiliser l'audio complet
                extracted_paths.append(audio_path)

        # Recalculer la dur√©e totale apr√®s extraction
        extracted_duration_ms = 0
        for path in extracted_paths:
            extracted_duration_ms += await self._audio_processor.get_audio_duration_ms(
                path
            )

        logger.info(
            f"[MODEL_CREATOR] Audio extrait: {extracted_duration_ms}ms "
            f"(original: {total_duration_ms}ms, {len(extracted_paths)} fichiers)"
        )

        # Concat√©ner les audios extraits si multiples
        if len(extracted_paths) > 1:
            combined_audio = await self._audio_processor.concatenate_audios(
                extracted_paths,
                output_dir=user_dir,
                user_id=user_id
            )
        else:
            combined_audio = extracted_paths[0]

        # G√©n√©rer un profile_id unique
        profile_id = uuid_module.uuid4().hex[:12]
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")

        # Extraire l'embedding de voix (du locuteur principal uniquement)
        embedding = await self._audio_processor.extract_voice_embedding(
            combined_audio, user_dir
        )

        # Calculer score de qualit√©
        quality_score = self._audio_processor.calculate_quality_score(
            extracted_duration_ms, len(valid_paths)
        )

        # Chemin de l'embedding avec nouvelle convention
        # S√âCURIT√â: .npy au lieu de .pkl (numpy safe vs pickle unsafe)
        embedding_filename = f"{user_id}_{profile_id}_{timestamp}.npy"
        embedding_path = str(user_dir / embedding_filename)

        # Cr√©er le mod√®le avec les caract√©ristiques vocales du locuteur principal
        model = VoiceModel(
            user_id=user_id,
            embedding_path=embedding_path,
            audio_count=len(valid_paths),
            total_duration_ms=extracted_duration_ms,  # Dur√©e extraite
            quality_score=quality_score,
            profile_id=profile_id,
            version=1,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            next_recalibration_at=datetime.now() + timedelta(
                days=self.VOICE_MODEL_MAX_AGE_DAYS
            ),
            embedding=embedding,
            voice_characteristics=primary_voice_chars
        )

        # G√©n√©rer l'empreinte vocale unique
        if model.voice_characteristics or model.embedding is not None:
            fingerprint = model.generate_fingerprint()
            if fingerprint:
                logger.info(
                    f"[MODEL_CREATOR] Empreinte vocale: "
                    f"{fingerprint.fingerprint_id}"
                )

        # Sauvegarder
        await self._cache_manager.save_model_to_cache(model)

        processing_time = int((time.time() - start_time) * 1000)
        logger.info(
            f"[MODEL_CREATOR] ‚úÖ Mod√®le cr√©√© pour {user_id}: "
            f"quality={quality_score:.2f}, time={processing_time}ms"
        )

        return model

    async def _improve_model(
        self,
        existing_model: VoiceModel,
        new_audio_path: str,
        improvement_weight_old: float = 0.7,
        improvement_weight_new: float = 0.3
    ) -> VoiceModel:
        """
        Am√©liore un mod√®le existant avec un nouvel audio.

        R√àGLE: La mise √† jour n'est effectu√©e QUE si la signature vocale
        du nouvel audio correspond au profil existant (similarit√© > 80%).

        Args:
            existing_model: Mod√®le vocal existant √† am√©liorer
            new_audio_path: Chemin vers le nouvel audio
            improvement_weight_old: Poids de l'ancien embedding (d√©faut: 0.7)
            improvement_weight_new: Poids du nouveau embedding (d√©faut: 0.3)

        Returns:
            VoiceModel am√©lior√©
        """
        logger.info(
            f"[MODEL_CREATOR] üîÑ V√©rification am√©lioration mod√®le pour "
            f"{existing_model.user_id}"
        )

        voice_analyzer = get_voice_analyzer()

        # Charger l'embedding existant si n√©cessaire
        if existing_model.embedding is None:
            existing_model = await self._cache_manager.load_embedding(existing_model)

        # V√©rifier si la signature correspond avant mise √† jour
        if existing_model.fingerprint:
            metadata = await voice_analyzer.analyze_audio(new_audio_path)
            can_update, reason, matched_speaker = voice_analyzer.can_update_user_profile(
                metadata,
                existing_model.fingerprint,
                similarity_threshold=0.80
            )

            if not can_update:
                logger.warning(
                    f"[MODEL_CREATOR] ‚ö†Ô∏è Mise √† jour refus√©e pour "
                    f"{existing_model.user_id}: {reason}"
                )
                # Retourner le mod√®le existant sans modification
                return existing_model

            logger.info(f"[MODEL_CREATOR] ‚úÖ Signature vocale v√©rifi√©e: {reason}")

        # Extraire embedding du nouvel audio
        user_dir = self.voice_cache_dir / existing_model.user_id / "temp"
        user_dir.mkdir(parents=True, exist_ok=True)

        new_embedding = await self._audio_processor.extract_voice_embedding(
            new_audio_path, user_dir
        )

        if existing_model.embedding is not None and new_embedding is not None:
            # Moyenne pond√©r√©e (plus de poids aux anciens pour stabilit√©)
            improved_embedding = (
                improvement_weight_old * existing_model.embedding +
                improvement_weight_new * new_embedding
            )
        else:
            improved_embedding = (
                new_embedding if new_embedding is not None
                else existing_model.embedding
            )

        # Mettre √† jour le mod√®le
        existing_model.embedding = improved_embedding
        existing_model.updated_at = datetime.now()
        existing_model.audio_count += 1
        existing_model.quality_score = min(1.0, existing_model.quality_score + 0.05)
        existing_model.version += 1
        existing_model.next_recalibration_at = datetime.now() + timedelta(
            days=self.VOICE_MODEL_MAX_AGE_DAYS
        )

        # R√©g√©n√©rer l'empreinte vocale avec le nouvel embedding
        if existing_model.voice_characteristics:
            existing_model.generate_fingerprint()

        # Sauvegarder
        await self._cache_manager.save_model_to_cache(existing_model)

        logger.info(
            f"[MODEL_CREATOR] ‚úÖ Mod√®le am√©lior√© pour {existing_model.user_id} "
            f"(v{existing_model.version})"
        )
        return existing_model
