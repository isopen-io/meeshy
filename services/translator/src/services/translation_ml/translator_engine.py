"""
Module moteur de traduction ML
Responsabilit√©s:
- Logique de traduction ML (batch et individuelle)
- D√©tection de langue
- Gestion des pipelines thread-local
- Optimisations performance (inference mode, batch processing)
- D√©coupage intelligent de textes longs
"""

import logging
import asyncio
import threading
import re
from typing import List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)


def smart_split_text(text: str, max_chars: int = 200) -> List[str]:
    """
    D√©coupe intelligemment un texte long en morceaux aux ponctuations naturelles.

    Strat√©gie de d√©coupage (par ordre de pr√©f√©rence):
    1. Aux points, points d'exclamation, points d'interrogation (. ! ?)
    2. Aux points-virgules et deux-points (; :)
    3. Aux virgules (,)
    4. Aux espaces si aucune ponctuation
    5. Force le d√©coupage si vraiment trop long

    Args:
        text: Texte √† d√©couper
        max_chars: Taille maximale de chaque morceau (d√©faut: 200 caract√®res)

    Returns:
        Liste de morceaux de texte (‚â§ max_chars chacun)

    Exemples:
        >>> smart_split_text("Bonjour. Comment allez-vous? Tr√®s bien!", 25)
        ['Bonjour.', 'Comment allez-vous?', 'Tr√®s bien!']

        >>> smart_split_text("Un texte tr√®s tr√®s tr√®s long sans ponctuation", 20)
        ['Un texte tr√®s tr√®s', 'tr√®s long sans', 'ponctuation']
    """
    if len(text) <= max_chars:
        return [text]

    chunks = []
    remaining = text

    while len(remaining) > max_chars:
        # Chercher un point de coupure dans les premiers max_chars caract√®res
        chunk = remaining[:max_chars]

        # 1. Priorit√©: couper aux points forts (. ! ?)
        strong_punct = max(
            chunk.rfind('. '),
            chunk.rfind('! '),
            chunk.rfind('? ')
        )

        # 2. Sinon, couper aux points moyens (; :)
        if strong_punct == -1:
            medium_punct = max(
                chunk.rfind('; '),
                chunk.rfind(': ')
            )
            cut_pos = medium_punct
        else:
            cut_pos = strong_punct

        # 3. Sinon, couper aux virgules
        if cut_pos == -1:
            cut_pos = chunk.rfind(', ')

        # 4. Sinon, couper aux espaces
        if cut_pos == -1:
            cut_pos = chunk.rfind(' ')

        # 5. Si aucune ponctuation/espace, forcer la coupure
        if cut_pos == -1:
            cut_pos = max_chars - 1
        else:
            # Inclure la ponctuation dans le chunk
            cut_pos += 1
            if cut_pos < len(chunk) and chunk[cut_pos] == ' ':
                cut_pos += 1  # Inclure l'espace apr√®s la ponctuation

        # Extraire le chunk et continuer avec le reste
        chunks.append(remaining[:cut_pos].strip())
        remaining = remaining[cut_pos:].strip()

    # Ajouter le dernier morceau
    if remaining:
        chunks.append(remaining)

    return chunks

# Import conditionnel des d√©pendances ML
ML_AVAILABLE = False
try:
    import torch
    from transformers import pipeline as create_pipeline
    ML_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è Dependencies ML non disponibles")

from utils.performance import (
    PerformanceConfig,
    create_inference_context
)


class TranslatorEngine:
    """
    Moteur de traduction utilisant les mod√®les NLLB
    G√®re la traduction ML batch et individuelle avec optimisations
    """

    def __init__(self, model_loader, executor: ThreadPoolExecutor):
        """
        Initialise le moteur de traduction

        Args:
            model_loader: Instance de ModelLoader avec mod√®les charg√©s
            executor: ThreadPoolExecutor pour traductions asynchrones
        """
        self.model_loader = model_loader
        self.executor = executor
        self.perf_config = PerformanceConfig()

        # Pool de pipelines thread-local pour √©viter recr√©ation
        self._thread_local_pipelines = {}
        self._pipeline_lock = threading.Lock()

        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn',
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }

        logger.info("‚öôÔ∏è TranslatorEngine initialis√©")

    def detect_language(self, text: str) -> str:
        """
        D√©tection de langue simple bas√©e sur mots-cl√©s

        Args:
            text: Texte √† analyser

        Returns:
            Code langue d√©tect√© ('fr', 'en', 'es', 'de')
        """
        text_lower = text.lower()

        # Mots caract√©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # D√©faut

    def _get_thread_local_pipeline(self, model_type: str) -> Tuple[Optional[any], bool]:
        """
        OPTIMISATION: Obtient ou cr√©e un pipeline de traduction pour le thread actuel

        Au lieu de cr√©er un pipeline √† chaque traduction (100-500ms overhead),
        on r√©utilise le pipeline du thread. Gains: 3-5x plus rapide.

        Args:
            model_type: Type de mod√®le

        Returns:
            tuple: (pipeline, nllb_codes_supported) ou (None, False) si erreur
        """
        if not ML_AVAILABLE:
            return None, False

        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"

        # V√©rifier le cache
        if cache_key in self._thread_local_pipelines:
            return self._thread_local_pipelines[cache_key], True

        # Cr√©er nouveau pipeline
        with self._pipeline_lock:
            # Double-check
            if cache_key in self._thread_local_pipelines:
                return self._thread_local_pipelines[cache_key], True

            try:
                model = self.model_loader.get_model(model_type)
                if model is None:
                    logger.error(f"‚ùå Mod√®le {model_type} non charg√©")
                    return None, False

                tokenizer = self.model_loader.get_thread_local_tokenizer(model_type)
                if tokenizer is None:
                    logger.error(f"‚ùå Tokenizer non disponible pour {model_type}")
                    return None, False

                # Cr√©er le pipeline UNE SEULE FOIS pour ce thread
                device = self.model_loader.device
                new_pipeline = create_pipeline(
                    "translation",
                    model=model,
                    tokenizer=tokenizer,
                    device=0 if device == 'cuda' and torch.cuda.is_available() else -1,
                    max_length=512,
                    batch_size=8
                )

                self._thread_local_pipelines[cache_key] = new_pipeline
                logger.info(f"‚úÖ Pipeline thread-local cr√©√©: {cache_key} (sera r√©utilis√©)")
                return new_pipeline, True

            except Exception as e:
                logger.error(f"‚ùå Erreur cr√©ation pipeline thread-local: {e}")
                import traceback
                traceback.print_exc()
                return None, False

    async def translate_text(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        model_type: str
    ) -> str:
        """
        Traduction ML d'un texte individuel avec pipeline r√©utilisable.

        Pour les textes longs (>200 caract√®res), d√©coupe intelligemment
        aux ponctuations et traduit par morceaux pour √©viter la troncature.

        Args:
            text: Texte √† traduire
            source_lang: Langue source ('fr', 'en', etc)
            target_lang: Langue cible
            model_type: Type de mod√®le ('basic', 'premium')

        Returns:
            Texte traduit
        """
        if not self.model_loader.is_model_loaded(model_type):
            raise Exception(f"Mod√®le {model_type} non charg√©")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # D√âCOUPAGE INTELLIGENT: Textes longs d√©coup√©s aux ponctuations
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if len(text) > 200:
            logger.info(
                f"[TRANSLATE] Texte long d√©tect√© ({len(text)} chars) ‚Üí "
                f"d√©coupage intelligent aux ponctuations"
            )

            # D√©couper en morceaux de max 200 caract√®res
            chunks = smart_split_text(text, max_chars=200)

            logger.info(
                f"[TRANSLATE] Texte d√©coup√© en {len(chunks)} morceaux "
                f"(tailles: {[len(c) for c in chunks]})"
            )

            # Traduire chaque morceau
            translated_chunks = []
            for i, chunk in enumerate(chunks):
                logger.debug(f"[TRANSLATE] Chunk {i+1}/{len(chunks)}: '{chunk[:50]}...'")
                translated_chunk = await self._translate_single_chunk(
                    chunk, source_lang, target_lang, model_type
                )
                translated_chunks.append(translated_chunk)

            # Recoller les morceaux traduits
            final_translation = ' '.join(translated_chunks)

            logger.info(
                f"[TRANSLATE] ‚úÖ Traduction compl√®te: {len(text)} ‚Üí {len(final_translation)} chars"
            )

            return final_translation

        # Texte court: traduction directe
        return await self._translate_single_chunk(text, source_lang, target_lang, model_type)

    async def _translate_single_chunk(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        model_type: str
    ) -> str:
        """
        Traduit un seul morceau de texte (‚â§ 200 caract√®res).

        Args:
            text: Texte √† traduire
            source_lang: Langue source
            target_lang: Langue cible
            model_type: Type de mod√®le

        Returns:
            Texte traduit
        """

        def translate_sync():
            """Traduction synchrone dans un thread"""
            try:
                # R√©utiliser le pipeline thread-local
                reusable_pipeline, is_available = self._get_thread_local_pipeline(model_type)

                if not is_available or reusable_pipeline is None:
                    raise Exception(f"Pipeline non disponible pour {model_type}")

                # Codes NLLB
                nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                # OPTIMISATION AVANC√âE: Greedy decoding (4x plus rapide)
                with create_inference_context():
                    result = reusable_pipeline(
                        text,
                        src_lang=nllb_source,
                        tgt_lang=nllb_target,
                        max_length=256,       # Optimis√© pour chunks courts (d√©coupage intelligent avant)
                        num_beams=1,          # GREEDY (4x plus rapide!)
                        do_sample=False       # D√©terministe
                        # early_stopping retir√©: incompatible avec num_beams=1 (greedy decoding)
                    )

                # Extraire le r√©sultat
                if result and len(result) > 0 and 'translation_text' in result[0]:
                    return result[0]['translation_text']
                else:
                    return f"[NLLB-No-Result] {text}"

            except Exception as e:
                logger.error(f"Erreur pipeline {model_type}: {e}")
                return f"[ML-Pipeline-Error] {text}"

        # Ex√©cuter de mani√®re asynchrone
        loop = asyncio.get_event_loop()
        translated = await loop.run_in_executor(self.executor, translate_sync)
        return translated

    async def translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str,
        model_type: str
    ) -> List[str]:
        """
        OPTIMISATION: Traduction BATCH pour plusieurs textes √† la fois

        Avantages:
        - Pipeline cr√©√© UNE SEULE fois au lieu de N fois
        - Batch processing natif du mod√®le (padding optimis√©)
        - Meilleure utilisation GPU/CPU
        - R√©duction overhead de 70% sur pipeline creation

        Args:
            texts: Liste de textes √† traduire
            source_lang: Code langue source
            target_lang: Code langue cible
            model_type: Type de mod√®le

        Returns:
            Liste des textes traduits (m√™me ordre)
        """
        if not texts:
            return []

        # Fallback si peu de textes
        if len(texts) <= 2:
            results = []
            for text in texts:
                translated = await self.translate_text(text, source_lang, target_lang, model_type)
                results.append(translated)
            return results

        if not self.model_loader.is_model_loaded(model_type):
            raise Exception(f"Mod√®le {model_type} non charg√©")

        batch_size = self.perf_config.batch_size

        def translate_batch_sync():
            """Traduction batch synchrone - OPTIMIS√â POUR VITESSE"""
            try:
                logger.info(f"[BATCH-SYNC] üöÄ FAST translate_batch_sync: {len(texts)} textes, {source_lang}‚Üí{target_lang}")

                # R√©utiliser le pipeline thread-local
                reusable_pipeline, is_available = self._get_thread_local_pipeline(model_type)

                if not is_available or reusable_pipeline is None:
                    raise Exception(f"Pipeline non disponible pour {model_type}")

                nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                all_results = []

                # OPTIMISATION: Traitement direct SANS timeout wrapper (overhead supprim√©)
                with create_inference_context():
                    for i in range(0, len(texts), batch_size):
                        chunk = texts[i:i + batch_size]

                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        # OPTIMISATIONS NLLB AVANC√âES:
                        # - num_beams=1: Greedy decoding (4x plus rapide que beam search)
                        # - do_sample=False: D√©sactive sampling (d√©terministe)
                        # - max_length=256: Optimis√© pour segments courts
                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        results = reusable_pipeline(
                            chunk,
                            src_lang=nllb_source,
                            tgt_lang=nllb_target,
                            max_length=256,       # Optimis√© pour segments courts (d√©coupage avant si besoin)
                            num_beams=1,          # GREEDY DECODING (4x plus rapide!)
                            do_sample=False       # D√©terministe
                            # early_stopping retir√©: incompatible avec num_beams=1
                        )

                        logger.info(f"[BATCH-SYNC] ‚úÖ Chunk {i//batch_size + 1}: {len(results)} r√©sultats")

                        for result in results:
                            if isinstance(result, dict) and 'translation_text' in result:
                                all_results.append(result['translation_text'])
                            elif isinstance(result, list) and len(result) > 0:
                                all_results.append(result[0].get('translation_text', '[No-Result]'))
                            else:
                                all_results.append('[Batch-No-Result]')

                logger.info(f"[BATCH-SYNC] ‚úÖ Sortie inference_context, {len(all_results)} r√©sultats")

                # Nettoyage m√©moire p√©riodique
                if self.perf_config.enable_memory_cleanup and len(texts) > 20:
                    from utils.performance import get_performance_optimizer
                    perf_optimizer = get_performance_optimizer()
                    perf_optimizer.cleanup_memory()

                logger.info(f"[BATCH-SYNC] ‚úÖ Fin translate_batch_sync: {len(all_results)} traductions")
                return all_results

            except Exception as e:
                logger.error(f"[BATCH-SYNC] ‚ùå Erreur batch pipeline {model_type}: {e}")
                import traceback
                traceback.print_exc()
                return [f"[ML-Batch-Error] {t}" for t in texts]

        # Ex√©cuter de mani√®re asynchrone
        logger.info(f"[BATCH] üîÑ Soumission √† executor (threads actifs: {self.executor._max_workers})")
        loop = asyncio.get_event_loop()
        logger.info(f"[BATCH] Event loop obtenue: {loop}")
        results = await loop.run_in_executor(self.executor, translate_batch_sync)
        logger.info(f"[BATCH] ‚úÖ run_in_executor termin√©, {len(results)} r√©sultats")

        logger.info(f"‚ö° [BATCH] {len(texts)} textes traduits en batch ({source_lang}‚Üí{target_lang})")
        return results

    def cleanup(self):
        """Lib√®re les ressources du moteur"""
        logger.info("üßπ Nettoyage TranslatorEngine...")
        self._thread_local_pipelines.clear()
        logger.info("‚úÖ TranslatorEngine nettoy√©")
