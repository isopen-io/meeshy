"""
MMS TTS Backend
===============

Backend Meta MMS TTS - Support 1100+ langues (sans clonage vocal)
IdÃ©al pour les langues africaines.

INTÃ‰GRATION: Ce backend utilise le ModelManager centralisÃ© pour:
- Gestion mÃ©moire unifiÃ©e avec Ã©viction LRU
- Statistiques globales sur tous les modÃ¨les MMS chargÃ©s
- Pas de duplication de cache entre services
"""

import asyncio
import logging
from typing import Optional, Dict, Any

from ..base import BaseTTSBackend
from config.settings import get_settings
from ...model_manager import TTSBackend as TTSBackendEnum

logger = logging.getLogger(__name__)


class MMSBackend(BaseTTSBackend):
    """Backend Meta MMS TTS - Support 1100+ langues (sans clonage vocal)

    IdÃ©al pour les langues africaines non supportÃ©es par Chatterbox/XTTS:
    - Amharic (am), Swahili (sw), Yoruba (yo), Hausa (ha)
    - Kinyarwanda (rw), Kirundi (rn), Shona (sn), Luganda (lg)
    - Oromo (om), Tigrinya (ti), Chichewa (ny), Ewe (ee)
    - Fula (ff), Malagasy (mg), Somali (so), Tsonga (ts)
    """

    # Mapping ISO 639-1/2 vers ISO 639-3 (codes MMS)
    LANGUAGE_CODE_MAP = {
        # Langues africaines
        'am': 'amh',    # Amharic
        'sw': 'swh',    # Swahili
        'yo': 'yor',    # Yoruba
        'ha': 'hau',    # Hausa
        'rw': 'kin',    # Kinyarwanda
        'rn': 'run',    # Kirundi
        'sn': 'sna',    # Shona
        'lg': 'lug',    # Luganda
        'om': 'orm',    # Oromo
        'ti': 'tir',    # Tigrinya
        'ny': 'nya',    # Chichewa/Nyanja
        'ee': 'ewe',    # Ewe
        'ff': 'ful',    # Fula
        'mg': 'mlg',    # Malagasy
        'so': 'som',    # Somali
        'ts': 'tso',    # Tsonga
        'bem': 'bem',   # Bemba
        'ybb': 'ybb',   # Yemba
        # Langues principales (fallback)
        'en': 'eng',
        'fr': 'fra',
        'es': 'spa',
        'de': 'deu',
        'pt': 'por',
        'it': 'ita',
        'ru': 'rus',
        'ar': 'arb',
        'hi': 'hin',
        'bn': 'ben',
        'zh': 'cmn',
        'ja': 'jpn',
        'ko': 'kor',
    }

    # Langues africaines supportÃ©es par MMS TTS
    AFRICAN_LANGUAGES = {
        'am', 'sw', 'yo', 'ha', 'rw', 'rn', 'sn', 'lg',
        'om', 'ti', 'ny', 'ee', 'ff', 'mg', 'so', 'ts',
        'bem', 'ybb'
    }

    def __init__(self, device: str = "cpu"):
        super().__init__()
        self.device = device
        self._available = False
        # NOTE: Les modÃ¨les sont maintenant gÃ©rÃ©s par le ModelManager centralisÃ©
        # via les mÃ©thodes hÃ©ritÃ©es _register_model() et _get_model()
        self._settings = get_settings()

        try:
            from transformers import VitsModel, AutoTokenizer
            self._available = True
            logger.info("âœ… [TTS] MMS TTS (transformers) disponible")
        except ImportError:
            logger.warning("âš ï¸ [TTS] MMS TTS (transformers) non disponible - pip install transformers")

    @property
    def is_available(self) -> bool:
        return self._available

    def is_model_downloaded(self) -> bool:
        """MMS tÃ©lÃ©charge les modÃ¨les Ã  la demande"""
        return self._available

    async def download_model(self) -> bool:
        """MMS tÃ©lÃ©charge automatiquement"""
        return self._available

    async def initialize(self) -> bool:
        """MMS s'initialise Ã  la demande par langue"""
        if not self._available:
            return False
        self._initialized = True
        logger.info("âœ… [TTS] MMS TTS initialisÃ© (modÃ¨les chargÃ©s Ã  la demande)")
        return True

    def _get_mms_code(self, language: str) -> str:
        """Convertit un code ISO 639-1/2 vers le code MMS (ISO 639-3)"""
        lang = language.lower().split('-')[0]
        return self.LANGUAGE_CODE_MAP.get(lang, lang)

    async def _load_model_for_language(self, language: str):
        """Charge le modÃ¨le MMS pour une langue spÃ©cifique"""
        mms_code = self._get_mms_code(language)
        model_id_in_manager = f"tts_mms_transformers_{mms_code}"

        # VÃ©rifier si dÃ©jÃ  dans le ModelManager
        cached = self._get_model(model_id_in_manager)
        if cached is not None:
            logger.debug(f"[TTS] ModÃ¨le MMS {mms_code} rÃ©cupÃ©rÃ© depuis ModelManager")
            return cached

        try:
            from transformers import VitsModel, AutoTokenizer
            import torch

            hf_model_id = f"facebook/mms-tts-{mms_code}"
            logger.info(f"[TTS] ðŸ“¥ Chargement modÃ¨le MMS: {hf_model_id}")

            loop = asyncio.get_event_loop()

            def load():
                tokenizer = AutoTokenizer.from_pretrained(hf_model_id)
                model = VitsModel.from_pretrained(hf_model_id)
                if self.device != "cpu" and torch.cuda.is_available():
                    model = model.to(self.device)
                return tokenizer, model

            tokenizer, model = await loop.run_in_executor(None, load)

            # Enregistrer dans le ModelManager centralisÃ©
            # Le tuple (tokenizer, model) est stockÃ© comme objet unique
            model_tuple = (tokenizer, model)
            self._register_model(
                model_id=model_id_in_manager,
                model_object=model_tuple,
                backend=TTSBackendEnum.MMS.value,
                language=language,
                priority=2  # Normale - peut Ãªtre Ã©victÃ© si besoin
            )

            logger.info(f"âœ… [TTS] ModÃ¨le MMS {mms_code} chargÃ© et enregistrÃ© via ModelManager")
            return model_tuple

        except Exception as e:
            logger.error(f"âŒ [TTS] Erreur chargement MMS {mms_code}: {e}")
            raise RuntimeError(f"ModÃ¨le MMS non disponible pour {language}: {e}")

    async def synthesize(
        self,
        text: str,
        language: str,
        speaker_audio_path: Optional[str] = None,  # IgnorÃ© - MMS ne supporte pas le clonage
        output_path: str = None,
        **kwargs
    ) -> str:
        """SynthÃ©tise le texte avec MMS TTS

        Note: MMS ne supporte pas le clonage vocal.
        """
        import torch
        import scipy.io.wavfile as wavfile

        if not self._initialized:
            await self.initialize()

        tokenizer, model = await self._load_model_for_language(language)

        loop = asyncio.get_event_loop()

        def generate():
            inputs = tokenizer(text, return_tensors="pt")
            if self.device != "cpu" and torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                output = model(**inputs).waveform

            waveform = output.squeeze().cpu().numpy()
            return waveform

        waveform = await loop.run_in_executor(None, generate)

        # Sauvegarder en WAV
        sample_rate = model.config.sampling_rate
        wavfile.write(output_path, sample_rate, waveform)

        logger.info(f"âœ… [TTS] MMS synthÃ¨se terminÃ©e: {language} -> {output_path}")
        return output_path

    async def close(self):
        """LibÃ¨re les modÃ¨les chargÃ©s"""
        # NOTE: Les modÃ¨les sont gÃ©rÃ©s par le ModelManager centralisÃ©
        # Ils seront automatiquement Ã©victÃ©s via LRU si mÃ©moire faible
        # ou dÃ©chargÃ©s globalement via unload_models_by_type()
        self._initialized = False
        logger.info("[TTS] MMS TTS fermÃ© (modÃ¨les gÃ©rÃ©s par ModelManager)")

    def supports_language(self, language: str) -> bool:
        """VÃ©rifie si MMS supporte une langue"""
        lang = language.lower().split('-')[0]
        return lang in self.LANGUAGE_CODE_MAP or lang in self.AFRICAN_LANGUAGES

    def is_african_language(self, language: str) -> bool:
        """VÃ©rifie si c'est une langue africaine"""
        lang = language.lower().split('-')[0]
        return lang in self.AFRICAN_LANGUAGES
