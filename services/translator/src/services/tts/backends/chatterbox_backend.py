"""
Chatterbox TTS Backend
======================

Backend Chatterbox (Resemble AI) - MOD√àLE PAR D√âFAUT
Supporte le clonage vocal pour 23 langues.

INT√âGRATION: Ce backend utilise le ModelManager centralis√© pour:
- Gestion m√©moire unifi√©e avec √©viction LRU
- Statistiques globales sur les mod√®les Chatterbox charg√©s
- Pas de duplication de mod√®les entre sessions
"""

import os
import asyncio
import logging
import io
from pathlib import Path
from typing import Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor

from ..base import BaseTTSBackend
from config.settings import get_settings
from services.voice_analyzer_service import VoiceAnalyzerService, VoiceCharacteristics
from ...model_manager import TTSBackend as TTSBackendEnum

logger = logging.getLogger(__name__)

_background_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="tts_download")


class ChatterboxBackend(BaseTTSBackend):
    """Backend Chatterbox (Resemble AI) - MOD√àLE PAR D√âFAUT ET FALLBACK

    Supporte 2 modes:
    - Monolingual (ChatterboxTTS): Anglais uniquement, plus l√©ger
    - Multilingual (ChatterboxMultilingualTTS): 23 langues support√©es
    """

    # Langues support√©es par le mod√®le multilingue
    MULTILINGUAL_LANGUAGES = {
        'ar', 'da', 'de', 'el', 'en', 'es', 'fi', 'fr', 'he', 'hi',
        'it', 'ja', 'ko', 'ms', 'nl', 'no', 'pl', 'pt', 'ru', 'sv',
        'sw', 'tr', 'zh'
    }

    def __init__(self, device: str = "auto", turbo: bool = False):
        super().__init__()
        self.device = device
        self.turbo = turbo
        # NOTE: Les mod√®les sont maintenant g√©r√©s par le ModelManager centralis√©
        # au lieu d'attributs locaux. On garde les IDs pour les r√©cup√©rer.
        backend_name = TTSBackendEnum.CHATTERBOX_TURBO.value if turbo else TTSBackendEnum.CHATTERBOX.value
        self._model_id = f"tts_{backend_name}_mono"
        self._model_id_multi = f"tts_{backend_name}_multilingual"
        self._available = False
        self._available_multilingual = False
        self._initialized_multilingual = False
        self._settings = get_settings()
        self._models_path = Path(self._settings.huggingface_cache_path)

        try:
            from chatterbox.tts import ChatterboxTTS
            self._available = True
            logger.info(f"‚úÖ [TTS] Chatterbox {'Turbo' if turbo else ''} package disponible")
        except ImportError:
            logger.warning(f"‚ö†Ô∏è [TTS] Chatterbox {'Turbo' if turbo else ''} package non disponible")

        try:
            from chatterbox.mtl_tts import ChatterboxMultilingualTTS
            self._available_multilingual = True
            logger.info("‚úÖ [TTS] Chatterbox Multilingual (23 langues) disponible")
        except ImportError:
            logger.warning("‚ö†Ô∏è [TTS] Chatterbox Multilingual non disponible")

    @property
    def is_available(self) -> bool:
        return self._available or self._available_multilingual

    def is_model_downloaded(self) -> bool:
        """
        V√©rifie si le mod√®le Chatterbox est t√©l√©charg√©.

        PRIORIT√â MULTILINGUE: V√©rifie d'abord le mod√®le multilingual,
        puis fallback sur le monolingual.

        V√©rifie dans l'ordre:
        1. Le cache personnalis√© (models/huggingface/)
        2. Le cache global HuggingFace (~/.cache/huggingface/hub/)

        Note: Chatterbox utilise tokenizer.json au lieu de config.json
        """
        if not self._available and not self._available_multilingual:
            return False

        try:
            from huggingface_hub import try_to_load_from_cache

            # PRIORIT√â 1: V√©rifier le mod√®le CHATTERBOX (contient mono + multi)
            # NOTE: Le mod√®le multilingual et monolingual partagent le M√äME repo HuggingFace
            # ResembleAI/chatterbox contient les deux variantes (classes Python diff√©rentes)
            if self._available_multilingual or self._available:
                model_id = "ResembleAI/chatterbox-turbo" if self.turbo else "ResembleAI/chatterbox"
                check_file = "tokenizer.json"

                # 1a. Cache personnalis√©
                file_path = try_to_load_from_cache(
                    model_id,
                    check_file,
                    cache_dir=str(self._models_path)
                )
                if file_path is not None:
                    logger.debug(f"[TTS] Chatterbox trouv√© dans cache personnalis√©: {model_id}")
                    return True

                # 1b. Cache global
                file_path = try_to_load_from_cache(model_id, check_file)
                if file_path is not None:
                    logger.debug(f"[TTS] Chatterbox trouv√© dans cache global: {model_id}")
                    return True

            return False
        except Exception as e:
            logger.debug(f"[TTS] V√©rification cache Chatterbox: {e}")
            return False

    async def download_model(self) -> bool:
        """
        T√©l√©charge le mod√®le Chatterbox.

        PRIORIT√â MULTILINGUE: T√©l√©charge le mod√®le multilingual (23 langues) en priorit√©
        pour supporter une plateforme multilingue comme Meeshy.
        """
        if not self._available and not self._available_multilingual:
            return False

        self._downloading = True
        self._download_progress = 0.0

        try:
            from huggingface_hub import snapshot_download

            # T√©l√©charger ResembleAI/chatterbox qui contient MONO + MULTI
            # NOTE: Le m√™me repo contient les 2 variantes (classes Python diff√©rentes)
            model_id = "ResembleAI/chatterbox-turbo" if self.turbo else "ResembleAI/chatterbox"

            if self._available_multilingual:
                logger.info(f"[TTS] üåç T√©l√©chargement Chatterbox (avec support Multilingual 23 langues) vers {self._models_path}...")
                logger.info("[TTS] Langues support√©es via ChatterboxMultilingualTTS: ar, da, de, el, en, es, fi, fr, he, hi, it, ja, ko, ms, nl, no, pl, pt, ru, sv, sw, tr, zh")
            else:
                logger.info(f"[TTS] üì• T√©l√©chargement Chatterbox (ChatterboxTTS - anglais) vers {self._models_path}...")
                logger.warning("[TTS] ‚ö†Ô∏è Chatterbox Multilingual non disponible - support anglais uniquement")

            loop = asyncio.get_event_loop()

            def download():
                return snapshot_download(
                    repo_id=model_id,
                    cache_dir=str(self._models_path),
                    resume_download=True
                )

            await loop.run_in_executor(_background_executor, download)

            self._download_progress = 100.0
            logger.info(f"[TTS] ‚úÖ {model_id} t√©l√©charg√© avec succ√®s")
            return True

        except Exception as e:
            logger.error(f"[TTS] ‚ùå Erreur t√©l√©chargement Chatterbox: {e}")
            return False

        finally:
            self._downloading = False

    def _get_device(self):
        """D√©termine le device √† utiliser."""
        import torch
        if self.device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return self.device

    async def initialize(self) -> bool:
        """
        Initialise le mod√®le Chatterbox.

        PRIORIT√â MULTILINGUE: Pour une plateforme multilingue comme Meeshy,
        charge le mod√®le multilingual (23 langues) par d√©faut si disponible.
        Fallback sur le monolingual (anglais uniquement) si √©chec.
        """
        # Si d√©j√† initialis√© (mono ou multi), retourner True
        if self._initialized or self._initialized_multilingual:
            return True

        # √âTAPE 1: Essayer de charger le mod√®le MULTILINGUAL en priorit√©
        if self._available_multilingual:
            logger.info("[TTS] üåç Tentative de chargement Chatterbox Multilingual (23 langues)...")
            success = await self.initialize_multilingual()
            if success:
                logger.info("[TTS] ‚úÖ Chatterbox Multilingual charg√© - support de 23 langues activ√©")
                self._initialized = True  # Marquer comme initialis√© aussi
                return True
            else:
                logger.warning("[TTS] ‚ö†Ô∏è √âchec chargement multilingual, fallback sur monolingual...")

        # √âTAPE 2: Fallback sur le mod√®le MONOLINGUAL (anglais uniquement)
        if self._initialized:
            return True

        # V√©rifier si d√©j√† dans le ModelManager
        if self._has_model(self._model_id):
            logger.debug(f"[TTS] Chatterbox mono r√©cup√©r√© depuis ModelManager")
            self._initialized = True
            return True

        if not self._available:
            return False

        try:
            from chatterbox.tts import ChatterboxTTS

            device = self._get_device()
            model_name = "Turbo" if self.turbo else ""
            logger.info(f"[TTS] üîÑ Chargement Chatterbox {model_name} (monolingual - anglais uniquement)...")

            loop = asyncio.get_event_loop()

            if self.turbo:
                model = await loop.run_in_executor(
                    None,
                    lambda: ChatterboxTTS.from_pretrained("ResembleAI/chatterbox-turbo", device=device)
                )
            else:
                model = await loop.run_in_executor(
                    None,
                    lambda: ChatterboxTTS.from_pretrained(device=device)
                )

            # Enregistrer dans le ModelManager centralis√©
            backend_name = TTSBackendEnum.CHATTERBOX_TURBO.value if self.turbo else TTSBackendEnum.CHATTERBOX.value
            self._register_model(
                model_id=self._model_id,
                model_object=model,
                backend=backend_name,
                priority=1  # Haute priorit√© - mod√®le principal
            )

            self._initialized = True
            logger.info(f"‚úÖ [TTS] Chatterbox {model_name} monolingual initialis√© sur {device}")
            logger.warning("[TTS] ‚ö†Ô∏è Support multilingue limit√© - seulement anglais disponible")
            return True

        except Exception as e:
            logger.error(f"‚ùå [TTS] Erreur initialisation Chatterbox: {e}")
            return False

    async def initialize_multilingual(self) -> bool:
        """Initialise le mod√®le multilingue (23 langues)."""
        if self._initialized_multilingual:
            return True

        # V√©rifier si d√©j√† dans le ModelManager
        if self._has_model(self._model_id_multi):
            logger.debug(f"[TTS] Chatterbox multilingue r√©cup√©r√© depuis ModelManager")
            self._initialized_multilingual = True
            return True

        if not self._available_multilingual:
            logger.warning("[TTS] Chatterbox Multilingual non disponible")
            return False

        try:
            from chatterbox.mtl_tts import ChatterboxMultilingualTTS
            import torch

            device = self._get_device()
            logger.info(f"[TTS] üîÑ Chargement Chatterbox Multilingual (23 langues)...")

            # Monkey patch torch.load pour g√©rer le mapping CUDA -> CPU/MPS
            original_torch_load = torch.load

            def patched_torch_load(*args, **kwargs):
                if 'map_location' not in kwargs:
                    kwargs['map_location'] = device
                if 'weights_only' not in kwargs:
                    kwargs['weights_only'] = False
                return original_torch_load(*args, **kwargs)

            torch.load = patched_torch_load

            try:
                loop = asyncio.get_event_loop()
                model_multilingual = await loop.run_in_executor(
                    None,
                    lambda: ChatterboxMultilingualTTS.from_pretrained(device=device)
                )
            finally:
                torch.load = original_torch_load

            # Enregistrer dans le ModelManager centralis√©
            backend_name = TTSBackendEnum.CHATTERBOX_TURBO.value if self.turbo else TTSBackendEnum.CHATTERBOX.value
            self._register_model(
                model_id=self._model_id_multi,
                model_object=model_multilingual,
                backend=backend_name,
                priority=1  # Haute priorit√© - mod√®le multilingue tr√®s utilis√©
            )

            self._initialized_multilingual = True
            logger.info(f"‚úÖ [TTS] Chatterbox Multilingual initialis√© sur {device} (via ModelManager)")
            return True

        except Exception as e:
            logger.error(f"‚ùå [TTS] Erreur initialisation Chatterbox Multilingual: {e}")
            import traceback
            traceback.print_exc()
            return False

    # Valeurs par d√©faut des param√®tres Chatterbox optimis√©es pour fluidit√© et naturel
    # Ces valeurs ont √©t√© calibr√©es pour une synth√®se vocale fluide avec clonage
    # Recommandations ML Agent: cfg_weight min 0.30, repetition_penalty 1.20
    DEFAULT_PARAMS = {
        "exaggeration": 0.45,     # 0.0-1.0: Expressivit√© vocale (r√©duit pour plus de naturel)
        "cfg_weight": 0.50,       # 0.0-1.0: Guidance du mod√®le (augment√© pour √©viter artefacts)
        "temperature": 0.85,      # 0.0-2.0: Cr√©ativit√©/al√©atoire (√©quilibr√© pour fluidit√©)
        "repetition_penalty": 1.15,  # 1.0-3.0: P√©nalit√© r√©p√©tition (mono) - r√©duit pour moins de saccades
        "repetition_penalty_multilingual": 1.20,  # 1.0-3.0: P√©nalit√© r√©p√©tition (multi) - r√©duit pour plus de fluidit√©
        "min_p": 0.04,           # 0.0-1.0: Probabilit√© minimum sampling (l√©g√®rement r√©duit)
        "top_p": 0.95,           # 0.0-1.0: Nucleus sampling (l√©g√®rement r√©duit pour coh√©rence)
    }

    async def synthesize(
        self,
        text: str,
        language: str,
        speaker_audio_path: Optional[str] = None,
        output_path: str = None,
        # Param√®tres de clonage vocal
        exaggeration: Optional[float] = None,
        cfg_weight: Optional[float] = None,
        # Param√®tres de g√©n√©ration
        temperature: Optional[float] = None,
        repetition_penalty: Optional[float] = None,
        min_p: Optional[float] = None,
        top_p: Optional[float] = None,
        # Options
        auto_optimize_params: bool = True,
        voice_characteristics: Optional[VoiceCharacteristics] = None,
        # NOUVEAU: Conditionals Chatterbox pr√©-calcul√©s
        conditionals: Optional[Any] = None,  # Chatterbox Conditionals object
        **kwargs
    ) -> str:
        """
        Synth√®se vocale avec Chatterbox.

        Args:
            text: Texte √† synth√©tiser
            language: Code langue (ex: 'fr', 'en', 'es')
            speaker_audio_path: Chemin vers l'audio de r√©f√©rence pour le clonage
            output_path: Chemin de sortie

            PARAM√àTRES DE CLONAGE VOCAL:
            exaggeration: Expressivit√© (0.0-1.0). Amplifie les caract√©ristiques vocales.
                         0.0 = neutre, 1.0 = tr√®s expressif. Si None, auto-calcul√©
            cfg_weight: Guidance (0.0-1.0). Contr√¥le la fid√©lit√© au texte.
                       0.0 = cr√©atif, 1.0 = strict. Si None, auto-calcul√©

            PARAM√àTRES DE G√âN√âRATION:
            temperature: Cr√©ativit√© (0.0-2.0). Contr√¥le le caract√®re al√©atoire.
                        0.0 = d√©terministe, 2.0 = tr√®s cr√©atif. D√©faut: 0.8
            repetition_penalty: P√©nalit√© r√©p√©tition (1.0-3.0). √âvite les r√©p√©titions.
                               1.0 = pas de p√©nalit√©, 3.0 = forte p√©nalit√©.
                               D√©faut: 1.2 (mono), 2.0 (multi)
            min_p: Probabilit√© minimum (0.0-1.0). Filtre les tokens improbables.
                   D√©faut: 0.05
            top_p: Nucleus sampling (0.0-1.0). Limite aux tokens les plus probables.
                   D√©faut: 1.0

            OPTIONS:
            auto_optimize_params: Calculer automatiquement exaggeration/cfg_weight
            voice_characteristics: Caract√©ristiques vocales pr√©-analys√©es
            conditionals: Conditionals Chatterbox pr√©-calcul√©s pour √©viter de recalculer
                         √† chaque synth√®se. Si fourni, speaker_audio_path est ignor√©.
        """
        import torchaudio

        # Normaliser le code langue (ex: fr-FR -> fr)
        lang_code = language.split('-')[0].lower() if language else 'en'

        # D√©terminer si on utilise le mod√®le multilingue
        # Note: On utilise le multilingual pour TOUTES les langues support√©es,
        # y compris l'anglais, car le mod√®le est charg√© au d√©marrage.
        # Le mod√®le monolingual n'est plus utilis√© par d√©faut.
        use_multilingual = (
            lang_code in self.MULTILINGUAL_LANGUAGES and
            self._available_multilingual
        )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # AUTO-OPTIMISATION DES PARAM√àTRES DE CLONAGE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Si au moins un param√®tre n'est pas sp√©cifi√© et qu'on a un audio de r√©f√©rence
        any_param_missing = any(p is None for p in [exaggeration, cfg_weight, temperature, repetition_penalty, min_p, top_p])

        if auto_optimize_params and speaker_audio_path and os.path.exists(speaker_audio_path) and any_param_missing:
            optimal_params = await self._get_optimal_params(
                speaker_audio_path,
                lang_code,
                voice_characteristics
            )
            # Appliquer les param√®tres optimaux uniquement pour ceux non sp√©cifi√©s
            if exaggeration is None:
                exaggeration = optimal_params["exaggeration"]
            if cfg_weight is None:
                cfg_weight = optimal_params["cfg_weight"]
            if temperature is None:
                temperature = optimal_params.get("temperature", self.DEFAULT_PARAMS["temperature"])
            if repetition_penalty is None:
                repetition_penalty = optimal_params.get("repetition_penalty", self.DEFAULT_PARAMS["repetition_penalty"])
            if min_p is None:
                min_p = optimal_params.get("min_p", self.DEFAULT_PARAMS["min_p"])
            if top_p is None:
                top_p = optimal_params.get("top_p", self.DEFAULT_PARAMS["top_p"])

            logger.info(
                f"[TTS] Auto-optimisation: exp={exaggeration:.2f}, cfg={cfg_weight:.2f}, "
                f"temp={temperature:.2f}, rep_pen={repetition_penalty:.2f} "
                f"({optimal_params.get('explanation', '')})"
            )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # APPLIQUER LES VALEURS PAR D√âFAUT
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if exaggeration is None:
            exaggeration = self.DEFAULT_PARAMS["exaggeration"]
        if cfg_weight is None:
            cfg_weight = self.DEFAULT_PARAMS["cfg_weight"]
        if temperature is None:
            temperature = self.DEFAULT_PARAMS["temperature"]
        if repetition_penalty is None:
            repetition_penalty = (
                self.DEFAULT_PARAMS["repetition_penalty_multilingual"]
                if use_multilingual
                else self.DEFAULT_PARAMS["repetition_penalty"]
            )
        if min_p is None:
            min_p = self.DEFAULT_PARAMS["min_p"]
        if top_p is None:
            top_p = self.DEFAULT_PARAMS["top_p"]

        logger.debug(
            f"[TTS] Params: lang={lang_code}, exp={exaggeration:.2f}, cfg={cfg_weight:.2f}, "
            f"temp={temperature:.2f}, rep_pen={repetition_penalty:.2f}, min_p={min_p:.2f}, top_p={top_p:.2f}"
        )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # INITIALISATION DU MOD√àLE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        model_mono = None
        model_multi = None

        if use_multilingual:
            if not self._initialized_multilingual:
                await self.initialize_multilingual()

            model_multi = self._get_model(self._model_id_multi)
            if not model_multi:
                logger.warning(f"[TTS] Multilingual non disponible, fallback sur monolingual pour {lang_code}")
                use_multilingual = False

        if not use_multilingual:
            if not self._initialized:
                await self.initialize()

            model_mono = self._get_model(self._model_id)
            if not model_mono:
                raise RuntimeError("Chatterbox non initialis√©")

        loop = asyncio.get_event_loop()

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # G√âN√âRATION AUDIO
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if use_multilingual:
            # Pour le clonage cross-langue, cfg_weight r√©duit am√©liore la qualit√©
            # IMPORTANT: cfg_weight=0.0 d√©sactive compl√®tement la guidance et cause
            # des artefacts vocaux impr√©visibles. Valeur minimum recommand√©e: 0.30
            # pour maintenir l'articulation tout en r√©duisant le transfert d'accent.
            # ML Agent recommendation: minimum 0.30 pour √©viter les artefacts et r√©p√©titions
            if lang_code != 'en':
                # R√©duire l√©g√®rement le cfg pour langues non-anglaises mais garder minimum 0.30
                effective_cfg = max(0.30, cfg_weight * 0.7)  # Minimum 0.30, sinon 70% de la valeur
                logger.debug(f"[TTS] Cross-language cfg_weight: {cfg_weight:.2f} ‚Üí {effective_cfg:.2f} (langue: {lang_code})")
            else:
                effective_cfg = cfg_weight

            # Capturer les param√®tres locaux pour le lambda
            _model = model_multi
            _text = text
            _lang_code = lang_code
            _speaker = speaker_audio_path
            _exp = exaggeration
            _cfg = effective_cfg
            _temp = temperature
            _rep_pen = repetition_penalty
            _min_p = min_p
            _top_p = top_p
            _conditionals = conditionals

            # Si des conditionals pr√©-calcul√©s sont fournis, les utiliser directement
            if _conditionals is not None:
                logger.info("[CHATTERBOX] üé§ Utilisation des conditionals pr√©-calcul√©s pour le clonage")
                _model.conds = _conditionals
                # G√©n√©rer sans audio_prompt_path puisque les conditionals sont d√©j√† pr√™ts
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        text=_text,
                        language_id=_lang_code,
                        # PAS de audio_prompt_path car conditionals d√©j√† charg√©s
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )
            elif speaker_audio_path and os.path.exists(speaker_audio_path):
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        text=_text,
                        language_id=_lang_code,
                        audio_prompt_path=_speaker,
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )
            else:
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        text=_text,
                        language_id=_lang_code,
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )

            sample_rate = model_multi.sr
            logger.debug(f"[TTS] Synth√®se multilingue: {lang_code}")
        else:
            # Capturer les param√®tres locaux pour le lambda
            _model = model_mono
            _text = text
            _speaker = speaker_audio_path
            _exp = exaggeration
            _cfg = cfg_weight
            _temp = temperature
            _rep_pen = repetition_penalty
            _min_p = min_p
            _top_p = top_p
            _conditionals = conditionals

            # Si des conditionals pr√©-calcul√©s sont fournis, les utiliser directement
            if _conditionals is not None:
                logger.info("[CHATTERBOX] üé§ Utilisation des conditionals pr√©-calcul√©s pour le clonage")
                _model.conds = _conditionals
                # G√©n√©rer sans audio_prompt_path puisque les conditionals sont d√©j√† pr√™ts
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        _text,
                        # PAS de audio_prompt_path car conditionals d√©j√† charg√©s
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )
            elif speaker_audio_path and os.path.exists(speaker_audio_path):
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        _text,
                        audio_prompt_path=_speaker,
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )
            else:
                wav = await loop.run_in_executor(
                    None,
                    lambda: _model.generate(
                        _text,
                        exaggeration=_exp,
                        cfg_weight=_cfg,
                        temperature=_temp,
                        repetition_penalty=_rep_pen,
                        min_p=_min_p,
                        top_p=_top_p
                    )
                )

            sample_rate = model_mono.sr
            logger.debug(f"[TTS] Synth√®se monolingual: en")

        await loop.run_in_executor(
            None,
            lambda: torchaudio.save(output_path, wav, sample_rate)
        )

        return output_path

    async def _get_optimal_params(
        self,
        speaker_audio_path: str,
        target_language: str,
        voice_characteristics: Optional[VoiceCharacteristics] = None
    ) -> dict:
        """
        Calcule les param√®tres optimaux bas√©s sur l'analyse vocale.

        Args:
            speaker_audio_path: Chemin vers l'audio de r√©f√©rence
            target_language: Langue cible
            voice_characteristics: Caract√©ristiques pr√©-analys√©es (optionnel)

        Returns:
            Dict avec tous les param√®tres Chatterbox optimis√©s:
            - exaggeration, cfg_weight, temperature, repetition_penalty, min_p, top_p
        """
        try:
            analyzer = VoiceAnalyzerService()
            await analyzer.initialize()

            # Utiliser les caract√©ristiques fournies ou analyser
            if voice_characteristics is None:
                characteristics = await analyzer.analyze(speaker_audio_path)
            else:
                characteristics = voice_characteristics

            # Obtenir les param√®tres optimaux (tous les 6 param√®tres)
            optimal = analyzer.get_optimal_clone_params(characteristics, target_language)

            logger.debug(
                f"[TTS] Analyse vocale: type={characteristics.voice_type}, "
                f"pitch={characteristics.pitch_mean:.1f}Hz, "
                f"expressivit√©={optimal['analysis']['expressiveness_score']:.2f}, "
                f"stabilit√©={optimal['analysis']['stability_score']:.2f}"
            )
            logger.debug(
                f"[TTS] Params optimaux: exp={optimal['exaggeration']:.2f}, "
                f"cfg={optimal['cfg_weight']:.2f}, temp={optimal['temperature']:.2f}, "
                f"rep_pen={optimal['repetition_penalty']:.2f}, "
                f"min_p={optimal['min_p']:.3f}, top_p={optimal['top_p']:.2f}"
            )

            return optimal

        except Exception as e:
            logger.warning(f"[TTS] Erreur auto-optimisation, utilisation valeurs par d√©faut: {e}")
            return {
                "exaggeration": self.DEFAULT_PARAMS["exaggeration"],
                "cfg_weight": self.DEFAULT_PARAMS["cfg_weight"],
                "temperature": self.DEFAULT_PARAMS["temperature"],
                "repetition_penalty": self.DEFAULT_PARAMS["repetition_penalty"],
                "min_p": self.DEFAULT_PARAMS["min_p"],
                "top_p": self.DEFAULT_PARAMS["top_p"],
                "confidence": 0.0,
                "explanation": "Valeurs par d√©faut (erreur analyse)"
            }

    async def prepare_voice_conditionals(
        self,
        audio_path: str,
        exaggeration: float = 0.5,
        serialize: bool = False
    ) -> Tuple[Optional[Any], Optional[bytes]]:
        """
        Pr√©pare les conditionals Chatterbox √† partir d'un fichier audio.

        Ces conditionals peuvent √™tre stock√©s et r√©utilis√©s pour √©viter
        de recalculer √† chaque synth√®se.

        Args:
            audio_path: Chemin vers l'audio de r√©f√©rence
            exaggeration: Niveau d'expressivit√© (0.0-1.0)
            serialize: Si True, retourne aussi les bytes s√©rialis√©s

        Returns:
            Tuple (Conditionals, bytes_s√©rialis√©s) ou (Conditionals, None)
        """
        if not self.is_initialized and not self._initialized_multilingual:
            logger.warning("[CHATTERBOX] Mod√®le non initialis√© pour pr√©parer conditionals")
            return None, None

        if not audio_path or not os.path.exists(audio_path):
            logger.warning(f"[CHATTERBOX] Audio non trouv√©: {audio_path}")
            return None, None

        try:
            # Prioriser le mod√®le multilingual s'il est disponible
            model = None
            if self._initialized_multilingual:
                model = self._get_model(self._model_id_multi)
            elif self._initialized:
                model = self._get_model(self._model_id)

            if not model:
                logger.warning("[CHATTERBOX] Aucun mod√®le disponible pour pr√©parer conditionals")
                return None, None

            loop = asyncio.get_event_loop()

            def _prepare():
                model.prepare_conditionals(audio_path, exaggeration=exaggeration)
                return model.conds

            conditionals = await loop.run_in_executor(None, _prepare)
            logger.info(f"[CHATTERBOX] ‚úÖ Conditionals pr√©par√©s depuis {audio_path}")

            # S√©rialiser si demand√©
            serialized = None
            if serialize and conditionals:
                serialized = await self.serialize_conditionals(conditionals)

            return conditionals, serialized

        except Exception as e:
            logger.error(f"[CHATTERBOX] ‚ùå Erreur pr√©paration conditionals: {e}")
            return None, None

    async def serialize_conditionals(self, conditionals) -> Optional[bytes]:
        """
        S√©rialise les conditionals Chatterbox en bytes pour stockage.

        Args:
            conditionals: Objet Conditionals Chatterbox

        Returns:
            Bytes s√©rialis√©s ou None si √©chec
        """
        if conditionals is None:
            return None

        try:
            import torch

            # Cr√©er un buffer en m√©moire
            buffer = io.BytesIO()

            # S√©rialiser avec torch.save (comme Conditionals.save())
            arg_dict = dict(
                t3=conditionals.t3.__dict__,
                gen=conditionals.gen
            )
            torch.save(arg_dict, buffer)

            # Retourner les bytes
            buffer.seek(0)
            serialized_bytes = buffer.read()

            logger.info(f"[CHATTERBOX] ‚úÖ Conditionals s√©rialis√©s ({len(serialized_bytes)} bytes)")
            return serialized_bytes

        except Exception as e:
            logger.error(f"[CHATTERBOX] ‚ùå Erreur s√©rialisation conditionals: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def deserialize_conditionals(self, data: bytes, device: str = "cpu"):
        """
        D√©s√©rialise les conditionals Chatterbox depuis bytes.

        Args:
            data: Bytes s√©rialis√©s
            device: Device cible (cpu, cuda, mps)

        Returns:
            Objet Conditionals ou None si √©chec
        """
        if data is None or len(data) == 0:
            return None

        try:
            import torch
            from chatterbox.tts import Conditionals
            from chatterbox.models.t3.modules.cond_enc import T3Cond

            # Charger depuis le buffer
            buffer = io.BytesIO(data)
            map_location = torch.device(device)

            kwargs = torch.load(buffer, map_location=map_location, weights_only=True)
            conditionals = Conditionals(T3Cond(**kwargs['t3']), kwargs['gen'])

            # D√©placer vers le device appropri√©
            conditionals = conditionals.to(device)

            logger.info(f"[CHATTERBOX] ‚úÖ Conditionals d√©s√©rialis√©s ({len(data)} bytes)")
            return conditionals

        except Exception as e:
            logger.error(f"[CHATTERBOX] ‚ùå Erreur d√©s√©rialisation conditionals: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def close(self):
        # NOTE: Les mod√®les sont g√©r√©s par le ModelManager centralis√©
        # Ils seront automatiquement √©vict√©s via LRU si m√©moire faible
        self._initialized = False
        self._initialized_multilingual = False
        logger.info("[TTS] Chatterbox ferm√© (mod√®les g√©r√©s par ModelManager)")
