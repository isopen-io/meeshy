"""
Service de traduction ML unifiÃ© - Architecture centralisÃ©e
Un seul service ML qui charge les modÃ¨les au dÃ©marrage et sert tous les canaux
"""

import os
import logging
import time
import asyncio
import re
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path

# CRITIQUE: Charger les variables d'environnement AVANT tout import
try:
    from dotenv import load_dotenv
    # Charger .env puis .env.local (override)
    env_path = Path(__file__).parent.parent.parent / '.env'
    env_local_path = Path(__file__).parent.parent.parent / '.env.local'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    if env_local_path.exists():
        load_dotenv(env_local_path, override=True)
        print(f"ğŸ”§ [ML-SERVICE] .env.local chargÃ© depuis: {env_local_path}")
        print(f"ğŸ”§ [ML-SERVICE] MODELS_PATH: {os.getenv('MODELS_PATH', 'NOT SET')}")
except ImportError:
    print("âš ï¸ [ML-SERVICE] python-dotenv non disponible")

# Import des settings
from config.settings import get_settings

# CRITIQUE: DÃ©finir les variables d'environnement AVANT d'importer transformers
# Transformers lit ces variables au moment de l'import
_settings = get_settings()
os.environ['HF_HOME'] = str(_settings.models_path)
os.environ['TRANSFORMERS_CACHE'] = str(_settings.models_path)
os.environ['HUGGINGFACE_HUB_CACHE'] = str(_settings.models_path)
print(f"ğŸ”§ [ML-SERVICE] Variables HuggingFace dÃ©finies: {_settings.models_path}")

# Import du module de segmentation pour prÃ©servation de structure
from utils.text_segmentation import TextSegmenter

# Import des optimisations de performance Linux/CUDA
from utils.performance import (
    PerformanceOptimizer,
    PerformanceConfig,
    BatchProcessor,
    TranslationPriorityQueue,
    Priority,
    get_performance_optimizer,
    create_inference_context
)

# Import du cache Redis pour segment-level caching
CACHE_AVAILABLE = False
_translation_cache = None
try:
    from services.redis_service import get_translation_cache_service
    CACHE_AVAILABLE = True
except ImportError:
    pass

# Import du ModelManager centralisÃ© pour gestion mÃ©moire GPU/CPU
MODEL_MANAGER_AVAILABLE = False
try:
    from services.model_manager import get_model_manager, ModelType
    MODEL_MANAGER_AVAILABLE = True
except ImportError:
    pass

# Import des modÃ¨les ML optimisÃ©s
try:
    import torch
    
    # SOLUTION: DÃ©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # DÃ©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

@dataclass
class TranslationResult:
    """RÃ©sultat d'une traduction unifiÃ©"""
    translated_text: str
    detected_language: str
    confidence: float
    model_used: str
    from_cache: bool
    processing_time: float
    source_channel: str  # 'zmq', 'rest', 'websocket'

class TranslationMLService:
    """
    Service de traduction ML unifiÃ© - Singleton
    Charge les modÃ¨les une seule fois au dÃ©marrage et sert tous les canaux
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern pour garantir une seule instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, settings, model_type: str = "all", max_workers: int = 4, quantization_level: str = "float16"):
        if self._initialized:
            return
            
        # Charger les settings
        self.settings = settings
        
        self.model_type = model_type
        # OPTIMISATION CPU MULTICORE: Utiliser 16 workers pour AMD 18 cores
        # Laisser 2 cores pour l'OS et les opÃ©rations systÃ¨me
        import os
        cpu_workers = min(max_workers, int(os.getenv('ML_MAX_WORKERS', '16')))
        self.max_workers = cpu_workers
        self.quantization_level = quantization_level
        self.executor = ThreadPoolExecutor(max_workers=cpu_workers)
        
        # ModÃ¨les ML chargÃ©s (partagÃ©s entre tous les canaux)
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
        # Cache thread-local de tokenizers pour Ã©viter "Already borrowed"
        self._thread_local_tokenizers = {}
        self._tokenizer_lock = threading.Lock()

        # OPTIMISATION CRITIQUE: Pool de pipelines thread-local pour Ã©viter la recrÃ©ation
        # Chaque thread a son propre pipeline rÃ©utilisable (Ã©vite 100-500ms overhead par requÃªte)
        self._thread_local_pipelines = {}
        self._pipeline_lock = threading.Lock()

        # Segmenteur de texte pour prÃ©servation de structure
        self.text_segmenter = TextSegmenter(max_segment_length=100)

        # OPTIMISATION: Performance optimizer pour Linux/CUDA
        self.perf_optimizer = get_performance_optimizer()
        self.perf_config = PerformanceConfig()

        # Configuration des modÃ¨les depuis les settings et .env
        self.models_path = Path(self.settings.models_path)
        logger.info(f"ğŸ” [ML-SERVICE] models_path configurÃ©: {self.models_path}")
        logger.info(f"ğŸ” [ML-SERVICE] models_path existe: {self.models_path.exists()}")
        logger.info(f"ğŸ” [ML-SERVICE] HF_HOME env: {os.getenv('HF_HOME', 'NOT SET')}")
        logger.info(f"ğŸ” [ML-SERVICE] TRANSFORMERS_CACHE env: {os.getenv('TRANSFORMERS_CACHE', 'NOT SET')}")
        self.device = os.getenv('DEVICE', 'cpu')
        
        # Deux modÃ¨les NLLB uniquement: basic (600M) et premium (1.3B)
        self.model_configs = {
            'basic': {
                'model_name': self.settings.basic_model,
                'local_path': self.models_path / self.settings.basic_model,
                'description': 'NLLB 600M - Rapide, bonne qualitÃ©',
                'device': self.device,
                'priority': 1  # ChargÃ© en premier
            },
            'premium': {
                'model_name': self.settings.premium_model,
                'local_path': self.models_path / self.settings.premium_model,
                'description': 'NLLB 1.3B - Haute qualitÃ©',
                'device': self.device,
                'priority': 2
            }
        }
        # Alias pour compatibilitÃ©
        self.model_configs['medium'] = self.model_configs['basic']
        
        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn', 
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }
        
        
        # Stats globales (partagÃ©es entre tous les canaux)
        self.stats = {
            'translations_count': 0,
            'zmq_translations': 0,
            'rest_translations': 0,
            'websocket_translations': 0,
            'avg_processing_time': 0.0,
            'models_loaded': False,
            'startup_time': None
        }
        self.request_times = []
        
        # Ã‰tat d'initialisation
        self.is_initialized = False
        self.is_loading = False
        self._startup_lock = asyncio.Lock()
        
        self._initialized = True
        self._configure_environment()
        logger.info(f"ğŸ¤– Service ML UnifiÃ© crÃ©Ã© (Singleton) avec {max_workers} workers")
    
    def _configure_environment(self):
        """Configure les variables d'environnement basÃ©es sur les settings"""
        import os
        
        # OPTIMISATION XET: Configuration pour rÃ©duire les warnings du nouveau systÃ¨me
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # OPTIMISATION RÃ‰SEAU: Configuration pour amÃ©liorer la connectivitÃ© Docker
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(self.settings.huggingface_timeout)
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = str(self.settings.model_download_max_retries)
        
        # SOLUTION: DÃ©sactiver les tensors meta pour Ã©viter l'erreur Tensor.item()
        os.environ['PYTORCH_DISABLE_META'] = '1'
        os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
        os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
        
        # Configuration pour Ã©viter les problÃ¨mes de proxy/corporate network
        # VÃ©rifier si le fichier de certificats existe, sinon utiliser le systÃ¨me par dÃ©faut
        if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        else:
            # Utiliser le systÃ¨me par dÃ©faut
            logger.info("âš ï¸ Fichier de certificats SSL non trouvÃ©, utilisation du systÃ¨me par dÃ©faut")
        
        # Option pour dÃ©sactiver temporairement la vÃ©rification SSL si nÃ©cessaire
        if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['CURL_CA_BUNDLE'] = ''
            logger.info("âš ï¸ VÃ©rification SSL dÃ©sactivÃ©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    async def initialize(self) -> bool:
        """Initialise les modÃ¨les ML une seule fois au dÃ©marrage"""
        async with self._startup_lock:
            if self.is_initialized:
                logger.info("âœ… Service ML dÃ©jÃ  initialisÃ©")
                return True
                
            if self.is_loading:
                logger.info("â³ Initialisation ML en cours...")
                # Attendre que l'initialisation se termine
                while self.is_loading and not self.is_initialized:
                    await asyncio.sleep(0.5)
                return self.is_initialized
            
            self.is_loading = True
            startup_start = time.time()
            
            if not ML_AVAILABLE:
                logger.error("âŒ Transformers non disponible. Service ML dÃ©sactivÃ©.")
                self.is_loading = False
                return False
            
            try:
                logger.info("ğŸš€ Initialisation du Service ML UnifiÃ©...")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # OPTIMISATION LINUX/CUDA: Initialiser le performance optimizer
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                if ML_AVAILABLE:
                    # Initialiser les optimisations Linux/CUDA
                    self.device = self.perf_optimizer.initialize()
                    logger.info(f"âš™ï¸ Device configurÃ© via PerformanceOptimizer: {self.device}")

                    # Configuration supplÃ©mentaire des threads PyTorch pour AMD multicore
                    # NOTE: set_num_interop_threads ne peut Ãªtre appelÃ© qu'une seule fois
                    # avant le dÃ©but du travail parallÃ¨le. On ignore l'erreur si dÃ©jÃ  configurÃ©.
                    try:
                        torch.set_num_threads(self.perf_config.num_omp_threads)
                        torch.set_num_interop_threads(2)  # 2 threads pour opÃ©rations inter-op
                    except RuntimeError as e:
                        if "interop threads" in str(e):
                            logger.debug(f"âš™ï¸ Threads PyTorch dÃ©jÃ  configurÃ©s (rÃ©utilisation)")
                        else:
                            raise
                    logger.info(f"âš™ï¸ PyTorch configurÃ©: {torch.get_num_threads()} threads intra-op, {torch.get_num_interop_threads()} threads inter-op")

                    if self.perf_optimizer.cuda_available:
                        logger.info(f"ğŸ® CUDA disponible: {torch.cuda.get_device_name(0)}")
                    else:
                        logger.info(f"ğŸ–¥ï¸ Mode CPU avec optimisations Linux")

                logger.info("ğŸ“š Chargement des modÃ¨les NLLB...")
                
                # Charger les modÃ¨les par ordre de prioritÃ©
                models_to_load = sorted(
                    self.model_configs.items(), 
                    key=lambda x: x[1]['priority']
                )
                
                for model_type, config in models_to_load:
                    try:
                        await self._load_model(model_type)
                    except Exception as e:
                        logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                        # Continuer avec les autres modÃ¨les
                
                # VÃ©rifier qu'au moins un modÃ¨le est chargÃ©
                if not self.models:
                    logger.error("âŒ Aucun modÃ¨le ML chargÃ©")
                    self.is_loading = False
                    return False
                
                startup_time = time.time() - startup_start
                self.stats['startup_time'] = startup_time
                self.stats['models_loaded'] = True
                self.is_initialized = True
                self.is_loading = False
                
                logger.info(f"âœ… Service ML UnifiÃ© initialisÃ© en {startup_time:.2f}s")
                logger.info(f"ğŸ“Š ModÃ¨les chargÃ©s: {list(self.models.keys())}")
                logger.info(f"ğŸ¯ PrÃªt Ã  servir tous les canaux: ZMQ, REST, WebSocket")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ Erreur critique initialisation ML: {e}")
                self.is_loading = False
                return False
    
    def _get_thread_local_tokenizer(self, model_type: str) -> Optional[AutoTokenizer]:
        """Obtient ou crÃ©e un tokenizer pour le thread actuel (Ã©vite 'Already borrowed')"""
        import threading
        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"

        # VÃ©rifier le cache thread-local
        if cache_key in self._thread_local_tokenizers:
            return self._thread_local_tokenizers[cache_key]

        # CrÃ©er un nouveau tokenizer pour ce thread
        with self._tokenizer_lock:
            # Double-check aprÃ¨s acquisition du lock
            if cache_key in self._thread_local_tokenizers:
                return self._thread_local_tokenizers[cache_key]

            try:
                model_name = self.model_configs[model_type]['model_name']
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path),
                    use_fast=True
                )
                self._thread_local_tokenizers[cache_key] = tokenizer
                logger.debug(f"âœ… Tokenizer thread-local crÃ©Ã©: {cache_key}")
                return tokenizer
            except Exception as e:
                logger.error(f"âŒ Erreur crÃ©ation tokenizer thread-local: {e}")
                return None

    def _get_thread_local_pipeline(self, model_type: str):
        """
        OPTIMISATION CRITIQUE: Obtient ou crÃ©e un pipeline de traduction pour le thread actuel.

        Au lieu de crÃ©er un pipeline Ã  chaque traduction (100-500ms overhead),
        on rÃ©utilise le pipeline du thread. Gains attendus: 3-5x plus rapide.

        Returns:
            tuple: (pipeline, nllb_codes_supported) ou (None, False) si erreur
        """
        import threading
        from transformers import pipeline as create_pipeline

        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"

        # VÃ©rifier le cache thread-local
        if cache_key in self._thread_local_pipelines:
            return self._thread_local_pipelines[cache_key], True

        # CrÃ©er un nouveau pipeline pour ce thread
        with self._pipeline_lock:
            # Double-check aprÃ¨s acquisition du lock
            if cache_key in self._thread_local_pipelines:
                return self._thread_local_pipelines[cache_key], True

            try:
                if model_type not in self.models:
                    logger.error(f"âŒ ModÃ¨le {model_type} non chargÃ©")
                    return None, False

                shared_model = self.models[model_type]
                thread_tokenizer = self._get_thread_local_tokenizer(model_type)

                if thread_tokenizer is None:
                    logger.error(f"âŒ Tokenizer non disponible pour {model_type}")
                    return None, False

                # CrÃ©er le pipeline UNE SEULE FOIS pour ce thread
                new_pipeline = create_pipeline(
                    "translation",
                    model=shared_model,
                    tokenizer=thread_tokenizer,
                    device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                    max_length=512,
                    batch_size=8  # OptimisÃ© pour multicore
                )

                self._thread_local_pipelines[cache_key] = new_pipeline
                logger.info(f"âœ… Pipeline thread-local crÃ©Ã©: {cache_key} (sera rÃ©utilisÃ©)")
                return new_pipeline, True

            except Exception as e:
                logger.error(f"âŒ Erreur crÃ©ation pipeline thread-local: {e}")
                return None, False
    
    async def _load_model(self, model_type: str):
        """Charge un modÃ¨le spÃ©cifique depuis local ou HuggingFace"""
        if model_type in self.models:
            return  # DÃ©jÃ  chargÃ©

        config = self.model_configs[model_type]
        model_name = config['model_name']

        # OPTIMISATION: VÃ©rifier si ce modÃ¨le exact est dÃ©jÃ  chargÃ© sous un autre nom
        # (ex: 'medium' et 'basic' utilisent le mÃªme modÃ¨le 600M)
        for existing_type, existing_model in self.models.items():
            existing_config = self.model_configs.get(existing_type)
            if existing_config and existing_config['model_name'] == model_name:
                # RÃ©utiliser le modÃ¨le dÃ©jÃ  chargÃ© au lieu de le recharger
                self.models[model_type] = existing_model
                self.tokenizers[model_type] = self.tokenizers[existing_type]
                logger.info(f"â™»ï¸ ModÃ¨le {model_type} rÃ©utilise {existing_type}: {model_name}")
                return
        local_path = config['local_path']
        device = config['device']
        
        logger.info(f"ğŸ“¥ Chargement {model_type}: {model_name}")
        
        # Charger dans un thread pour Ã©viter de bloquer
        def load_model():
            try:
                # Tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=str(self.models_path),
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512  # Limiter la taille
                )
                
                # ModÃ¨le avec quantification
                # OPTIMISATION CPU: Utiliser float32 au lieu de float16 sur CPU pour Ã©viter les erreurs
                # et amÃ©liorer la compatibilitÃ©. Sur CPU, float16 n'apporte pas d'accÃ©lÃ©ration.
                dtype = torch.float32 if device == "cpu" else (
                    getattr(torch, self.quantization_level) if hasattr(torch, self.quantization_level) else torch.float32
                )
                
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path), 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,  # Optimisation mÃ©moire
                    device_map="auto" if device == "cuda" else None
                )
                
                # OPTIMISATION CPU: Mettre le modÃ¨le en mode eval pour dÃ©sactiver dropout
                model.eval()
                
                # CORRECTION: Pas de pipeline partagÃ© pour Ã©viter "Already borrowed"
                # On crÃ©e les pipelines Ã  la demande dans _ml_translate
                
                return tokenizer, model
                
            except Exception as e:
                logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                return None, None
        
        # Charger de maniÃ¨re asynchrone
        loop = asyncio.get_event_loop()
        tokenizer, model = await loop.run_in_executor(self.executor, load_model)
        
        if model and tokenizer:
            self.tokenizers[model_type] = tokenizer

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # OPTIMISATION: torch.compile pour accÃ©lÃ©rer l'infÃ©rence
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.perf_config.enable_torch_compile:
                model = self.perf_optimizer.compile_model(model, f"nllb_{model_type}")

            self.models[model_type] = model

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # OPTIMISATION: Enregistrer dans le ModelManager centralisÃ©
            # Permet la gestion globale de la mÃ©moire GPU/CPU
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if MODEL_MANAGER_AVAILABLE:
                try:
                    model_manager = get_model_manager()
                    model_manager.register_model(
                        model_id=f"translation_{model_type}",
                        model_type=ModelType.TRANSLATION,
                        model_name=model_name,
                        model_object=model,
                        priority=1 if model_type == 'basic' else 2  # basic = haute prioritÃ©
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ Impossible d'enregistrer dans ModelManager: {e}")

            logger.info(f"âœ… ModÃ¨le {model_type} chargÃ©: {model_name}")
            if local_path.exists():
                logger.info(f"ğŸ“ ModÃ¨le disponible en local: {local_path}")
        else:
            raise Exception(f"Ã‰chec chargement {model_type}")
    
    async def translate(self, text: str, source_language: str = "auto", 
                       target_language: str = "en", model_type: str = "basic",
                       source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Interface unique de traduction pour tous les canaux
        source_channel: 'zmq', 'rest', 'websocket'
        """
        start_time = time.time()
        
        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")
            
            # VÃ©rifier que le service est initialisÃ©
            if not self.is_initialized:
                logger.warning("Service ML non initialisÃ©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # Fallback si modÃ¨le spÃ©cifique pas disponible  
            if model_type not in self.models:
                # Utiliser le premier modÃ¨le disponible
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"ModÃ¨le demandÃ© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # DÃ©tecter la langue source si nÃ©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)
            
            # Traduire avec le vrai modÃ¨le ML
            translated_text = await self._ml_translate(text, detected_lang, target_language, model_type)
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)
            
            result = {
                'translated_text': translated_text,
                'detected_language': detected_lang,
                'confidence': 0.95,  # Confiance Ã©levÃ©e pour les vrais modÃ¨les
                'model_used': f"{model_type}_ml",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
            logger.info(f"âœ… [ML-{source_channel.upper()}] '{text[:20]}...' â†’ '{translated_text[:20]}...' ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur traduction ML [{source_channel}]: {e}")
            # Fallback en cas d'erreur
            return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

    async def translate_with_structure(self, text: str, source_language: str = "auto",
                                      target_language: str = "en", model_type: str = "basic",
                                      source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Traduction avec prÃ©servation de structure (paragraphes, emojis, sauts de ligne)

        Cette mÃ©thode segmente le texte, traduit chaque segment sÃ©parÃ©ment,
        puis rÃ©assemble en prÃ©servant la structure originale

        AMÃ‰LIORATION: SÃ©lection automatique du modÃ¨le selon la longueur du texte
        """
        start_time = time.time()

        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")

            # AMÃ‰LIORATION: SÃ©lection automatique du modÃ¨le selon la longueur
            # - Textes < 50 chars: basic (rapide)
            # - Textes >= 50 chars: medium (meilleure qualitÃ©)
            # - Textes >= 200 chars: premium si disponible (qualitÃ© maximale)
            text_length = len(text)
            original_model_type = model_type

            if text_length >= 200 and 'premium' in self.models:
                model_type = 'premium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars â†’ Using PREMIUM model for best quality")
            elif text_length >= 50 and 'medium' in self.models:
                model_type = 'medium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars â†’ Using MEDIUM model for better quality")
            elif model_type not in self.models and 'basic' in self.models:
                model_type = 'basic'
                logger.info(f"[STRUCTURED] Requested model not available â†’ Using BASIC model")

            if model_type != original_model_type:
                logger.info(f"[STRUCTURED] Model switched: {original_model_type} â†’ {model_type}")

            # VÃ©rifier si le texte est court et sans structure complexe
            if len(text) <= 100 and '\n\n' not in text and not self.text_segmenter.extract_emojis(text)[1]:
                # Texte simple, utiliser la traduction standard
                logger.debug(f"[STRUCTURED] Text is simple, using standard translation")
                return await self.translate(text, source_language, target_language, model_type, source_channel)

            logger.info(f"[STRUCTURED] Starting structured translation: {len(text)} chars")

            # VÃ©rifier que le service est initialisÃ©
            if not self.is_initialized:
                logger.warning("Service ML non initialisÃ©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # Fallback si modÃ¨le spÃ©cifique pas disponible
            if model_type not in self.models:
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"ModÃ¨le demandÃ© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # DÃ©tecter la langue source si nÃ©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)

            # 1. Segmenter le texte (extraction emojis + dÃ©coupage par paragraphes)
            segments, emojis_map = self.text_segmenter.segment_text(text)
            logger.info(f"[STRUCTURED] Text segmented into {len(segments)} parts with {len(emojis_map)} emojis")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # OPTIMISATION: BATCH ML + CACHE PARALLÃˆLE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 1. VÃ©rifier le cache pour TOUS les segments en parallÃ¨le
            # 2. Collecter les segments NON-CACHÃ‰S
            # 3. Traduire en UN SEUL appel batch ML (30-50% plus rapide)
            # 4. Distribuer les rÃ©sultats et mettre en cache

            # Initialiser le cache si disponible
            global _translation_cache
            if CACHE_AVAILABLE and _translation_cache is None:
                try:
                    _translation_cache = get_translation_cache_service()
                except Exception:
                    pass

            parallel_start = time.time()
            cache_hits = 0
            translated_segments = [None] * len(segments)

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Ã‰TAPE 1: VÃ©rifier le cache pour tous les segments en parallÃ¨le
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            segments_to_translate = []  # Liste de (idx, text) Ã  traduire

            async def check_cache_for_segment(idx: int, segment: dict) -> tuple:
                """VÃ©rifie le cache pour un segment, retourne (idx, cached_text ou None)"""
                segment_type = segment.get('type', 'line')

                # PrÃ©server les sÃ©parateurs, lignes vides et blocs de code
                if segment_type in ['paragraph_break', 'separator', 'empty_line', 'code']:
                    return (idx, segment, 'preserved')

                if segment_type == 'line':
                    segment_text = segment.get('text', '')
                    if not segment_text.strip():
                        return (idx, segment, 'empty')

                    # VÃ©rifier le cache
                    if _translation_cache:
                        try:
                            cached = await _translation_cache.get_translation(
                                text=segment_text,
                                source_lang=detected_lang,
                                target_lang=target_language,
                                model_type=model_type
                            )
                            if cached:
                                return (idx, {'type': 'line', 'text': cached.get('translated_text', segment_text)}, 'cached')
                        except Exception:
                            pass

                    # Pas en cache, Ã  traduire
                    return (idx, segment_text, 'to_translate')

                return (idx, segment, 'preserved')

            # VÃ©rifier le cache en parallÃ¨le
            cache_tasks = [check_cache_for_segment(i, seg) for i, seg in enumerate(segments)]
            cache_results = await asyncio.gather(*cache_tasks, return_exceptions=True)

            # Traiter les rÃ©sultats du cache
            for result in cache_results:
                if isinstance(result, Exception):
                    continue

                idx, data, status = result

                if status == 'cached':
                    translated_segments[idx] = data
                    cache_hits += 1
                elif status in ['preserved', 'empty']:
                    translated_segments[idx] = data
                elif status == 'to_translate':
                    segments_to_translate.append((idx, data))

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Ã‰TAPE 2: Traduire les segments non-cachÃ©s en BATCH
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if segments_to_translate:
                logger.info(f"[BATCH-STRUCT] âš¡ {len(segments_to_translate)} segments Ã  traduire en batch (cache hits: {cache_hits})")

                # Extraire les textes Ã  traduire
                indices = [item[0] for item in segments_to_translate]
                texts_to_translate = [item[1] for item in segments_to_translate]

                try:
                    # Appel BATCH ML - traduit tous les segments en une fois
                    translated_texts = await self._ml_translate_batch(
                        texts=texts_to_translate,
                        source_lang=detected_lang,
                        target_lang=target_language,
                        model_type=model_type
                    )

                    # Distribuer les rÃ©sultats et mettre en cache
                    cache_items = []  # Liste de (original_text, translated_text) Ã  cacher
                    for i, (idx, original_text) in enumerate(segments_to_translate):
                        translated_text = translated_texts[i] if i < len(translated_texts) else original_text
                        translated_segments[idx] = {'type': 'line', 'text': translated_text}
                        cache_items.append((original_text, translated_text))

                    # Ã‰crire tous les rÃ©sultats en cache en parallÃ¨le (fire-and-forget)
                    if _translation_cache and cache_items:
                        async def cache_all_segments():
                            for orig_text, trans_text in cache_items:
                                try:
                                    await _translation_cache.set_translation(
                                        text=orig_text,
                                        source_lang=detected_lang,
                                        target_lang=target_language,
                                        translated_text=trans_text,
                                        model_type=model_type
                                    )
                                except Exception:
                                    pass

                        # Lancer le caching en arriÃ¨re-plan sans bloquer
                        asyncio.create_task(cache_all_segments())

                except Exception as e:
                    logger.error(f"[BATCH-STRUCT] Erreur batch ML: {e}")
                    # Fallback: traduire individuellement
                    for idx, text in segments_to_translate:
                        try:
                            translated = await self._ml_translate(text, detected_lang, target_language, model_type)
                            translated_segments[idx] = {'type': 'line', 'text': translated}
                        except Exception:
                            translated_segments[idx] = {'type': 'line', 'text': text}

            # Remplacer les None par les segments originaux (fallback)
            errors_count = 0
            for i, seg in enumerate(translated_segments):
                if seg is None:
                    translated_segments[i] = segments[i]
                    errors_count += 1

            parallel_time = (time.time() - parallel_start) * 1000
            batch_size = len(segments_to_translate) if segments_to_translate else 0
            logger.info(f"[BATCH-STRUCT] âœ… {len(segments)} segments traitÃ©s en {parallel_time:.0f}ms (cache: {cache_hits}, batch: {batch_size}, erreurs: {errors_count})")

            # 3. RÃ©assembler le texte traduit
            final_text = self.text_segmenter.reassemble_text(translated_segments, emojis_map)

            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)

            result = {
                'translated_text': final_text,
                'detected_language': detected_lang,
                'confidence': 0.95,
                'model_used': f"{model_type}_ml_structured",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel,
                'segments_count': len(segments),
                'emojis_count': len(emojis_map)
            }

            logger.info(f"âœ… [ML-STRUCTURED-{source_channel.upper()}] {len(text)}â†’{len(final_text)} chars, {len(segments)} segments, {len(emojis_map)} emojis ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur traduction structurÃ©e [{source_channel}]: {e}")
            # Fallback vers traduction standard en cas d'erreur
            return await self.translate(text, source_language, target_language, model_type, source_channel)

    async def _ml_translate(self, text: str, source_lang: str, target_lang: str, model_type: str) -> str:
        """
        Traduction avec le vrai modÃ¨le ML - OPTIMISÃ‰ avec pipeline rÃ©utilisable

        OPTIMISATION CRITIQUE APPLIQUÃ‰E:
        - Pipeline crÃ©Ã© UNE SEULE FOIS par thread et rÃ©utilisÃ©
        - Gains: 100-500ms Ã©conomisÃ©s par requÃªte (3-5x plus rapide)
        - Tokenizer thread-local pour Ã©viter 'Already borrowed'
        """
        try:
            if model_type not in self.models:
                raise Exception(f"ModÃ¨le {model_type} non chargÃ©")

            # Traduction dans un thread - OPTIMISATION: pipeline + tokenizer thread-local rÃ©utilisables
            def translate():
                try:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # OPTIMISATION CRITIQUE: RÃ©utiliser le pipeline au lieu de le recrÃ©er
                    # Ã‰conomie: 100-500ms par requÃªte
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    reusable_pipeline, is_available = self._get_thread_local_pipeline(model_type)

                    if not is_available or reusable_pipeline is None:
                        raise Exception(f"Pipeline non disponible pour {model_type}")

                    # NLLB: codes de langue spÃ©ciaux
                    nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                    nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # OPTIMISATION LINUX: inference_mode() pour dÃ©sactiver autograd
                    # Gains: ~15-20% vitesse, ~30% mÃ©moire en moins
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    with create_inference_context():
                        result = reusable_pipeline(
                            text,
                            src_lang=nllb_source,
                            tgt_lang=nllb_target,
                            max_length=512,
                            num_beams=4,  # Ã‰quilibre qualitÃ©/vitesse
                            early_stopping=True
                        )

                    # NLLB retourne translation_text
                    if result and len(result) > 0 and 'translation_text' in result[0]:
                        translated = result[0]['translation_text']
                    else:
                        translated = f"[NLLB-No-Result] {text}"

                    # NOTE: On ne supprime PAS le pipeline car il est rÃ©utilisÃ©

                    return translated

                except Exception as e:
                    logger.error(f"Erreur pipeline {model_type}: {e}")
                    return f"[ML-Pipeline-Error] {text}"

            # ExÃ©cuter de maniÃ¨re asynchrone
            loop = asyncio.get_event_loop()
            translated = await loop.run_in_executor(self.executor, translate)

            return translated

        except Exception as e:
            logger.error(f"âŒ Erreur modÃ¨le ML {model_type}: {e}")
            return f"[ML-Error] {text}"

    async def _ml_translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str,
        model_type: str
    ) -> List[str]:
        """
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        OPTIMISATION: Traduction BATCH pour traiter plusieurs textes Ã  la fois
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Avantages par rapport Ã  _ml_translate appelÃ© N fois:
        - Pipeline crÃ©Ã© UNE SEULE fois au lieu de N fois
        - Batch processing natif du modÃ¨le (padding optimisÃ©)
        - Meilleure utilisation GPU/CPU (pas de temps mort)
        - RÃ©duction overhead de 70% sur pipeline creation

        Args:
            texts: Liste de textes Ã  traduire
            source_lang: Code langue source (ex: 'fr')
            target_lang: Code langue cible (ex: 'en')
            model_type: Type de modÃ¨le ('basic', 'premium')

        Returns:
            Liste des textes traduits (mÃªme ordre que l'entrÃ©e)
        """
        if not texts:
            return []

        # Fallback vers traduction individuelle si peu de textes
        if len(texts) <= 2:
            results = []
            for text in texts:
                translated = await self._ml_translate(text, source_lang, target_lang, model_type)
                results.append(translated)
            return results

        try:
            if model_type not in self.models:
                raise Exception(f"ModÃ¨le {model_type} non chargÃ©")

            original_model_name = self.model_configs[model_type]['model_name']
            batch_size = self.perf_config.batch_size

            def translate_batch():
                try:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # OPTIMISATION: RÃ©utiliser le pipeline thread-local
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    reusable_pipeline, is_available = self._get_thread_local_pipeline(model_type)

                    if not is_available or reusable_pipeline is None:
                        raise Exception(f"Pipeline non disponible pour {model_type}")

                    nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                    nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                    all_results = []

                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # TRAITEMENT PAR CHUNKS AVEC inference_mode()
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    with create_inference_context():
                        for i in range(0, len(texts), batch_size):
                            chunk = texts[i:i + batch_size]

                            results = reusable_pipeline(
                                chunk,
                                src_lang=nllb_source,
                                tgt_lang=nllb_target,
                                max_length=512,
                                num_beams=4,
                                early_stopping=True
                            )

                            for result in results:
                                if isinstance(result, dict) and 'translation_text' in result:
                                    all_results.append(result['translation_text'])
                                elif isinstance(result, list) and len(result) > 0:
                                    all_results.append(result[0].get('translation_text', '[No-Result]'))
                                else:
                                    all_results.append('[Batch-No-Result]')

                    # NOTE: On ne supprime PAS le pipeline car il est rÃ©utilisÃ©

                    # Nettoyage mÃ©moire pÃ©riodique
                    if self.perf_config.enable_memory_cleanup and len(texts) > 20:
                        self.perf_optimizer.cleanup_memory()

                    return all_results

                except Exception as e:
                    logger.error(f"Erreur batch pipeline {model_type}: {e}")
                    return [f"[ML-Batch-Error] {t}" for t in texts]

            # ExÃ©cuter de maniÃ¨re asynchrone
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(self.executor, translate_batch)

            logger.info(f"âš¡ [BATCH] {len(texts)} textes traduits en batch ({source_lang}â†’{target_lang})")
            return results

        except Exception as e:
            logger.error(f"âŒ Erreur batch ML {model_type}: {e}")
            # Fallback: traduction individuelle
            results = []
            for text in texts:
                try:
                    translated = await self._ml_translate(text, source_lang, target_lang, model_type)
                    results.append(translated)
                except Exception:
                    results.append(f"[ML-Error] {text}")
            return results

    def _detect_language(self, text: str) -> str:
        """DÃ©tection de langue simple"""
        text_lower = text.lower()
        
        # Mots caractÃ©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # DÃ©faut
    
    async def _fallback_translate(self, text: str, source_lang: str, target_lang: str, 
                                 model_type: str, source_channel: str) -> Dict[str, Any]:
        """Traduction de fallback si ML non disponible"""
        logger.warning(f"Utilisation du fallback pour {model_type} [{source_channel}]")
        
        # Dictionnaire simple comme fallback
        translations = {
            ('fr', 'en'): {
                'bonjour': 'hello', 'comment': 'how', 'vous': 'you', 'allez': 'are',
                'Ãªtes': 'are', 'tout': 'all', 'le': 'the', 'monde': 'world'
            },
            ('en', 'fr'): {
                'hello': 'bonjour', 'how': 'comment', 'you': 'vous', 'are': 'Ãªtes',
                'all': 'tout', 'the': 'le', 'world': 'monde'
            },
            ('es', 'fr'): {
                'hola': 'bonjour', 'como': 'comment', 'estas': 'allez-vous'
            },
            ('en', 'de'): {
                'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie'
            }
        }
        
        # Traduction simple mot par mot
        lang_pair = (source_lang, target_lang)
        if lang_pair in translations:
            words = text.lower().split()
            translated_words = []
            for word in words:
                translated_word = translations[lang_pair].get(word, word)
                translated_words.append(translated_word)
            translated_text = ' '.join(translated_words)
        else:
            translated_text = f"[FALLBACK-{source_lang}â†’{target_lang}] {text}"
        
        self._update_stats(0.001, source_channel)
        
        return {
            'translated_text': translated_text,
            'detected_language': source_lang,
            'confidence': 0.3,  # Faible confiance pour fallback
            'model_used': f"{model_type}_fallback",
            'from_cache': False,
            'processing_time': 0.001,
            'source_channel': source_channel
        }
    
    def _update_stats(self, processing_time: float, source_channel: str):
        """Met Ã  jour les statistiques globales"""
        self.stats['translations_count'] += 1
        
        # Mettre Ã  jour les stats par canal (canaux connus seulement)
        if source_channel in ['zmq', 'rest', 'websocket']:
            self.stats[f'{source_channel}_translations'] += 1
        
        self.request_times.append(processing_time)
        
        if len(self.request_times) > 200:
            self.request_times = self.request_times[-200:]
        
        if self.request_times:
            self.stats['avg_processing_time'] = sum(self.request_times) / len(self.request_times)
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques globales du service"""
        return {
            'service_type': 'unified_ml',
            'is_singleton': True,
            'translations_count': self.stats['translations_count'],
            'zmq_translations': self.stats['zmq_translations'],
            'rest_translations': self.stats['rest_translations'], 
            'websocket_translations': self.stats['websocket_translations'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'models_loaded': {
                model_type: {
                    'name': self.model_configs[model_type]['model_name'],
                    'description': self.model_configs[model_type]['description'],
                    'local_path': str(self.model_configs[model_type]['local_path']),
                    'is_local': self.model_configs[model_type]['local_path'].exists()
                } for model_type in self.models.keys()
            },
            'ml_available': ML_AVAILABLE,
            'is_initialized': self.is_initialized,
            'startup_time': self.stats['startup_time'],
            'supported_languages': list(self.lang_codes.keys()),
            'models_path': str(self.models_path),
            'device': self.device
        }
    
    async def get_health(self) -> Dict[str, Any]:
        """Health check du service unifiÃ©"""
        return {
            'status': 'healthy' if self.is_initialized else 'initializing',
            'models_count': len(self.models),
            'pipelines_count': len(self.pipelines),
            'ml_available': ML_AVAILABLE,
            'translations_served': self.stats['translations_count']
        }

    async def close(self):
        """Ferme proprement le service ML et libÃ¨re les ressources"""
        logger.info("ğŸ›‘ ArrÃªt du service ML unifiÃ©...")

        try:
            # 1. ArrÃªter le ThreadPoolExecutor
            if hasattr(self, 'executor') and self.executor:
                self.executor.shutdown(wait=False)
                logger.info("âœ… ThreadPoolExecutor arrÃªtÃ©")

            # 2. LibÃ©rer les modÃ¨les de la mÃ©moire
            if ML_AVAILABLE and self.models:
                for model_type in list(self.models.keys()):
                    try:
                        del self.models[model_type]
                    except Exception:
                        pass
                self.models.clear()
                logger.info("âœ… ModÃ¨les ML libÃ©rÃ©s de la mÃ©moire")

            # 3. LibÃ©rer les tokenizers et pipelines thread-local
            if self.tokenizers:
                self.tokenizers.clear()
                self._thread_local_tokenizers.clear()
                logger.info("âœ… Tokenizers libÃ©rÃ©s")

            # LibÃ©rer les pipelines thread-local
            if hasattr(self, '_thread_local_pipelines') and self._thread_local_pipelines:
                self._thread_local_pipelines.clear()
                logger.info("âœ… Pipelines thread-local libÃ©rÃ©s")

            # 4. LibÃ©rer les pipelines
            if self.pipelines:
                self.pipelines.clear()

            # 5. Nettoyage mÃ©moire GPU/CPU
            if ML_AVAILABLE:
                try:
                    import gc
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logger.info("âœ… Cache CUDA vidÃ©")
                except Exception:
                    pass

            # 6. RÃ©initialiser l'Ã©tat
            self.is_initialized = False
            self.is_loading = False
            self.stats['models_loaded'] = False

            # 7. IMPORTANT: RÃ©initialiser le singleton pour permettre une nouvelle instance
            # Ceci permet aux tests de recrÃ©er un service ML propre
            self._initialized = False
            TranslationMLService._instance = None

            logger.info("âœ… Service ML unifiÃ© arrÃªtÃ© proprement")

        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'arrÃªt du service ML: {e}")

# Instance globale du service (Singleton)
def get_unified_ml_service(max_workers: int = 4) -> TranslationMLService:
    """Retourne l'instance unique du service ML"""
    return TranslationMLService(get_settings(), max_workers=max_workers)
