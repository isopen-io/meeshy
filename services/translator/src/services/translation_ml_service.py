"""
Service de traduction ML unifiÃ© - Architecture centralisÃ©e
Un seul service ML qui charge les modÃ¨les au dÃ©marrage et sert tous les canaux
"""

import os
import logging
import time
import asyncio
import re
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path

# CRITIQUE: Charger les variables d'environnement AVANT tout import
try:
    from dotenv import load_dotenv
    # Charger .env puis .env.local (override)
    env_path = Path(__file__).parent.parent.parent / '.env'
    env_local_path = Path(__file__).parent.parent.parent / '.env.local'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    if env_local_path.exists():
        load_dotenv(env_local_path, override=True)
        print(f"ğŸ”§ [ML-SERVICE] .env.local chargÃ© depuis: {env_local_path}")
        print(f"ğŸ”§ [ML-SERVICE] MODELS_PATH: {os.getenv('MODELS_PATH', 'NOT SET')}")
except ImportError:
    print("âš ï¸ [ML-SERVICE] python-dotenv non disponible")

# Import des settings
from config.settings import get_settings

# CRITIQUE: DÃ©finir les variables d'environnement AVANT d'importer transformers
# Transformers lit ces variables au moment de l'import
_settings = get_settings()
os.environ['HF_HOME'] = str(_settings.models_path)
os.environ['TRANSFORMERS_CACHE'] = str(_settings.models_path)
os.environ['HUGGINGFACE_HUB_CACHE'] = str(_settings.models_path)
print(f"ğŸ”§ [ML-SERVICE] Variables HuggingFace dÃ©finies: {_settings.models_path}")

# Import du module de segmentation pour prÃ©servation de structure
from utils.text_segmentation import TextSegmenter

# Import des optimisations de performance Linux/CUDA
from utils.performance import (
    PerformanceOptimizer,
    PerformanceConfig,
    BatchProcessor,
    TranslationPriorityQueue,
    Priority,
    get_performance_optimizer,
    create_inference_context
)

# Import du cache Redis pour segment-level caching
CACHE_AVAILABLE = False
_translation_cache = None
try:
    from services.redis_service import get_translation_cache_service
    CACHE_AVAILABLE = True
except ImportError:
    pass

# Import des modÃ¨les ML optimisÃ©s
try:
    import torch
    
    # SOLUTION: DÃ©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # DÃ©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

@dataclass
class TranslationResult:
    """RÃ©sultat d'une traduction unifiÃ©"""
    translated_text: str
    detected_language: str
    confidence: float
    model_used: str
    from_cache: bool
    processing_time: float
    source_channel: str  # 'zmq', 'rest', 'websocket'

class TranslationMLService:
    """
    Service de traduction ML unifiÃ© - Singleton
    Charge les modÃ¨les une seule fois au dÃ©marrage et sert tous les canaux
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern pour garantir une seule instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, settings, model_type: str = "all", max_workers: int = 4, quantization_level: str = "float16"):
        if self._initialized:
            return
            
        # Charger les settings
        self.settings = settings
        
        self.model_type = model_type
        # OPTIMISATION CPU MULTICORE: Utiliser 16 workers pour AMD 18 cores
        # Laisser 2 cores pour l'OS et les opÃ©rations systÃ¨me
        import os
        cpu_workers = min(max_workers, int(os.getenv('ML_MAX_WORKERS', '16')))
        self.max_workers = cpu_workers
        self.quantization_level = quantization_level
        self.executor = ThreadPoolExecutor(max_workers=cpu_workers)
        
        # ModÃ¨les ML chargÃ©s (partagÃ©s entre tous les canaux)
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
        # Cache thread-local de tokenizers pour Ã©viter "Already borrowed"
        self._thread_local_tokenizers = {}
        self._tokenizer_lock = threading.Lock()

        # Segmenteur de texte pour prÃ©servation de structure
        self.text_segmenter = TextSegmenter(max_segment_length=100)

        # OPTIMISATION: Performance optimizer pour Linux/CUDA
        self.perf_optimizer = get_performance_optimizer()
        self.perf_config = PerformanceConfig()

        # Configuration des modÃ¨les depuis les settings et .env
        self.models_path = Path(self.settings.models_path)
        logger.info(f"ğŸ” [ML-SERVICE] models_path configurÃ©: {self.models_path}")
        logger.info(f"ğŸ” [ML-SERVICE] models_path existe: {self.models_path.exists()}")
        logger.info(f"ğŸ” [ML-SERVICE] HF_HOME env: {os.getenv('HF_HOME', 'NOT SET')}")
        logger.info(f"ğŸ” [ML-SERVICE] TRANSFORMERS_CACHE env: {os.getenv('TRANSFORMERS_CACHE', 'NOT SET')}")
        self.device = os.getenv('DEVICE', 'cpu')
        
        # Deux modÃ¨les NLLB uniquement: basic (600M) et premium (1.3B)
        self.model_configs = {
            'basic': {
                'model_name': self.settings.basic_model,
                'local_path': self.models_path / self.settings.basic_model,
                'description': 'NLLB 600M - Rapide, bonne qualitÃ©',
                'device': self.device,
                'priority': 1  # ChargÃ© en premier
            },
            'premium': {
                'model_name': self.settings.premium_model,
                'local_path': self.models_path / self.settings.premium_model,
                'description': 'NLLB 1.3B - Haute qualitÃ©',
                'device': self.device,
                'priority': 2
            }
        }
        # Alias pour compatibilitÃ©
        self.model_configs['medium'] = self.model_configs['basic']
        
        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn', 
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }
        
        
        # Stats globales (partagÃ©es entre tous les canaux)
        self.stats = {
            'translations_count': 0,
            'zmq_translations': 0,
            'rest_translations': 0,
            'websocket_translations': 0,
            'avg_processing_time': 0.0,
            'models_loaded': False,
            'startup_time': None
        }
        self.request_times = []
        
        # Ã‰tat d'initialisation
        self.is_initialized = False
        self.is_loading = False
        self._startup_lock = asyncio.Lock()
        
        self._initialized = True
        self._configure_environment()
        logger.info(f"ğŸ¤– Service ML UnifiÃ© crÃ©Ã© (Singleton) avec {max_workers} workers")
    
    def _configure_environment(self):
        """Configure les variables d'environnement basÃ©es sur les settings"""
        import os
        
        # OPTIMISATION XET: Configuration pour rÃ©duire les warnings du nouveau systÃ¨me
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # OPTIMISATION RÃ‰SEAU: Configuration pour amÃ©liorer la connectivitÃ© Docker
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(self.settings.huggingface_timeout)
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = str(self.settings.model_download_max_retries)
        
        # SOLUTION: DÃ©sactiver les tensors meta pour Ã©viter l'erreur Tensor.item()
        os.environ['PYTORCH_DISABLE_META'] = '1'
        os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
        os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
        
        # Configuration pour Ã©viter les problÃ¨mes de proxy/corporate network
        # VÃ©rifier si le fichier de certificats existe, sinon utiliser le systÃ¨me par dÃ©faut
        if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        else:
            # Utiliser le systÃ¨me par dÃ©faut
            logger.info("âš ï¸ Fichier de certificats SSL non trouvÃ©, utilisation du systÃ¨me par dÃ©faut")
        
        # Option pour dÃ©sactiver temporairement la vÃ©rification SSL si nÃ©cessaire
        if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['CURL_CA_BUNDLE'] = ''
            logger.info("âš ï¸ VÃ©rification SSL dÃ©sactivÃ©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    async def initialize(self) -> bool:
        """Initialise les modÃ¨les ML une seule fois au dÃ©marrage"""
        async with self._startup_lock:
            if self.is_initialized:
                logger.info("âœ… Service ML dÃ©jÃ  initialisÃ©")
                return True
                
            if self.is_loading:
                logger.info("â³ Initialisation ML en cours...")
                # Attendre que l'initialisation se termine
                while self.is_loading and not self.is_initialized:
                    await asyncio.sleep(0.5)
                return self.is_initialized
            
            self.is_loading = True
            startup_start = time.time()
            
            if not ML_AVAILABLE:
                logger.error("âŒ Transformers non disponible. Service ML dÃ©sactivÃ©.")
                self.is_loading = False
                return False
            
            try:
                logger.info("ğŸš€ Initialisation du Service ML UnifiÃ©...")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # OPTIMISATION LINUX/CUDA: Initialiser le performance optimizer
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                if ML_AVAILABLE:
                    # Initialiser les optimisations Linux/CUDA
                    self.device = self.perf_optimizer.initialize()
                    logger.info(f"âš™ï¸ Device configurÃ© via PerformanceOptimizer: {self.device}")

                    # Configuration supplÃ©mentaire des threads PyTorch pour AMD multicore
                    torch.set_num_threads(self.perf_config.num_omp_threads)
                    torch.set_num_interop_threads(2)  # 2 threads pour opÃ©rations inter-op
                    logger.info(f"âš™ï¸ PyTorch configurÃ©: {torch.get_num_threads()} threads intra-op, {torch.get_num_interop_threads()} threads inter-op")

                    if self.perf_optimizer.cuda_available:
                        logger.info(f"ğŸ® CUDA disponible: {torch.cuda.get_device_name(0)}")
                    else:
                        logger.info(f"ğŸ–¥ï¸ Mode CPU avec optimisations Linux")

                logger.info("ğŸ“š Chargement des modÃ¨les NLLB...")
                
                # Charger les modÃ¨les par ordre de prioritÃ©
                models_to_load = sorted(
                    self.model_configs.items(), 
                    key=lambda x: x[1]['priority']
                )
                
                for model_type, config in models_to_load:
                    try:
                        await self._load_model(model_type)
                    except Exception as e:
                        logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                        # Continuer avec les autres modÃ¨les
                
                # VÃ©rifier qu'au moins un modÃ¨le est chargÃ©
                if not self.models:
                    logger.error("âŒ Aucun modÃ¨le ML chargÃ©")
                    self.is_loading = False
                    return False
                
                startup_time = time.time() - startup_start
                self.stats['startup_time'] = startup_time
                self.stats['models_loaded'] = True
                self.is_initialized = True
                self.is_loading = False
                
                logger.info(f"âœ… Service ML UnifiÃ© initialisÃ© en {startup_time:.2f}s")
                logger.info(f"ğŸ“Š ModÃ¨les chargÃ©s: {list(self.models.keys())}")
                logger.info(f"ğŸ¯ PrÃªt Ã  servir tous les canaux: ZMQ, REST, WebSocket")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ Erreur critique initialisation ML: {e}")
                self.is_loading = False
                return False
    
    def _get_thread_local_tokenizer(self, model_type: str) -> Optional[AutoTokenizer]:
        """Obtient ou crÃ©e un tokenizer pour le thread actuel (Ã©vite 'Already borrowed')"""
        import threading
        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"
        
        # VÃ©rifier le cache thread-local
        if cache_key in self._thread_local_tokenizers:
            return self._thread_local_tokenizers[cache_key]
        
        # CrÃ©er un nouveau tokenizer pour ce thread
        with self._tokenizer_lock:
            # Double-check aprÃ¨s acquisition du lock
            if cache_key in self._thread_local_tokenizers:
                return self._thread_local_tokenizers[cache_key]
            
            try:
                model_name = self.model_configs[model_type]['model_name']
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path),
                    use_fast=True
                )
                self._thread_local_tokenizers[cache_key] = tokenizer
                logger.debug(f"âœ… Tokenizer thread-local crÃ©Ã©: {cache_key}")
                return tokenizer
            except Exception as e:
                logger.error(f"âŒ Erreur crÃ©ation tokenizer thread-local: {e}")
                return None
    
    async def _load_model(self, model_type: str):
        """Charge un modÃ¨le spÃ©cifique depuis local ou HuggingFace"""
        if model_type in self.models:
            return  # DÃ©jÃ  chargÃ©
        
        config = self.model_configs[model_type]
        model_name = config['model_name']
        local_path = config['local_path']
        device = config['device']
        
        logger.info(f"ğŸ“¥ Chargement {model_type}: {model_name}")
        
        # Charger dans un thread pour Ã©viter de bloquer
        def load_model():
            try:
                # Tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=str(self.models_path),
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512  # Limiter la taille
                )
                
                # ModÃ¨le avec quantification
                # OPTIMISATION CPU: Utiliser float32 au lieu de float16 sur CPU pour Ã©viter les erreurs
                # et amÃ©liorer la compatibilitÃ©. Sur CPU, float16 n'apporte pas d'accÃ©lÃ©ration.
                dtype = torch.float32 if device == "cpu" else (
                    getattr(torch, self.quantization_level) if hasattr(torch, self.quantization_level) else torch.float32
                )
                
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path), 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,  # Optimisation mÃ©moire
                    device_map="auto" if device == "cuda" else None
                )
                
                # OPTIMISATION CPU: Mettre le modÃ¨le en mode eval pour dÃ©sactiver dropout
                model.eval()
                
                # CORRECTION: Pas de pipeline partagÃ© pour Ã©viter "Already borrowed"
                # On crÃ©e les pipelines Ã  la demande dans _ml_translate
                
                return tokenizer, model
                
            except Exception as e:
                logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                return None, None
        
        # Charger de maniÃ¨re asynchrone
        loop = asyncio.get_event_loop()
        tokenizer, model = await loop.run_in_executor(self.executor, load_model)
        
        if model and tokenizer:
            self.tokenizers[model_type] = tokenizer

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # OPTIMISATION: torch.compile pour accÃ©lÃ©rer l'infÃ©rence
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.perf_config.enable_torch_compile:
                model = self.perf_optimizer.compile_model(model, f"nllb_{model_type}")

            self.models[model_type] = model
            logger.info(f"âœ… ModÃ¨le {model_type} chargÃ©: {model_name}")
            if local_path.exists():
                logger.info(f"ğŸ“ ModÃ¨le disponible en local: {local_path}")
        else:
            raise Exception(f"Ã‰chec chargement {model_type}")
    
    async def translate(self, text: str, source_language: str = "auto", 
                       target_language: str = "en", model_type: str = "basic",
                       source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Interface unique de traduction pour tous les canaux
        source_channel: 'zmq', 'rest', 'websocket'
        """
        start_time = time.time()
        
        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")
            
            # VÃ©rifier que le service est initialisÃ©
            if not self.is_initialized:
                logger.warning("Service ML non initialisÃ©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # Fallback si modÃ¨le spÃ©cifique pas disponible  
            if model_type not in self.models:
                # Utiliser le premier modÃ¨le disponible
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"ModÃ¨le demandÃ© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # DÃ©tecter la langue source si nÃ©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)
            
            # Traduire avec le vrai modÃ¨le ML
            translated_text = await self._ml_translate(text, detected_lang, target_language, model_type)
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)
            
            result = {
                'translated_text': translated_text,
                'detected_language': detected_lang,
                'confidence': 0.95,  # Confiance Ã©levÃ©e pour les vrais modÃ¨les
                'model_used': f"{model_type}_ml",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
            logger.info(f"âœ… [ML-{source_channel.upper()}] '{text[:20]}...' â†’ '{translated_text[:20]}...' ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur traduction ML [{source_channel}]: {e}")
            # Fallback en cas d'erreur
            return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

    async def translate_with_structure(self, text: str, source_language: str = "auto",
                                      target_language: str = "en", model_type: str = "basic",
                                      source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Traduction avec prÃ©servation de structure (paragraphes, emojis, sauts de ligne)

        Cette mÃ©thode segmente le texte, traduit chaque segment sÃ©parÃ©ment,
        puis rÃ©assemble en prÃ©servant la structure originale

        AMÃ‰LIORATION: SÃ©lection automatique du modÃ¨le selon la longueur du texte
        """
        start_time = time.time()

        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")

            # AMÃ‰LIORATION: SÃ©lection automatique du modÃ¨le selon la longueur
            # - Textes < 50 chars: basic (rapide)
            # - Textes >= 50 chars: medium (meilleure qualitÃ©)
            # - Textes >= 200 chars: premium si disponible (qualitÃ© maximale)
            text_length = len(text)
            original_model_type = model_type

            if text_length >= 200 and 'premium' in self.models:
                model_type = 'premium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars â†’ Using PREMIUM model for best quality")
            elif text_length >= 50 and 'medium' in self.models:
                model_type = 'medium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars â†’ Using MEDIUM model for better quality")
            elif model_type not in self.models and 'basic' in self.models:
                model_type = 'basic'
                logger.info(f"[STRUCTURED] Requested model not available â†’ Using BASIC model")

            if model_type != original_model_type:
                logger.info(f"[STRUCTURED] Model switched: {original_model_type} â†’ {model_type}")

            # VÃ©rifier si le texte est court et sans structure complexe
            if len(text) <= 100 and '\n\n' not in text and not self.text_segmenter.extract_emojis(text)[1]:
                # Texte simple, utiliser la traduction standard
                logger.debug(f"[STRUCTURED] Text is simple, using standard translation")
                return await self.translate(text, source_language, target_language, model_type, source_channel)

            logger.info(f"[STRUCTURED] Starting structured translation: {len(text)} chars")

            # VÃ©rifier que le service est initialisÃ©
            if not self.is_initialized:
                logger.warning("Service ML non initialisÃ©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # Fallback si modÃ¨le spÃ©cifique pas disponible
            if model_type not in self.models:
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"ModÃ¨le demandÃ© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # DÃ©tecter la langue source si nÃ©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)

            # 1. Segmenter le texte (extraction emojis + dÃ©coupage par paragraphes)
            segments, emojis_map = self.text_segmenter.segment_text(text)
            logger.info(f"[STRUCTURED] Text segmented into {len(segments)} parts with {len(emojis_map)} emojis")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PARALLÃ‰LISATION DES SEGMENTS AVEC CACHE (TTL 1 mois)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Les segments sont INDÃ‰PENDANTS et peuvent Ãªtre traduits en parallÃ¨le.
            # Chaque segment est cachÃ© par hash(text + source + target + model).
            # Si le segment existe en cache, on le rÃ©utilise immÃ©diatement.

            # Initialiser le cache si disponible
            global _translation_cache
            if CACHE_AVAILABLE and _translation_cache is None:
                try:
                    _translation_cache = get_translation_cache_service()
                except Exception:
                    pass

            # Semaphore pour limiter la charge (max 5 traductions parallÃ¨les)
            max_concurrent = int(os.getenv('MAX_PARALLEL_SEGMENTS', '5'))
            semaphore = asyncio.Semaphore(max_concurrent)
            cache_hits = 0

            async def translate_segment_parallel(idx: int, segment: dict) -> tuple:
                """Traduit un segment en parallÃ¨le avec cache et gestion de concurrence"""
                nonlocal cache_hits
                segment_type = segment.get('type', 'line')

                # PrÃ©server les sÃ©parateurs, lignes vides et blocs de code
                if segment_type in ['paragraph_break', 'separator', 'empty_line', 'code']:
                    return (idx, segment)

                # Traduire uniquement les lignes de texte
                if segment_type == 'line':
                    segment_text = segment.get('text', '')
                    if not segment_text.strip():
                        return (idx, segment)

                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # VÃ‰RIFIER LE CACHE SEGMENT (hash-based, TTL 1 mois)
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if _translation_cache:
                        try:
                            cached = await _translation_cache.get_translation(
                                text=segment_text,
                                source_lang=detected_lang,
                                target_lang=target_language,
                                model_type=model_type
                            )
                            if cached:
                                cache_hits += 1
                                return (idx, {'type': 'line', 'text': cached.get('translated_text', segment_text)})
                        except Exception as cache_err:
                            logger.debug(f"[CACHE] Erreur lecture segment {idx}: {cache_err}")

                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # TRADUIRE SI PAS EN CACHE
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    async with semaphore:  # Limiter la concurrence
                        try:
                            translated = await self._ml_translate(
                                segment_text,
                                detected_lang,
                                target_language,
                                model_type
                            )

                            # Mettre en cache la traduction du segment (TTL 1 mois)
                            if _translation_cache:
                                try:
                                    await _translation_cache.set_translation(
                                        text=segment_text,
                                        source_lang=detected_lang,
                                        target_lang=target_language,
                                        translated_text=translated,
                                        model_type=model_type
                                    )
                                except Exception:
                                    pass  # Cache optionnel, ne pas bloquer

                            return (idx, {'type': 'line', 'text': translated})
                        except Exception as e:
                            logger.error(f"[PARALLEL] Erreur segment {idx}: {e}")
                            return (idx, segment)  # Garder l'original en cas d'erreur

                return (idx, segment)

            # Lancer toutes les traductions en parallÃ¨le
            parallel_start = time.time()
            tasks = [translate_segment_parallel(i, seg) for i, seg in enumerate(segments)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            parallel_time = (time.time() - parallel_start) * 1000

            # Reconstruire la liste ordonnÃ©e des segments traduits
            translated_segments = [None] * len(segments)
            errors_count = 0
            for result in results:
                if isinstance(result, Exception):
                    errors_count += 1
                    logger.error(f"[PARALLEL] Exception: {result}")
                else:
                    idx, translated_seg = result
                    translated_segments[idx] = translated_seg

            # Remplacer les None par les segments originaux (fallback)
            for i, seg in enumerate(translated_segments):
                if seg is None:
                    translated_segments[i] = segments[i]

            logger.info(f"[PARALLEL] âš¡ {len(segments)} segments traduits en {parallel_time:.0f}ms ({cache_hits} cache hits, {errors_count} erreurs)")

            # 3. RÃ©assembler le texte traduit
            final_text = self.text_segmenter.reassemble_text(translated_segments, emojis_map)

            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)

            result = {
                'translated_text': final_text,
                'detected_language': detected_lang,
                'confidence': 0.95,
                'model_used': f"{model_type}_ml_structured",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel,
                'segments_count': len(segments),
                'emojis_count': len(emojis_map)
            }

            logger.info(f"âœ… [ML-STRUCTURED-{source_channel.upper()}] {len(text)}â†’{len(final_text)} chars, {len(segments)} segments, {len(emojis_map)} emojis ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur traduction structurÃ©e [{source_channel}]: {e}")
            # Fallback vers traduction standard en cas d'erreur
            return await self.translate(text, source_language, target_language, model_type, source_channel)

    async def _ml_translate(self, text: str, source_lang: str, target_lang: str, model_type: str) -> str:
        """
        Traduction avec le vrai modÃ¨le ML - tokenizers thread-local pour Ã©viter 'Already borrowed'
        
        XXX: PARALLÃ‰LISATION OPPORTUNITÃ‰ #2 - Traduction batch pour multiples segments
        TODO: Cette mÃ©thode pourrait accepter une LISTE de textes au lieu d'un seul
        TODO: Avantages du batch processing:
              - RÃ©duire l'overhead de crÃ©ation de pipeline (1 fois au lieu de N fois)
              - Utiliser batch_size optimal du modÃ¨le (traiter 8-16 segments Ã  la fois)
              - Meilleure utilisation GPU/CPU (pas de temps mort entre segments)
        TODO: Signature suggÃ©rÃ©e:
              async def _ml_translate_batch(
                  self, 
                  texts: List[str], 
                  source_lang: str, 
                  target_lang: str, 
                  model_type: str
              ) -> List[str]:
                  # CrÃ©er pipeline UNE fois
                  # Traduire tous les textes en batch_size chunks
                  # Retourner rÃ©sultats dans le mÃªme ordre
        TODO: Gains attendus:
              - 3-5x plus rapide pour 10+ segments
              - RÃ©duction de 70% du temps de setup (pipeline creation)
              - Meilleure utilisation mÃ©moire GPU
        """
        try:
            if model_type not in self.models:
                raise Exception(f"ModÃ¨le {model_type} non chargÃ©")
            
            # CORRECTION: Sauvegarder le model_name original pour Ã©viter les collisions dans la boucle de fallback
            original_model_name = self.model_configs[model_type]['model_name']
            
            # Traduction dans un thread - OPTIMISATION: tokenizer thread-local + inference_mode
            def translate():
                try:
                    from transformers import pipeline

                    # ModÃ¨le partagÃ© (thread-safe en lecture)
                    shared_model = self.models[model_type]

                    # OPTIMISATION: Utiliser le tokenizer thread-local cachÃ© (Ã©vite recrÃ©ation)
                    thread_tokenizer = self._get_thread_local_tokenizer(model_type)
                    if thread_tokenizer is None:
                        raise Exception(f"Impossible d'obtenir le tokenizer pour {model_type}")

                    # NLLB: utiliser translation avec tokenizer thread-local
                    # OPTIMISATION MULTICORE: ParamÃ¨tres optimisÃ©s pour AMD 18 cores
                    temp_pipeline = pipeline(
                        "translation",
                        model=shared_model,
                        tokenizer=thread_tokenizer,  # â† TOKENIZER THREAD-LOCAL
                        device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                        max_length=512,  # AugmentÃ© pour qualitÃ©
                        batch_size=8  # OptimisÃ© pour multicore
                    )

                    # NLLB: codes de langue spÃ©ciaux
                    nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                    nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # OPTIMISATION LINUX: inference_mode() pour dÃ©sactiver autograd
                    # Gains: ~15-20% vitesse, ~30% mÃ©moire en moins
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    with create_inference_context():
                        result = temp_pipeline(
                            text,
                            src_lang=nllb_source,
                            tgt_lang=nllb_target,
                            max_length=512,
                            num_beams=4,  # Ã‰quilibre qualitÃ©/vitesse
                            early_stopping=True
                        )

                    # NLLB retourne translation_text
                    if result and len(result) > 0 and 'translation_text' in result[0]:
                        translated = result[0]['translation_text']
                    else:
                        translated = f"[NLLB-No-Result] {text}"

                    # Nettoyer pipeline temporaire
                    del temp_pipeline

                    return translated

                except Exception as e:
                    logger.error(f"Erreur pipeline {original_model_name}: {e}")
                    return f"[ML-Pipeline-Error] {text}"
            
            # ExÃ©cuter de maniÃ¨re asynchrone
            loop = asyncio.get_event_loop()
            translated = await loop.run_in_executor(self.executor, translate)
            
            return translated
            
        except Exception as e:
            logger.error(f"âŒ Erreur modÃ¨le ML {model_type}: {e}")
            return f"[ML-Error] {text}"

    async def _ml_translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str,
        model_type: str
    ) -> List[str]:
        """
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        OPTIMISATION: Traduction BATCH pour traiter plusieurs textes Ã  la fois
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Avantages par rapport Ã  _ml_translate appelÃ© N fois:
        - Pipeline crÃ©Ã© UNE SEULE fois au lieu de N fois
        - Batch processing natif du modÃ¨le (padding optimisÃ©)
        - Meilleure utilisation GPU/CPU (pas de temps mort)
        - RÃ©duction overhead de 70% sur pipeline creation

        Args:
            texts: Liste de textes Ã  traduire
            source_lang: Code langue source (ex: 'fr')
            target_lang: Code langue cible (ex: 'en')
            model_type: Type de modÃ¨le ('basic', 'premium')

        Returns:
            Liste des textes traduits (mÃªme ordre que l'entrÃ©e)
        """
        if not texts:
            return []

        # Fallback vers traduction individuelle si peu de textes
        if len(texts) <= 2:
            results = []
            for text in texts:
                translated = await self._ml_translate(text, source_lang, target_lang, model_type)
                results.append(translated)
            return results

        try:
            if model_type not in self.models:
                raise Exception(f"ModÃ¨le {model_type} non chargÃ©")

            original_model_name = self.model_configs[model_type]['model_name']
            batch_size = self.perf_config.batch_size

            def translate_batch():
                try:
                    from transformers import pipeline

                    shared_model = self.models[model_type]
                    thread_tokenizer = self._get_thread_local_tokenizer(model_type)

                    if thread_tokenizer is None:
                        raise Exception(f"Impossible d'obtenir le tokenizer pour {model_type}")

                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # CRÃ‰ATION DU PIPELINE UNE SEULE FOIS
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    batch_pipeline = pipeline(
                        "translation",
                        model=shared_model,
                        tokenizer=thread_tokenizer,
                        device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                        max_length=512,
                        batch_size=batch_size
                    )

                    nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                    nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')

                    all_results = []

                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # TRAITEMENT PAR CHUNKS AVEC inference_mode()
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    with create_inference_context():
                        for i in range(0, len(texts), batch_size):
                            chunk = texts[i:i + batch_size]

                            results = batch_pipeline(
                                chunk,
                                src_lang=nllb_source,
                                tgt_lang=nllb_target,
                                max_length=512,
                                num_beams=4,
                                early_stopping=True
                            )

                            for result in results:
                                if isinstance(result, dict) and 'translation_text' in result:
                                    all_results.append(result['translation_text'])
                                elif isinstance(result, list) and len(result) > 0:
                                    all_results.append(result[0].get('translation_text', '[No-Result]'))
                                else:
                                    all_results.append('[Batch-No-Result]')

                    del batch_pipeline

                    # Nettoyage mÃ©moire pÃ©riodique
                    if self.perf_config.enable_memory_cleanup and len(texts) > 20:
                        self.perf_optimizer.cleanup_memory()

                    return all_results

                except Exception as e:
                    logger.error(f"Erreur batch pipeline {original_model_name}: {e}")
                    return [f"[ML-Batch-Error] {t}" for t in texts]

            # ExÃ©cuter de maniÃ¨re asynchrone
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(self.executor, translate_batch)

            logger.info(f"âš¡ [BATCH] {len(texts)} textes traduits en batch ({source_lang}â†’{target_lang})")
            return results

        except Exception as e:
            logger.error(f"âŒ Erreur batch ML {model_type}: {e}")
            # Fallback: traduction individuelle
            results = []
            for text in texts:
                try:
                    translated = await self._ml_translate(text, source_lang, target_lang, model_type)
                    results.append(translated)
                except Exception:
                    results.append(f"[ML-Error] {text}")
            return results

    def _detect_language(self, text: str) -> str:
        """DÃ©tection de langue simple"""
        text_lower = text.lower()
        
        # Mots caractÃ©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # DÃ©faut
    
    async def _fallback_translate(self, text: str, source_lang: str, target_lang: str, 
                                 model_type: str, source_channel: str) -> Dict[str, Any]:
        """Traduction de fallback si ML non disponible"""
        logger.warning(f"Utilisation du fallback pour {model_type} [{source_channel}]")
        
        # Dictionnaire simple comme fallback
        translations = {
            ('fr', 'en'): {
                'bonjour': 'hello', 'comment': 'how', 'vous': 'you', 'allez': 'are',
                'Ãªtes': 'are', 'tout': 'all', 'le': 'the', 'monde': 'world'
            },
            ('en', 'fr'): {
                'hello': 'bonjour', 'how': 'comment', 'you': 'vous', 'are': 'Ãªtes',
                'all': 'tout', 'the': 'le', 'world': 'monde'
            },
            ('es', 'fr'): {
                'hola': 'bonjour', 'como': 'comment', 'estas': 'allez-vous'
            },
            ('en', 'de'): {
                'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie'
            }
        }
        
        # Traduction simple mot par mot
        lang_pair = (source_lang, target_lang)
        if lang_pair in translations:
            words = text.lower().split()
            translated_words = []
            for word in words:
                translated_word = translations[lang_pair].get(word, word)
                translated_words.append(translated_word)
            translated_text = ' '.join(translated_words)
        else:
            translated_text = f"[FALLBACK-{source_lang}â†’{target_lang}] {text}"
        
        self._update_stats(0.001, source_channel)
        
        return {
            'translated_text': translated_text,
            'detected_language': source_lang,
            'confidence': 0.3,  # Faible confiance pour fallback
            'model_used': f"{model_type}_fallback",
            'from_cache': False,
            'processing_time': 0.001,
            'source_channel': source_channel
        }
    
    def _update_stats(self, processing_time: float, source_channel: str):
        """Met Ã  jour les statistiques globales"""
        self.stats['translations_count'] += 1
        
        # Mettre Ã  jour les stats par canal (canaux connus seulement)
        if source_channel in ['zmq', 'rest', 'websocket']:
            self.stats[f'{source_channel}_translations'] += 1
        
        self.request_times.append(processing_time)
        
        if len(self.request_times) > 200:
            self.request_times = self.request_times[-200:]
        
        if self.request_times:
            self.stats['avg_processing_time'] = sum(self.request_times) / len(self.request_times)
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques globales du service"""
        return {
            'service_type': 'unified_ml',
            'is_singleton': True,
            'translations_count': self.stats['translations_count'],
            'zmq_translations': self.stats['zmq_translations'],
            'rest_translations': self.stats['rest_translations'], 
            'websocket_translations': self.stats['websocket_translations'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'models_loaded': {
                model_type: {
                    'name': self.model_configs[model_type]['model_name'],
                    'description': self.model_configs[model_type]['description'],
                    'local_path': str(self.model_configs[model_type]['local_path']),
                    'is_local': self.model_configs[model_type]['local_path'].exists()
                } for model_type in self.models.keys()
            },
            'ml_available': ML_AVAILABLE,
            'is_initialized': self.is_initialized,
            'startup_time': self.stats['startup_time'],
            'supported_languages': list(self.lang_codes.keys()),
            'models_path': str(self.models_path),
            'device': self.device
        }
    
    async def get_health(self) -> Dict[str, Any]:
        """Health check du service unifiÃ©"""
        return {
            'status': 'healthy' if self.is_initialized else 'initializing',
            'models_count': len(self.models),
            'pipelines_count': len(self.pipelines),
            'ml_available': ML_AVAILABLE,
            'translations_served': self.stats['translations_count']
        }

# Instance globale du service (Singleton)
def get_unified_ml_service(max_workers: int = 4) -> TranslationMLService:
    """Retourne l'instance unique du service ML"""
    return TranslationMLService(get_settings(), max_workers=max_workers)
