"""
Service de clonage vocal - Singleton
G√®re les mod√®les de voix des utilisateurs avec cache et am√©lioration continue.
Architecture: OpenVoice V2 pour extraction d'embedding, cache fichier pour persistance.
"""

import os
import logging
import time
import asyncio
import threading
import pickle
import json
import hashlib
import struct
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path

# Configuration du logging
logger = logging.getLogger(__name__)

# Flags de disponibilit√© des d√©pendances
OPENVOICE_AVAILABLE = False
AUDIO_PROCESSING_AVAILABLE = False

try:
    from openvoice import se_extractor
    from openvoice.api import ToneColorConverter
    OPENVOICE_AVAILABLE = True
    logger.info("‚úÖ [VOICE_CLONE] OpenVoice disponible")
except ImportError:
    logger.warning("‚ö†Ô∏è [VOICE_CLONE] OpenVoice non disponible - clonage vocal d√©sactiv√©")

try:
    import numpy as np
    from pydub import AudioSegment
    import soundfile as sf
    AUDIO_PROCESSING_AVAILABLE = True
    logger.info("‚úÖ [VOICE_CLONE] Audio processing disponible")
except ImportError:
    logger.warning("‚ö†Ô∏è [VOICE_CLONE] numpy/pydub/soundfile non disponibles")
    import numpy as np  # numpy should be available


@dataclass
class VoiceFingerprint:
    """
    Empreinte vocale unique pour identification et v√©rification.
    G√©n√®re une signature cryptographique bas√©e sur les caract√©ristiques vocales.
    """
    # Identifiants
    fingerprint_id: str = ""              # ID unique de l'empreinte (hash court)
    version: str = "1.0"                  # Version de l'algorithme de fingerprint

    # Signature principale (SHA-256)
    signature: str = ""                   # Hash SHA-256 des caract√©ristiques
    signature_short: str = ""             # Hash court (12 caract√®res) pour affichage

    # Composants du fingerprint (pour matching partiel)
    pitch_hash: str = ""                  # Hash du profil de pitch
    spectral_hash: str = ""               # Hash des caract√©ristiques spectrales
    prosody_hash: str = ""                # Hash de la prosodie

    # Vecteur d'embedding pour similarit√©
    embedding_vector: List[float] = field(default_factory=list)  # Vecteur normalis√©

    # M√©tadonn√©es
    created_at: datetime = field(default_factory=datetime.now)
    audio_duration_ms: int = 0
    sample_count: int = 1                 # Nombre d'√©chantillons utilis√©s

    # Checksum de v√©rification
    checksum: str = ""                    # CRC32 pour v√©rification d'int√©grit√©

    @staticmethod
    def generate_from_characteristics(
        chars: 'VoiceCharacteristics',
        audio_duration_ms: int = 0,
        embedding: np.ndarray = None
    ) -> 'VoiceFingerprint':
        """
        G√©n√®re une empreinte vocale √† partir des caract√©ristiques.

        L'algorithme combine:
        1. Pitch profile (F0 stats)
        2. Spectral features (brightness, warmth)
        3. Prosodic features (energy, silence)
        4. Optional: embedding vector from voice model
        """
        fp = VoiceFingerprint()
        fp.audio_duration_ms = audio_duration_ms
        fp.created_at = datetime.now()

        # 1. Cr√©er le hash du pitch
        pitch_data = struct.pack(
            'ffff',
            chars.pitch_mean_hz,
            chars.pitch_std_hz,
            chars.pitch_min_hz,
            chars.pitch_max_hz
        )
        fp.pitch_hash = hashlib.sha256(pitch_data).hexdigest()[:16]

        # 2. Cr√©er le hash spectral
        spectral_data = struct.pack(
            'ffff',
            chars.brightness,
            chars.warmth,
            chars.breathiness,
            chars.nasality
        )
        fp.spectral_hash = hashlib.sha256(spectral_data).hexdigest()[:16]

        # 3. Cr√©er le hash prosodique
        prosody_data = struct.pack(
            'ffff',
            chars.energy_mean,
            chars.energy_std,
            chars.silence_ratio,
            chars.speech_rate_wpm
        )
        fp.prosody_hash = hashlib.sha256(prosody_data).hexdigest()[:16]

        # 4. Signature principale (combinaison de tous les composants)
        combined = f"{fp.pitch_hash}{fp.spectral_hash}{fp.prosody_hash}"
        combined += f"{chars.voice_type}{chars.estimated_gender}"
        fp.signature = hashlib.sha256(combined.encode()).hexdigest()
        fp.signature_short = fp.signature[:12]
        fp.fingerprint_id = f"vfp_{fp.signature_short}"

        # 5. Cr√©er le vecteur d'embedding normalis√©
        fp.embedding_vector = [
            chars.pitch_mean_hz / 500.0,      # Normalize pitch (0-500 Hz -> 0-1)
            chars.pitch_std_hz / 100.0,       # Normalize std
            chars.brightness / 5000.0,         # Normalize brightness
            chars.warmth / 10.0,               # Normalize warmth
            chars.energy_mean * 100,           # Scale energy
            chars.silence_ratio,               # Already 0-1
            1.0 if chars.estimated_gender == "male" else 0.0,
            1.0 if chars.estimated_gender == "female" else 0.0,
            1.0 if chars.estimated_gender == "child" else 0.0,
        ]

        # Ajouter l'embedding du mod√®le si disponible
        if embedding is not None and len(embedding) > 0:
            # Prendre les 16 premi√®res valeurs de l'embedding
            embed_normalized = embedding.flatten()[:16]
            embed_normalized = embed_normalized / (np.linalg.norm(embed_normalized) + 1e-10)
            fp.embedding_vector.extend(embed_normalized.tolist())

        # 6. Calculer le checksum CRC32
        checksum_data = f"{fp.signature}{fp.pitch_hash}{fp.spectral_hash}{fp.prosody_hash}"
        fp.checksum = format(
            abs(hash(checksum_data)) % (2**32),
            '08x'
        )

        return fp

    def verify_checksum(self) -> bool:
        """V√©rifie l'int√©grit√© de l'empreinte"""
        expected_data = f"{self.signature}{self.pitch_hash}{self.spectral_hash}{self.prosody_hash}"
        expected_checksum = format(
            abs(hash(expected_data)) % (2**32),
            '08x'
        )
        return self.checksum == expected_checksum

    def similarity_score(self, other: 'VoiceFingerprint') -> float:
        """
        Calcule la similarit√© entre deux empreintes vocales.
        Retourne un score entre 0 (diff√©rent) et 1 (identique).
        """
        score = 0.0
        weights_total = 0.0

        # 1. Comparaison des hashes (match exact = 1.0, sinon 0)
        if self.pitch_hash == other.pitch_hash:
            score += 0.3
        weights_total += 0.3

        if self.spectral_hash == other.spectral_hash:
            score += 0.2
        weights_total += 0.2

        if self.prosody_hash == other.prosody_hash:
            score += 0.1
        weights_total += 0.1

        # 2. Comparaison des vecteurs d'embedding (cosine similarity)
        if self.embedding_vector and other.embedding_vector:
            min_len = min(len(self.embedding_vector), len(other.embedding_vector))
            v1 = np.array(self.embedding_vector[:min_len])
            v2 = np.array(other.embedding_vector[:min_len])

            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 > 0 and norm2 > 0:
                cosine_sim = np.dot(v1, v2) / (norm1 * norm2)
                # Convert from [-1, 1] to [0, 1]
                cosine_sim = (cosine_sim + 1) / 2
                score += cosine_sim * 0.4
            weights_total += 0.4

        return score / weights_total if weights_total > 0 else 0.0

    def matches(self, other: 'VoiceFingerprint', threshold: float = 0.85) -> bool:
        """V√©rifie si deux empreintes correspondent (m√™me personne)"""
        return self.similarity_score(other) >= threshold

    def to_dict(self) -> Dict[str, Any]:
        return {
            "fingerprint_id": self.fingerprint_id,
            "version": self.version,
            "signature": self.signature,
            "signature_short": self.signature_short,
            "components": {
                "pitch_hash": self.pitch_hash,
                "spectral_hash": self.spectral_hash,
                "prosody_hash": self.prosody_hash
            },
            "embedding_vector_length": len(self.embedding_vector),
            "checksum": self.checksum,
            "metadata": {
                "created_at": self.created_at.isoformat(),
                "audio_duration_ms": self.audio_duration_ms,
                "sample_count": self.sample_count
            },
            "integrity_valid": self.verify_checksum()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'VoiceFingerprint':
        """Reconstruit depuis un dictionnaire"""
        fp = cls()
        fp.fingerprint_id = data.get("fingerprint_id", "")
        fp.version = data.get("version", "1.0")
        fp.signature = data.get("signature", "")
        fp.signature_short = data.get("signature_short", "")

        components = data.get("components", {})
        fp.pitch_hash = components.get("pitch_hash", "")
        fp.spectral_hash = components.get("spectral_hash", "")
        fp.prosody_hash = components.get("prosody_hash", "")

        fp.embedding_vector = data.get("embedding_vector", [])
        fp.checksum = data.get("checksum", "")

        metadata = data.get("metadata", {})
        if metadata.get("created_at"):
            fp.created_at = datetime.fromisoformat(metadata["created_at"])
        fp.audio_duration_ms = metadata.get("audio_duration_ms", 0)
        fp.sample_count = metadata.get("sample_count", 1)

        return fp


@dataclass
class VoiceCharacteristics:
    """Caract√©ristiques d√©taill√©es d'une voix extraites de l'audio"""
    # Pitch (fr√©quence fondamentale)
    pitch_mean_hz: float = 0.0          # Pitch moyen en Hz
    pitch_std_hz: float = 0.0           # √âcart-type du pitch
    pitch_min_hz: float = 0.0           # Pitch minimum
    pitch_max_hz: float = 0.0           # Pitch maximum

    # Classification vocale
    voice_type: str = "unknown"          # "high_female", "medium_female", "low_female", "high_male", "medium_male", "low_male", "child"
    estimated_gender: str = "unknown"    # "male", "female", "child", "unknown"
    estimated_age_range: str = "unknown" # "child", "teen", "adult", "senior"

    # Caract√©ristiques spectrales
    brightness: float = 0.0             # Centro√Øde spectral moyen (Hz)
    warmth: float = 0.0                 # √ânergie basses fr√©quences
    breathiness: float = 0.0            # Niveau de souffle dans la voix
    nasality: float = 0.0               # Niveau de nasalit√©

    # Prosodie
    speech_rate_wpm: float = 0.0        # Mots par minute estim√©s
    energy_mean: float = 0.0            # √ânergie moyenne (volume)
    energy_std: float = 0.0             # Variation d'√©nergie
    silence_ratio: float = 0.0          # Ratio de silence

    # Qualit√© technique
    sample_rate: int = 0                # Taux d'√©chantillonnage
    bit_depth: int = 0                  # Profondeur de bits
    channels: int = 1                   # Nombre de canaux
    codec: str = ""                     # Codec audio d√©tect√©

    def to_dict(self) -> Dict[str, Any]:
        return {
            "pitch": {
                "mean_hz": round(self.pitch_mean_hz, 2),
                "std_hz": round(self.pitch_std_hz, 2),
                "min_hz": round(self.pitch_min_hz, 2),
                "max_hz": round(self.pitch_max_hz, 2)
            },
            "classification": {
                "voice_type": self.voice_type,
                "estimated_gender": self.estimated_gender,
                "estimated_age_range": self.estimated_age_range
            },
            "spectral": {
                "brightness": round(self.brightness, 2),
                "warmth": round(self.warmth, 2),
                "breathiness": round(self.breathiness, 2),
                "nasality": round(self.nasality, 2)
            },
            "prosody": {
                "speech_rate_wpm": round(self.speech_rate_wpm, 1),
                "energy_mean": round(self.energy_mean, 4),
                "energy_std": round(self.energy_std, 4),
                "silence_ratio": round(self.silence_ratio, 3)
            },
            "technical": {
                "sample_rate": self.sample_rate,
                "bit_depth": self.bit_depth,
                "channels": self.channels,
                "codec": self.codec
            }
        }

    def generate_fingerprint(
        self,
        audio_duration_ms: int = 0,
        embedding: np.ndarray = None
    ) -> 'VoiceFingerprint':
        """
        G√©n√®re une empreinte vocale unique √† partir de ces caract√©ristiques.

        Args:
            audio_duration_ms: Dur√©e de l'audio source en millisecondes
            embedding: Vecteur d'embedding optionnel (OpenVoice)

        Returns:
            VoiceFingerprint: Empreinte vocale unique avec signature et checksum
        """
        return VoiceFingerprint.generate_from_characteristics(
            self,
            audio_duration_ms=audio_duration_ms,
            embedding=embedding
        )


@dataclass
class SpeakerInfo:
    """Informations sur un locuteur d√©tect√© dans l'audio"""
    speaker_id: str                     # ID unique du locuteur (speaker_0, speaker_1, etc.)
    is_primary: bool = False            # True si c'est le locuteur principal
    speaking_time_ms: int = 0           # Temps de parole en ms
    speaking_ratio: float = 0.0         # Ratio du temps de parole total
    voice_characteristics: VoiceCharacteristics = field(default_factory=VoiceCharacteristics)

    # Segments temporels o√π ce locuteur parle
    segments: List[Dict[str, float]] = field(default_factory=list)  # [{"start": 0.0, "end": 2.5}, ...]

    # Empreinte vocale unique du locuteur
    fingerprint: Optional[VoiceFingerprint] = None

    def generate_fingerprint(self, embedding: np.ndarray = None) -> VoiceFingerprint:
        """
        G√©n√®re l'empreinte vocale unique de ce locuteur.

        Args:
            embedding: Vecteur d'embedding optionnel (OpenVoice)

        Returns:
            VoiceFingerprint: Empreinte vocale unique
        """
        self.fingerprint = self.voice_characteristics.generate_fingerprint(
            audio_duration_ms=self.speaking_time_ms,
            embedding=embedding
        )
        return self.fingerprint

    def to_dict(self) -> Dict[str, Any]:
        result = {
            "speaker_id": self.speaker_id,
            "is_primary": self.is_primary,
            "speaking_time_ms": self.speaking_time_ms,
            "speaking_ratio": round(self.speaking_ratio, 3),
            "voice_characteristics": self.voice_characteristics.to_dict(),
            "segments_count": len(self.segments),
            "segments": self.segments[:10]  # Limiter √† 10 segments pour la s√©rialisation
        }
        if self.fingerprint:
            result["fingerprint"] = self.fingerprint.to_dict()
        return result


@dataclass
class RecordingMetadata:
    """M√©tadonn√©es compl√®tes d'un enregistrement audio"""
    # Informations de base
    file_path: str
    duration_ms: int = 0
    file_size_bytes: int = 0
    format: str = ""                    # "wav", "mp3", "ogg", etc.

    # Qualit√© audio
    noise_level: float = 0.0            # 0 = pas de bruit, 1 = tr√®s bruit√©
    snr_db: float = 0.0                 # Signal-to-noise ratio en dB
    clarity_score: float = 1.0          # 0 = pas clair, 1 = tr√®s clair
    clipping_detected: bool = False     # True si saturation d√©tect√©e
    background_music: bool = False      # True si musique de fond d√©tect√©e

    # Environnement
    reverb_level: float = 0.0           # Niveau de r√©verb√©ration (0-1)
    room_size_estimate: str = "unknown" # "small", "medium", "large", "outdoor"

    # Multi-locuteurs
    speaker_count: int = 1              # Nombre de locuteurs d√©tect√©s
    speakers: List[SpeakerInfo] = field(default_factory=list)
    primary_speaker: Optional[SpeakerInfo] = None

    # Timestamps
    created_at: datetime = field(default_factory=datetime.now)
    analyzed_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "file": {
                "path": self.file_path,
                "duration_ms": self.duration_ms,
                "size_bytes": self.file_size_bytes,
                "format": self.format
            },
            "quality": {
                "noise_level": round(self.noise_level, 3),
                "snr_db": round(self.snr_db, 2),
                "clarity_score": round(self.clarity_score, 3),
                "clipping_detected": self.clipping_detected,
                "background_music": self.background_music
            },
            "environment": {
                "reverb_level": round(self.reverb_level, 3),
                "room_size_estimate": self.room_size_estimate
            },
            "speakers": {
                "count": self.speaker_count,
                "speakers": [s.to_dict() for s in self.speakers],
                "primary_speaker_id": self.primary_speaker.speaker_id if self.primary_speaker else None
            },
            "timestamps": {
                "created_at": self.created_at.isoformat(),
                "analyzed_at": self.analyzed_at.isoformat()
            }
        }


@dataclass
class AudioQualityMetadata:
    """M√©tadonn√©es de qualit√© audio pour s√©lection du meilleur audio"""
    attachment_id: str
    file_path: str
    duration_ms: int = 0
    noise_level: float = 0.0            # 0 = pas de bruit, 1 = tr√®s bruit√©
    clarity_score: float = 1.0          # 0 = pas clair, 1 = tr√®s clair
    has_other_speakers: bool = False    # True si d'autres voix d√©tect√©es
    speaker_count: int = 1              # Nombre de locuteurs
    created_at: datetime = field(default_factory=datetime.now)

    # M√©tadonn√©es √©tendues
    recording_metadata: Optional[RecordingMetadata] = None
    primary_voice: Optional[VoiceCharacteristics] = None

    # Score global calcul√© pour tri
    overall_score: float = 0.0

    def calculate_overall_score(self) -> float:
        """
        Calcule un score global pour la s√©lection du meilleur audio.
        Crit√®res (par ordre de priorit√©):
        1. Pas d'autres locuteurs (p√©nalit√© forte)
        2. Clart√© √©lev√©e
        3. Bruit faible
        4. Dur√©e longue (normalis√©e)
        5. Date r√©cente (bonus l√©ger)
        """
        score = 0.0

        # P√©nalit√© forte si autres locuteurs d√©tect√©s
        if self.has_other_speakers or self.speaker_count > 1:
            score -= 0.3 * (self.speaker_count - 1)

        # Score de clart√© (0-0.3)
        score += self.clarity_score * 0.3

        # Score de bruit invers√© (0-0.2)
        score += (1.0 - self.noise_level) * 0.2

        # Score de dur√©e (normalis√©, max 60s = 0.3)
        duration_score = min(self.duration_ms / 60000, 1.0) * 0.3
        score += duration_score

        # Bonus r√©cence (max 0.1 pour les audios de moins de 7 jours)
        age_days = (datetime.now() - self.created_at).days
        recency_bonus = max(0, 0.1 - (age_days * 0.01))
        score += recency_bonus

        self.overall_score = score
        return score

    def to_dict(self) -> Dict[str, Any]:
        result = {
            "attachment_id": self.attachment_id,
            "file_path": self.file_path,
            "duration_ms": self.duration_ms,
            "noise_level": self.noise_level,
            "clarity_score": self.clarity_score,
            "has_other_speakers": self.has_other_speakers,
            "speaker_count": self.speaker_count,
            "created_at": self.created_at.isoformat(),
            "overall_score": self.overall_score
        }
        if self.recording_metadata:
            result["recording_metadata"] = self.recording_metadata.to_dict()
        if self.primary_voice:
            result["primary_voice"] = self.primary_voice.to_dict()
        return result


@dataclass
class VoiceModel:
    """Mod√®le de voix clon√© d'un utilisateur"""
    user_id: str
    embedding_path: str  # Chemin vers le fichier d'embedding
    audio_count: int
    total_duration_ms: int
    quality_score: float  # 0-1
    profile_id: str = ""  # ID unique du profil vocal
    version: int = 1
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    next_recalibration_at: Optional[datetime] = None
    # Audio utilis√© pour la derni√®re g√©n√©ration
    source_audio_id: str = ""  # attachment_id de l'audio source

    # Profil vocal d√©taill√©
    voice_characteristics: Optional[VoiceCharacteristics] = None
    fingerprint: Optional[VoiceFingerprint] = None

    # Runtime only (not persisted)
    embedding: Optional[np.ndarray] = field(default=None, repr=False)

    def generate_fingerprint(self) -> Optional[VoiceFingerprint]:
        """
        G√©n√®re l'empreinte vocale unique de ce mod√®le.
        Utilise les caract√©ristiques vocales et l'embedding si disponibles.

        Returns:
            VoiceFingerprint: Empreinte vocale unique, ou None si pas de donn√©es
        """
        if self.voice_characteristics:
            self.fingerprint = self.voice_characteristics.generate_fingerprint(
                audio_duration_ms=self.total_duration_ms,
                embedding=self.embedding
            )
            return self.fingerprint
        elif self.embedding is not None:
            # G√©n√©rer une empreinte basique depuis l'embedding seul
            fp = VoiceFingerprint()
            fp.audio_duration_ms = self.total_duration_ms
            fp.created_at = datetime.now()

            # Hash de l'embedding
            embed_data = self.embedding.tobytes()
            fp.signature = hashlib.sha256(embed_data).hexdigest()
            fp.signature_short = fp.signature[:12]
            fp.fingerprint_id = f"vfp_{fp.signature_short}"

            # Vecteur normalis√©
            embed_normalized = self.embedding.flatten()[:25]
            embed_normalized = embed_normalized / (np.linalg.norm(embed_normalized) + 1e-10)
            fp.embedding_vector = embed_normalized.tolist()

            # Checksum
            fp.checksum = format(abs(hash(fp.signature)) % (2**32), '08x')

            self.fingerprint = fp
            return self.fingerprint
        return None

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire (pour JSON)"""
        result = {
            "user_id": self.user_id,
            "profile_id": self.profile_id,
            "embedding_path": self.embedding_path,
            "audio_count": self.audio_count,
            "total_duration_ms": self.total_duration_ms,
            "quality_score": self.quality_score,
            "version": self.version,
            "source_audio_id": self.source_audio_id,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "next_recalibration_at": self.next_recalibration_at.isoformat() if self.next_recalibration_at else None
        }
        if self.voice_characteristics:
            result["voice_characteristics"] = self.voice_characteristics.to_dict()
        if self.fingerprint:
            result["fingerprint"] = self.fingerprint.to_dict()
        return result

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'VoiceModel':
        """Cr√©e depuis un dictionnaire"""
        model = cls(
            user_id=data["user_id"],
            embedding_path=data["embedding_path"],
            audio_count=data["audio_count"],
            total_duration_ms=data["total_duration_ms"],
            quality_score=data["quality_score"],
            profile_id=data.get("profile_id", ""),
            version=data.get("version", 1),
            source_audio_id=data.get("source_audio_id", ""),
            created_at=datetime.fromisoformat(data["created_at"]),
            updated_at=datetime.fromisoformat(data["updated_at"]),
            next_recalibration_at=datetime.fromisoformat(data["next_recalibration_at"]) if data.get("next_recalibration_at") else None
        )

        # Restaurer les caract√©ristiques vocales si pr√©sentes
        if data.get("voice_characteristics"):
            vc_data = data["voice_characteristics"]
            model.voice_characteristics = VoiceCharacteristics(
                pitch_mean_hz=vc_data.get("pitch", {}).get("mean_hz", 0),
                pitch_std_hz=vc_data.get("pitch", {}).get("std_hz", 0),
                pitch_min_hz=vc_data.get("pitch", {}).get("min_hz", 0),
                pitch_max_hz=vc_data.get("pitch", {}).get("max_hz", 0),
                voice_type=vc_data.get("classification", {}).get("voice_type", "unknown"),
                estimated_gender=vc_data.get("classification", {}).get("estimated_gender", "unknown"),
                estimated_age_range=vc_data.get("classification", {}).get("estimated_age_range", "unknown"),
                brightness=vc_data.get("spectral", {}).get("brightness", 0),
                warmth=vc_data.get("spectral", {}).get("warmth", 0),
                breathiness=vc_data.get("spectral", {}).get("breathiness", 0),
                nasality=vc_data.get("spectral", {}).get("nasality", 0),
                speech_rate_wpm=vc_data.get("prosody", {}).get("speech_rate_wpm", 0),
                energy_mean=vc_data.get("prosody", {}).get("energy_mean", 0),
                energy_std=vc_data.get("prosody", {}).get("energy_std", 0),
                silence_ratio=vc_data.get("prosody", {}).get("silence_ratio", 0),
                sample_rate=vc_data.get("technical", {}).get("sample_rate", 0),
                bit_depth=vc_data.get("technical", {}).get("bit_depth", 0),
                channels=vc_data.get("technical", {}).get("channels", 1),
                codec=vc_data.get("technical", {}).get("codec", "")
            )

        # Restaurer l'empreinte vocale si pr√©sente
        if data.get("fingerprint"):
            model.fingerprint = VoiceFingerprint.from_dict(data["fingerprint"])

        return model


class VoiceAnalyzer:
    """
    Analyseur de voix pour extraction de caract√©ristiques d√©taill√©es.
    Utilise librosa pour l'analyse audio et pyannote (optionnel) pour la diarisation.
    """

    # Seuils de classification vocale par pitch
    PITCH_THRESHOLDS = {
        "child": (250, 500),      # Enfant: 250-500 Hz
        "high_female": (200, 350),
        "medium_female": (165, 250),
        "low_female": (140, 200),
        "high_male": (130, 180),
        "medium_male": (100, 150),
        "low_male": (65, 120),
    }

    def __init__(self):
        self._pyannote_available = False
        self._librosa_available = False

        try:
            import librosa
            self._librosa_available = True
        except ImportError:
            logger.warning("[VOICE_ANALYZER] librosa non disponible")

        try:
            from pyannote.audio import Pipeline
            self._pyannote_available = True
        except ImportError:
            logger.debug("[VOICE_ANALYZER] pyannote non disponible - diarisation d√©sactiv√©e")

    async def analyze_audio(self, audio_path: str) -> RecordingMetadata:
        """
        Analyse compl√®te d'un fichier audio.
        Extrait les caract√©ristiques vocales, d√©tecte les locuteurs, √©value la qualit√©.
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio non trouv√©: {audio_path}")

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._analyze_audio_sync, audio_path)

    def _analyze_audio_sync(self, audio_path: str) -> RecordingMetadata:
        """Analyse synchrone de l'audio (ex√©cut√©e dans un thread)"""
        import librosa

        # Charger l'audio
        audio, sr = librosa.load(audio_path, sr=22050)
        duration_ms = int(len(audio) / sr * 1000)

        # Informations fichier
        file_size = os.path.getsize(audio_path)
        file_format = os.path.splitext(audio_path)[1].lstrip('.')

        # Cr√©er les m√©tadonn√©es de base
        metadata = RecordingMetadata(
            file_path=audio_path,
            duration_ms=duration_ms,
            file_size_bytes=file_size,
            format=file_format,
            analyzed_at=datetime.now()
        )

        # Analyse de qualit√©
        metadata.noise_level = self._estimate_noise_level(audio, sr)
        metadata.snr_db = self._estimate_snr(audio, sr)
        metadata.clarity_score = self._calculate_clarity(audio, sr)
        metadata.clipping_detected = self._detect_clipping(audio)
        metadata.reverb_level = self._estimate_reverb(audio, sr)
        metadata.room_size_estimate = self._estimate_room_size(metadata.reverb_level)

        # Analyse des locuteurs (simplifi√©e sans pyannote)
        speakers = self._analyze_speakers(audio, sr)
        metadata.speaker_count = len(speakers)
        metadata.speakers = speakers
        metadata.primary_speaker = self._identify_primary_speaker(speakers)

        if metadata.speaker_count > 1:
            logger.info(f"[VOICE_ANALYZER] {metadata.speaker_count} locuteurs d√©tect√©s")

        return metadata

    def _estimate_noise_level(self, audio: np.ndarray, sr: int) -> float:
        """Estime le niveau de bruit (0 = propre, 1 = tr√®s bruit√©)"""
        try:
            import librosa

            # Calculer le spectre
            stft = np.abs(librosa.stft(audio))

            # Estimer le plancher de bruit (percentile bas)
            noise_floor = np.percentile(stft, 10)
            signal_level = np.percentile(stft, 90)

            if signal_level == 0:
                return 0.5

            # Ratio bruit/signal normalis√©
            noise_ratio = noise_floor / (signal_level + 1e-10)
            return min(1.0, noise_ratio * 2)

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur estimation bruit: {e}")
            return 0.0

    def _estimate_snr(self, audio: np.ndarray, sr: int) -> float:
        """Estime le rapport signal/bruit en dB"""
        try:
            # √ânergie du signal
            signal_power = np.mean(audio ** 2)

            # Estimer le bruit (segments silencieux)
            frame_length = int(sr * 0.02)  # 20ms frames
            hop_length = int(sr * 0.01)    # 10ms hop

            frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)
            frame_energy = np.sum(frames ** 2, axis=0)

            # Bruit = √©nergie des 10% frames les plus silencieux
            noise_power = np.percentile(frame_energy, 10) / frame_length

            if noise_power == 0:
                return 40.0  # Tr√®s propre

            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            return max(-10, min(60, snr))

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur estimation SNR: {e}")
            return 20.0

    def _calculate_clarity(self, audio: np.ndarray, sr: int) -> float:
        """Calcule un score de clart√© vocale (0-1)"""
        try:
            import librosa

            # Centro√Øde spectral (indicateur de clart√©)
            centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
            mean_centroid = np.mean(centroid)

            # Clart√© optimale entre 1000-3000 Hz pour la voix
            if 1000 <= mean_centroid <= 3000:
                clarity = 1.0
            elif mean_centroid < 1000:
                clarity = mean_centroid / 1000
            else:
                clarity = max(0, 1 - (mean_centroid - 3000) / 3000)

            # Ajuster par la variance spectrale (trop de variance = moins clair)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
            bandwidth_penalty = min(0.3, np.std(spectral_bandwidth) / 1000)

            return max(0, min(1, clarity - bandwidth_penalty))

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur calcul clart√©: {e}")
            return 0.5

    def _detect_clipping(self, audio: np.ndarray) -> bool:
        """D√©tecte la saturation audio"""
        threshold = 0.99
        clipped_samples = np.sum(np.abs(audio) >= threshold)
        clip_ratio = clipped_samples / len(audio)
        return clip_ratio > 0.001  # Plus de 0.1% de samples satur√©s

    def _estimate_reverb(self, audio: np.ndarray, sr: int) -> float:
        """Estime le niveau de r√©verb√©ration (0-1)"""
        try:
            import librosa

            # Utiliser la d√©croissance de l'enveloppe
            envelope = np.abs(librosa.onset.onset_strength(y=audio, sr=sr))

            # Calculer le temps de d√©croissance
            if len(envelope) < 10:
                return 0.0

            # Autocorr√©lation pour d√©tecter les √©chos
            autocorr = np.correlate(envelope, envelope, mode='full')
            autocorr = autocorr[len(autocorr)//2:]

            # Normaliser
            if autocorr[0] != 0:
                autocorr = autocorr / autocorr[0]

            # Chercher les pics secondaires (√©chos)
            peaks = []
            for i in range(1, min(len(autocorr), 50)):
                if autocorr[i] > 0.3:
                    peaks.append(autocorr[i])

            if not peaks:
                return 0.0

            return min(1.0, np.mean(peaks))

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur estimation reverb: {e}")
            return 0.0

    def _estimate_room_size(self, reverb_level: float) -> str:
        """Estime la taille de la pi√®ce bas√©e sur la r√©verb√©ration"""
        if reverb_level < 0.1:
            return "small"  # Proche du micro, peu de r√©verb
        elif reverb_level < 0.3:
            return "medium"
        elif reverb_level < 0.6:
            return "large"
        else:
            return "outdoor"  # Beaucoup de r√©verb/√©cho

    def _analyze_speakers(self, audio: np.ndarray, sr: int) -> List[SpeakerInfo]:
        """
        Analyse les locuteurs dans l'audio.
        Utilise pyannote si disponible, sinon une analyse simplifi√©e.
        """
        if self._pyannote_available:
            return self._analyze_speakers_pyannote(audio, sr)
        else:
            return self._analyze_speakers_simple(audio, sr)

    def _analyze_speakers_simple(self, audio: np.ndarray, sr: int) -> List[SpeakerInfo]:
        """
        Analyse simplifi√©e des locuteurs sans diarisation.
        Suppose un seul locuteur principal et extrait ses caract√©ristiques.
        """
        try:
            import librosa

            # Extraire les caract√©ristiques vocales
            voice_chars = self._extract_voice_characteristics(audio, sr)

            # Calculer le temps de parole (segments non-silencieux)
            rms = librosa.feature.rms(y=audio)[0]
            speech_frames = np.sum(rms > np.percentile(rms, 20))
            total_frames = len(rms)
            speaking_ratio = speech_frames / total_frames if total_frames > 0 else 1.0
            speaking_time_ms = int(len(audio) / sr * 1000 * speaking_ratio)

            # Cr√©er le locuteur principal
            primary_speaker = SpeakerInfo(
                speaker_id="speaker_0",
                is_primary=True,
                speaking_time_ms=speaking_time_ms,
                speaking_ratio=speaking_ratio,
                voice_characteristics=voice_chars,
                segments=[{"start": 0.0, "end": len(audio) / sr}]
            )

            return [primary_speaker]

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur analyse locuteurs: {e}")
            return [SpeakerInfo(speaker_id="speaker_0", is_primary=True)]

    def _analyze_speakers_pyannote(self, audio: np.ndarray, sr: int) -> List[SpeakerInfo]:
        """Analyse des locuteurs avec pyannote (diarisation)"""
        # TODO: Impl√©menter avec pyannote.audio quand disponible
        # Pour l'instant, fallback sur l'analyse simple
        return self._analyze_speakers_simple(audio, sr)

    def _extract_voice_characteristics(self, audio: np.ndarray, sr: int) -> VoiceCharacteristics:
        """Extrait les caract√©ristiques d√©taill√©es d'une voix"""
        try:
            import librosa

            chars = VoiceCharacteristics()
            chars.sample_rate = sr

            # Analyse du pitch (F0)
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio,
                fmin=librosa.note_to_hz('C2'),  # ~65 Hz
                fmax=librosa.note_to_hz('C7'),  # ~2093 Hz
                sr=sr
            )

            # Filtrer les valeurs NaN
            f0_valid = f0[~np.isnan(f0)]

            if len(f0_valid) > 0:
                chars.pitch_mean_hz = float(np.mean(f0_valid))
                chars.pitch_std_hz = float(np.std(f0_valid))
                chars.pitch_min_hz = float(np.min(f0_valid))
                chars.pitch_max_hz = float(np.max(f0_valid))

                # Classification vocale
                chars.voice_type, chars.estimated_gender = self._classify_voice(chars.pitch_mean_hz)
                chars.estimated_age_range = self._estimate_age(chars.pitch_mean_hz, chars.pitch_std_hz)

            # Caract√©ristiques spectrales
            # Brightness (centro√Øde spectral)
            centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
            chars.brightness = float(np.mean(centroid))

            # Warmth (√©nergie basses fr√©quences)
            spec = np.abs(librosa.stft(audio))
            low_freq_bins = int(500 / (sr / 2) * spec.shape[0])  # Jusqu'√† 500 Hz
            chars.warmth = float(np.mean(spec[:low_freq_bins, :]))

            # √ânergie
            rms = librosa.feature.rms(y=audio)[0]
            chars.energy_mean = float(np.mean(rms))
            chars.energy_std = float(np.std(rms))

            # Ratio de silence
            silence_threshold = np.percentile(rms, 10)
            chars.silence_ratio = float(np.sum(rms < silence_threshold) / len(rms))

            return chars

        except Exception as e:
            logger.warning(f"[VOICE_ANALYZER] Erreur extraction caract√©ristiques: {e}")
            return VoiceCharacteristics()

    def _classify_voice(self, pitch_hz: float) -> tuple:
        """Classifie le type de voix bas√© sur le pitch"""
        if pitch_hz >= 250:
            return ("child", "child")
        elif pitch_hz >= 200:
            return ("high_female", "female")
        elif pitch_hz >= 165:
            return ("medium_female", "female")
        elif pitch_hz >= 140:
            return ("low_female", "female")
        elif pitch_hz >= 130:
            return ("high_male", "male")
        elif pitch_hz >= 100:
            return ("medium_male", "male")
        else:
            return ("low_male", "male")

    def _estimate_age(self, pitch_hz: float, pitch_std: float) -> str:
        """Estime la tranche d'√¢ge bas√©e sur le pitch"""
        if pitch_hz >= 250:
            return "child"
        elif pitch_hz >= 200 and pitch_std > 30:
            return "teen"
        elif pitch_hz < 90:
            return "senior"
        else:
            return "adult"

    def _identify_primary_speaker(self, speakers: List[SpeakerInfo]) -> Optional[SpeakerInfo]:
        """Identifie le locuteur principal (celui qui parle le plus)"""
        if not speakers:
            return None

        # Trier par temps de parole d√©croissant
        sorted_speakers = sorted(speakers, key=lambda s: s.speaking_time_ms, reverse=True)

        # Marquer le premier comme principal
        primary = sorted_speakers[0]
        primary.is_primary = True

        return primary

    async def extract_primary_speaker_audio(
        self,
        audio_path: str,
        output_path: Optional[str] = None,
        min_segment_duration_ms: int = 100
    ) -> Tuple[str, RecordingMetadata]:
        """
        Extrait uniquement l'audio du locuteur principal.

        Analyse l'audio, identifie le locuteur principal (celui qui parle le plus),
        et extrait uniquement ses segments de parole pour le clonage vocal.

        Args:
            audio_path: Chemin vers le fichier audio source
            output_path: Chemin de sortie (optionnel, g√©n√©r√© automatiquement si None)
            min_segment_duration_ms: Dur√©e minimum d'un segment √† conserver

        Returns:
            Tuple[str, RecordingMetadata]: (chemin audio extrait, m√©tadonn√©es compl√®tes)
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio non trouv√©: {audio_path}")

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._extract_primary_speaker_sync,
            audio_path,
            output_path,
            min_segment_duration_ms
        )

    def _extract_primary_speaker_sync(
        self,
        audio_path: str,
        output_path: Optional[str],
        min_segment_duration_ms: int
    ) -> Tuple[str, RecordingMetadata]:
        """Extraction synchrone du locuteur principal"""
        import librosa
        import soundfile as sf

        # Analyser l'audio complet
        metadata = self._analyze_audio_sync(audio_path)

        if not metadata.primary_speaker:
            logger.warning("[VOICE_ANALYZER] Aucun locuteur principal d√©tect√©")
            return audio_path, metadata

        primary = metadata.primary_speaker
        segments = primary.segments

        if not segments:
            logger.warning("[VOICE_ANALYZER] Aucun segment pour le locuteur principal")
            return audio_path, metadata

        # Charger l'audio
        audio, sr = librosa.load(audio_path, sr=22050)

        # Filtrer les segments trop courts
        min_samples = int(min_segment_duration_ms * sr / 1000)
        valid_segments = []
        for seg in segments:
            start_sample = int(seg["start"] * sr)
            end_sample = int(seg["end"] * sr)
            if (end_sample - start_sample) >= min_samples:
                valid_segments.append((start_sample, end_sample))

        if not valid_segments:
            logger.warning("[VOICE_ANALYZER] Aucun segment valide apr√®s filtrage")
            return audio_path, metadata

        # Extraire et concat√©ner les segments du locuteur principal
        extracted_audio = []
        for start, end in valid_segments:
            extracted_audio.append(audio[start:end])

        primary_audio = np.concatenate(extracted_audio)

        # G√©n√©rer le chemin de sortie si non fourni
        if output_path is None:
            base_name = os.path.splitext(audio_path)[0]
            output_path = f"{base_name}_primary_speaker.wav"

        # Sauvegarder l'audio extrait
        sf.write(output_path, primary_audio, sr)

        # Mettre √† jour les m√©tadonn√©es
        extracted_duration_ms = int(len(primary_audio) / sr * 1000)
        logger.info(
            f"[VOICE_ANALYZER] Audio du locuteur principal extrait: "
            f"{len(valid_segments)} segments, {extracted_duration_ms}ms "
            f"(original: {metadata.duration_ms}ms)"
        )

        # Cr√©er de nouvelles m√©tadonn√©es pour l'audio extrait
        extracted_metadata = RecordingMetadata(
            file_path=output_path,
            duration_ms=extracted_duration_ms,
            file_size_bytes=os.path.getsize(output_path),
            format="wav",
            noise_level=metadata.noise_level,
            snr_db=metadata.snr_db,
            clarity_score=metadata.clarity_score,
            clipping_detected=metadata.clipping_detected,
            speaker_count=1,  # Maintenant un seul locuteur
            speakers=[primary],
            primary_speaker=primary,
            analyzed_at=datetime.now()
        )

        return output_path, extracted_metadata

    async def compare_voices(
        self,
        original_path: str,
        cloned_path: str
    ) -> Dict[str, Any]:
        """Compare une voix originale et clon√©e pour mesurer la similarit√©"""
        original_meta = await self.analyze_audio(original_path)
        cloned_meta = await self.analyze_audio(cloned_path)

        if not original_meta.primary_speaker or not cloned_meta.primary_speaker:
            return {"similarity": 0.0, "error": "Locuteur principal non d√©tect√©"}

        orig_voice = original_meta.primary_speaker.voice_characteristics
        clone_voice = cloned_meta.primary_speaker.voice_characteristics

        # Similarit√© du pitch
        if orig_voice.pitch_mean_hz > 0 and clone_voice.pitch_mean_hz > 0:
            pitch_diff = abs(orig_voice.pitch_mean_hz - clone_voice.pitch_mean_hz)
            pitch_sim = max(0, 1 - pitch_diff / orig_voice.pitch_mean_hz)
        else:
            pitch_sim = 0.0

        # Similarit√© de la brillance
        if orig_voice.brightness > 0:
            bright_diff = abs(orig_voice.brightness - clone_voice.brightness)
            bright_sim = max(0, 1 - bright_diff / orig_voice.brightness)
        else:
            bright_sim = 0.0

        # Similarit√© de l'√©nergie
        if orig_voice.energy_mean > 0:
            energy_diff = abs(orig_voice.energy_mean - clone_voice.energy_mean)
            energy_sim = max(0, 1 - energy_diff / orig_voice.energy_mean)
        else:
            energy_sim = 0.0

        # Score global
        overall = (pitch_sim * 0.4 + bright_sim * 0.3 + energy_sim * 0.3)

        return {
            "pitch_similarity": round(pitch_sim, 3),
            "brightness_similarity": round(bright_sim, 3),
            "energy_similarity": round(energy_sim, 3),
            "overall_similarity": round(overall, 3),
            "original_voice": orig_voice.to_dict(),
            "cloned_voice": clone_voice.to_dict()
        }


# Instance singleton de l'analyseur
_voice_analyzer_instance: Optional[VoiceAnalyzer] = None


def get_voice_analyzer() -> VoiceAnalyzer:
    """Retourne l'instance singleton de l'analyseur vocal"""
    global _voice_analyzer_instance
    if _voice_analyzer_instance is None:
        _voice_analyzer_instance = VoiceAnalyzer()
    return _voice_analyzer_instance


class VoiceCloneService:
    """
    Service de clonage vocal - Singleton

    Fonctionnalit√©s:
    - Cr√©ation de mod√®les de voix √† partir d'audios
    - Cache des mod√®les (90 jours / 3 mois)
    - Agr√©gation d'audios si dur√©e insuffisante
    - Am√©lioration continue des mod√®les
    - Recalibration trimestrielle
    - S√©lection du meilleur audio (le plus long, le plus clair, sans bruit)
    """

    _instance = None
    _lock = threading.Lock()

    # Configuration
    MIN_AUDIO_DURATION_MS = 10_000  # 10 secondes minimum pour clonage de qualit√©
    VOICE_MODEL_MAX_AGE_DAYS = 90   # Recalibration trimestrielle (3 mois)
    MAX_AUDIO_HISTORY = 20          # Nombre max d'audios √† agr√©ger
    IMPROVEMENT_WEIGHT_OLD = 0.7    # Poids de l'ancien embedding
    IMPROVEMENT_WEIGHT_NEW = 0.3    # Poids du nouveau

    def __new__(cls, *args, **kwargs):
        """Singleton pattern"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(
        self,
        voice_cache_dir: Optional[str] = None,
        device: str = "cpu",
        database_service = None
    ):
        if self._initialized:
            return

        # Configuration
        self.voice_cache_dir = Path(voice_cache_dir or os.getenv('VOICE_MODEL_CACHE_DIR', '/app/voice_models'))
        self.device = os.getenv('VOICE_CLONE_DEVICE', device)
        self.database_service = database_service

        # OpenVoice components
        self.tone_color_converter = None
        self.se_extractor_module = None

        # √âtat
        self.is_initialized = False
        self._init_lock = asyncio.Lock()

        # Cr√©er le r√©pertoire de cache
        self.voice_cache_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"[VOICE_CLONE] Service cr√©√©: cache_dir={self.voice_cache_dir}, device={self.device}")
        self._initialized = True

    def set_database_service(self, database_service):
        """Injecte le service de base de donn√©es"""
        self.database_service = database_service

    async def initialize(self) -> bool:
        """Initialise OpenVoice pour le clonage vocal"""
        if self.is_initialized:
            return True

        async with self._init_lock:
            if self.is_initialized:
                return True

            if not OPENVOICE_AVAILABLE:
                logger.warning("[VOICE_CLONE] OpenVoice non disponible - mode d√©grad√©")
                self.is_initialized = True
                return True

            try:
                start_time = time.time()
                logger.info("[VOICE_CLONE] üîÑ Initialisation d'OpenVoice...")

                # Charger dans un thread pour ne pas bloquer
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._load_openvoice)

                load_time = time.time() - start_time
                logger.info(f"[VOICE_CLONE] ‚úÖ OpenVoice initialis√© en {load_time:.2f}s")

                self.is_initialized = True
                return True

            except Exception as e:
                logger.error(f"[VOICE_CLONE] ‚ùå Erreur initialisation OpenVoice: {e}")
                import traceback
                traceback.print_exc()
                self.is_initialized = True  # Mode d√©grad√©
                return True

    def _load_openvoice(self):
        """Charge OpenVoice (appel√© dans un thread)"""
        checkpoints_dir = os.getenv('OPENVOICE_CHECKPOINTS', 'checkpoints/converter')
        self.tone_color_converter = ToneColorConverter(
            checkpoints_dir,
            device=self.device
        )
        self.se_extractor_module = se_extractor

    async def get_or_create_voice_model(
        self,
        user_id: str,
        current_audio_path: Optional[str] = None,
        current_audio_duration_ms: int = 0
    ) -> VoiceModel:
        """
        R√©cup√®re ou cr√©e un mod√®le de voix pour un utilisateur.

        Logique:
        1. Si mod√®le en cache et r√©cent ‚Üí utiliser
        2. Si mod√®le en cache mais ancien ‚Üí am√©liorer avec nouvel audio
        3. Si pas de mod√®le et audio trop court ‚Üí agr√©ger historique
        4. Cr√©er nouveau mod√®le

        Args:
            user_id: ID de l'utilisateur
            current_audio_path: Audio actuel pour le clonage (optionnel)
            current_audio_duration_ms: Dur√©e de l'audio actuel

        Returns:
            VoiceModel pr√™t √† l'emploi
        """
        # 1. V√©rifier le cache
        cached_model = await self._load_cached_model(user_id)

        if cached_model:
            age_days = (datetime.now() - cached_model.updated_at).days

            # Mod√®le r√©cent ‚Üí utiliser directement
            if age_days < self.VOICE_MODEL_MAX_AGE_DAYS:
                logger.info(f"[VOICE_CLONE] üì¶ Mod√®le en cache pour {user_id} (age: {age_days}j)")

                # Charger l'embedding si pas en m√©moire
                if cached_model.embedding is None:
                    cached_model = await self._load_embedding(cached_model)

                return cached_model

            # Mod√®le ancien ‚Üí am√©liorer si on a un nouvel audio
            if current_audio_path:
                logger.info(f"[VOICE_CLONE] üîÑ Mod√®le obsol√®te pour {user_id}, am√©lioration...")
                return await self._improve_model(cached_model, current_audio_path)

            # Sinon utiliser l'ancien mod√®le
            logger.info(f"[VOICE_CLONE] ‚ö†Ô∏è Mod√®le obsol√®te pour {user_id} mais pas de nouvel audio")
            if cached_model.embedding is None:
                cached_model = await self._load_embedding(cached_model)
            return cached_model

        # 2. Pas de mod√®le ‚Üí cr√©er
        if not current_audio_path:
            # Essayer de r√©cup√©rer l'historique audio
            audio_paths = await self._get_user_audio_history(user_id)
            if not audio_paths:
                raise ValueError(f"Aucun audio disponible pour cr√©er le mod√®le de voix de {user_id}")
            current_audio_path = audio_paths[0]
            current_audio_duration_ms = await self._get_audio_duration_ms(current_audio_path)

        audio_paths = [current_audio_path]
        total_duration = current_audio_duration_ms

        # Si audio trop court, chercher l'historique
        if total_duration < self.MIN_AUDIO_DURATION_MS:
            logger.info(f"[VOICE_CLONE] ‚ö†Ô∏è Audio trop court ({total_duration}ms), agr√©gation historique...")
            historical_audios = await self._get_user_audio_history(user_id, exclude=[current_audio_path])
            audio_paths.extend(historical_audios)
            total_duration = await self._calculate_total_duration(audio_paths)

            logger.info(f"[VOICE_CLONE] üìö {len(audio_paths)} audios agr√©g√©s, total: {total_duration}ms")

        # Cr√©er le mod√®le avec ce qu'on a
        return await self._create_voice_model(user_id, audio_paths, total_duration)

    async def _create_voice_model(
        self,
        user_id: str,
        audio_paths: List[str],
        total_duration_ms: int
    ) -> VoiceModel:
        """
        Cr√©e un nouveau mod√®le de voix √† partir des audios.

        IMPORTANT: Extrait uniquement les segments du locuteur principal
        pour garantir que le clonage ne concerne que sa voix.
        """
        import uuid as uuid_module
        start_time = time.time()
        logger.info(f"[VOICE_CLONE] üé§ Cr√©ation mod√®le pour {user_id} ({len(audio_paths)} audios)")

        # Filtrer les audios valides
        valid_paths = [p for p in audio_paths if os.path.exists(p)]
        if not valid_paths:
            raise ValueError("Aucun fichier audio valide trouv√©")

        # Cr√©er le dossier utilisateur: {voice_cache_dir}/{user_id}/
        user_dir = self.voice_cache_dir / user_id
        user_dir.mkdir(parents=True, exist_ok=True)

        # =====================================================================
        # EXTRACTION DU LOCUTEUR PRINCIPAL UNIQUEMENT
        # Pour chaque audio, extraire uniquement les segments du locuteur principal
        # =====================================================================
        voice_analyzer = get_voice_analyzer()
        extracted_paths = []
        primary_voice_chars = None
        recording_metadata = None

        for audio_path in valid_paths:
            try:
                # Extraire uniquement les segments du locuteur principal
                extracted_path, metadata = await voice_analyzer.extract_primary_speaker_audio(
                    audio_path,
                    output_path=str(user_dir / f"primary_{os.path.basename(audio_path)}"),
                    min_segment_duration_ms=100
                )
                extracted_paths.append(extracted_path)

                # Conserver les caract√©ristiques vocales du premier locuteur principal
                if primary_voice_chars is None and metadata.primary_speaker:
                    primary_voice_chars = metadata.primary_speaker.voice_characteristics
                    recording_metadata = metadata
                    logger.info(
                        f"[VOICE_CLONE] Locuteur principal d√©tect√©: "
                        f"gender={primary_voice_chars.estimated_gender}, "
                        f"pitch={primary_voice_chars.pitch_mean_hz:.1f}Hz"
                    )

            except Exception as e:
                logger.warning(f"[VOICE_CLONE] Erreur extraction locuteur principal: {e}")
                # Fallback: utiliser l'audio complet
                extracted_paths.append(audio_path)

        # Recalculer la dur√©e totale apr√®s extraction
        extracted_duration_ms = 0
        for path in extracted_paths:
            extracted_duration_ms += await self._get_audio_duration_ms(path)

        logger.info(
            f"[VOICE_CLONE] Audio extrait: {extracted_duration_ms}ms "
            f"(original: {total_duration_ms}ms, {len(extracted_paths)} fichiers)"
        )

        # Concat√©ner les audios extraits si multiples
        if len(extracted_paths) > 1:
            combined_audio = await self._concatenate_audios(extracted_paths, user_id)
        else:
            combined_audio = extracted_paths[0]

        # G√©n√©rer un profile_id unique
        profile_id = uuid_module.uuid4().hex[:12]
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")

        # Extraire l'embedding de voix (du locuteur principal uniquement)
        embedding = await self._extract_voice_embedding(combined_audio, user_dir)

        # Calculer score de qualit√©
        quality_score = self._calculate_quality_score(extracted_duration_ms, len(valid_paths))

        # Chemin de l'embedding avec nouvelle convention: {userId}_{profileId}_{timestamp}.pkl
        embedding_filename = f"{user_id}_{profile_id}_{timestamp}.pkl"
        embedding_path = str(user_dir / embedding_filename)

        # Cr√©er le mod√®le avec les caract√©ristiques vocales du locuteur principal
        model = VoiceModel(
            user_id=user_id,
            embedding_path=embedding_path,
            audio_count=len(valid_paths),
            total_duration_ms=extracted_duration_ms,  # Dur√©e extraite, pas originale
            quality_score=quality_score,
            profile_id=profile_id,
            version=1,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            next_recalibration_at=datetime.now() + timedelta(days=self.VOICE_MODEL_MAX_AGE_DAYS),
            embedding=embedding,
            voice_characteristics=primary_voice_chars  # Caract√©ristiques du locuteur principal
        )

        # G√©n√©rer l'empreinte vocale unique
        if model.voice_characteristics or model.embedding is not None:
            fingerprint = model.generate_fingerprint()
            if fingerprint:
                logger.info(f"[VOICE_CLONE] Empreinte vocale: {fingerprint.fingerprint_id}")

        # Sauvegarder
        await self._save_model_to_cache(model)

        processing_time = int((time.time() - start_time) * 1000)
        logger.info(f"[VOICE_CLONE] ‚úÖ Mod√®le cr√©√© pour {user_id}: quality={quality_score:.2f}, time={processing_time}ms")

        return model

    async def _improve_model(
        self,
        existing_model: VoiceModel,
        new_audio_path: str
    ) -> VoiceModel:
        """Am√©liore un mod√®le existant avec un nouvel audio"""
        logger.info(f"[VOICE_CLONE] üîÑ Am√©lioration mod√®le pour {existing_model.user_id}")

        # Charger l'embedding existant
        if existing_model.embedding is None:
            existing_model = await self._load_embedding(existing_model)

        # Extraire embedding du nouvel audio
        user_dir = self.voice_cache_dir / existing_model.user_id / "temp"
        user_dir.mkdir(parents=True, exist_ok=True)

        new_embedding = await self._extract_voice_embedding(new_audio_path, user_dir)

        if existing_model.embedding is not None and new_embedding is not None:
            # Moyenne pond√©r√©e (plus de poids aux anciens pour stabilit√©)
            improved_embedding = (
                self.IMPROVEMENT_WEIGHT_OLD * existing_model.embedding +
                self.IMPROVEMENT_WEIGHT_NEW * new_embedding
            )
        else:
            improved_embedding = new_embedding if new_embedding is not None else existing_model.embedding

        # Mettre √† jour le mod√®le
        existing_model.embedding = improved_embedding
        existing_model.updated_at = datetime.now()
        existing_model.audio_count += 1
        existing_model.quality_score = min(1.0, existing_model.quality_score + 0.05)
        existing_model.version += 1
        existing_model.next_recalibration_at = datetime.now() + timedelta(days=self.VOICE_MODEL_MAX_AGE_DAYS)

        # Sauvegarder
        await self._save_model_to_cache(existing_model)

        logger.info(f"[VOICE_CLONE] ‚úÖ Mod√®le am√©lior√© pour {existing_model.user_id} (v{existing_model.version})")
        return existing_model

    async def _extract_voice_embedding(self, audio_path: str, target_dir: Path) -> Optional[np.ndarray]:
        """Extrait l'embedding de voix d'un fichier audio"""
        if not OPENVOICE_AVAILABLE or self.se_extractor_module is None:
            logger.warning("[VOICE_CLONE] OpenVoice non disponible, embedding factice")
            return np.zeros(256)  # Embedding factice

        try:
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None,
                lambda: self.se_extractor_module.get_se(
                    audio_path,
                    self.tone_color_converter,
                    target_dir=str(target_dir)
                )
            )
            return embedding
        except Exception as e:
            logger.error(f"[VOICE_CLONE] ‚ùå Erreur extraction embedding: {e}")
            return np.zeros(256)

    async def _concatenate_audios(self, audio_paths: List[str], user_id: str) -> str:
        """Concat√®ne plusieurs fichiers audio en un seul"""
        if not AUDIO_PROCESSING_AVAILABLE:
            return audio_paths[0]  # Retourner le premier si pas de processing

        try:
            combined = AudioSegment.empty()
            for path in audio_paths:
                try:
                    audio = AudioSegment.from_file(path)
                    combined += audio
                except Exception as e:
                    logger.warning(f"[VOICE_CLONE] Impossible de lire {path}: {e}")

            # Sauvegarder le fichier combin√©
            output_path = self.voice_cache_dir / user_id / "combined_audio.wav"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            combined.export(str(output_path), format="wav")

            return str(output_path)

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur concat√©nation: {e}")
            return audio_paths[0]

    async def _get_user_audio_history(
        self,
        user_id: str,
        exclude: Optional[List[str]] = None,
        limit: int = None
    ) -> List[str]:
        """
        R√©cup√®re l'historique des messages audio d'un utilisateur.
        Utilise la base de donn√©es pour trouver les attachements audio.
        """
        limit = limit or self.MAX_AUDIO_HISTORY
        exclude = exclude or []

        if not self.database_service:
            logger.warning("[VOICE_CLONE] Database service non disponible")
            return []

        try:
            # Requ√™te pour r√©cup√©rer les audios de l'utilisateur
            attachments = await self.database_service.prisma.messageattachment.find_many(
                where={
                    "message": {
                        "senderId": user_id
                    },
                    "mimeType": {
                        "startswith": "audio/"
                    }
                },
                order={"createdAt": "desc"},
                take=limit
            )

            # Filtrer les fichiers existants
            audio_paths = []
            for att in attachments:
                if att.filePath and att.filePath not in exclude and os.path.exists(att.filePath):
                    audio_paths.append(att.filePath)

            logger.info(f"[VOICE_CLONE] üìö {len(audio_paths)} audios historiques trouv√©s pour {user_id}")
            return audio_paths

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur r√©cup√©ration historique: {e}")
            return []

    async def _get_best_audio_for_cloning(
        self,
        user_id: str,
        limit: int = 10
    ) -> Optional[AudioQualityMetadata]:
        """
        S√©lectionne le meilleur audio pour le clonage vocal.
        Crit√®res (par ordre de priorit√©):
        1. Le plus long
        2. Le plus clair (sans bruit)
        3. Sans autres locuteurs
        4. Le plus r√©cent

        Returns:
            AudioQualityMetadata du meilleur audio, ou None si aucun audio trouv√©
        """
        if not self.database_service:
            logger.warning("[VOICE_CLONE] Database service non disponible")
            return None

        try:
            # Requ√™te pour r√©cup√©rer les audios avec m√©tadonn√©es de qualit√©
            attachments = await self.database_service.prisma.messageattachment.find_many(
                where={
                    "message": {
                        "senderId": user_id
                    },
                    "mimeType": {
                        "startswith": "audio/"
                    }
                },
                order={"createdAt": "desc"},
                take=limit
            )

            if not attachments:
                return None

            # Convertir en AudioQualityMetadata et calculer les scores
            quality_audios: List[AudioQualityMetadata] = []
            for att in attachments:
                if att.filePath and os.path.exists(att.filePath):
                    duration_ms = await self._get_audio_duration_ms(att.filePath)

                    # Extraire les m√©tadonn√©es de qualit√© si disponibles
                    # Ces champs doivent √™tre ajout√©s au sch√©ma Prisma de MessageAttachment
                    noise_level = getattr(att, 'noiseLevel', 0.0) or 0.0
                    clarity_score = getattr(att, 'clarityScore', 1.0) or 1.0
                    has_other_speakers = getattr(att, 'hasOtherSpeakers', False) or False

                    audio_meta = AudioQualityMetadata(
                        attachment_id=att.id,
                        file_path=att.filePath,
                        duration_ms=duration_ms,
                        noise_level=noise_level,
                        clarity_score=clarity_score,
                        has_other_speakers=has_other_speakers,
                        created_at=att.createdAt if hasattr(att, 'createdAt') else datetime.now()
                    )
                    audio_meta.calculate_overall_score()
                    quality_audios.append(audio_meta)

            if not quality_audios:
                return None

            # Trier par score d√©croissant et retourner le meilleur
            quality_audios.sort(key=lambda x: x.overall_score, reverse=True)
            best_audio = quality_audios[0]

            logger.info(
                f"[VOICE_CLONE] üéØ Meilleur audio s√©lectionn√© pour {user_id}: "
                f"id={best_audio.attachment_id}, duration={best_audio.duration_ms}ms, "
                f"score={best_audio.overall_score:.2f}"
            )
            return best_audio

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur s√©lection meilleur audio: {e}")
            return None

    async def _calculate_total_duration(self, audio_paths: List[str]) -> int:
        """Calcule la dur√©e totale de plusieurs fichiers audio"""
        total = 0
        for path in audio_paths:
            duration = await self._get_audio_duration_ms(path)
            total += duration
        return total

    async def _get_audio_duration_ms(self, audio_path: str) -> int:
        """R√©cup√®re la dur√©e d'un fichier audio en millisecondes"""
        if not AUDIO_PROCESSING_AVAILABLE:
            return 0

        try:
            import librosa
            loop = asyncio.get_event_loop()
            duration = await loop.run_in_executor(
                None,
                lambda: librosa.get_duration(path=audio_path)
            )
            return int(duration * 1000)
        except Exception as e:
            logger.warning(f"[VOICE_CLONE] Impossible de lire la dur√©e de {audio_path}: {e}")
            return 0

    def _calculate_quality_score(self, duration_ms: int, audio_count: int) -> float:
        """
        Calcule un score de qualit√© bas√© sur la dur√©e et le nombre d'audios.

        - 0-10s: 0.3 (faible)
        - 10-30s: 0.5 (moyen)
        - 30-60s: 0.7 (bon)
        - 60s+: 0.9 (excellent)
        - Bonus: +0.05 par audio suppl√©mentaire (max +0.1)
        """
        if duration_ms < 10_000:
            base_score = 0.3
        elif duration_ms < 30_000:
            base_score = 0.5
        elif duration_ms < 60_000:
            base_score = 0.7
        else:
            base_score = 0.9

        audio_bonus = min(0.1, (audio_count - 1) * 0.05)
        return min(1.0, base_score + audio_bonus)

    async def _load_cached_model(self, user_id: str) -> Optional[VoiceModel]:
        """Charge un mod√®le depuis le cache fichier: {voice_cache_dir}/{user_id}/metadata.json"""
        metadata_path = self.voice_cache_dir / user_id / "metadata.json"

        if not metadata_path.exists():
            return None

        try:
            with open(metadata_path, 'r') as f:
                data = json.load(f)
            return VoiceModel.from_dict(data)
        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur chargement cache {user_id}: {e}")
            return None

    async def _load_embedding(self, model: VoiceModel) -> VoiceModel:
        """Charge l'embedding d'un mod√®le depuis le fichier pickle"""
        try:
            with open(model.embedding_path, 'rb') as f:
                model.embedding = pickle.load(f)
        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur chargement embedding: {e}")
            model.embedding = np.zeros(256)
        return model

    async def _save_model_to_cache(self, model: VoiceModel):
        """Sauvegarde un mod√®le dans le cache fichier: {voice_cache_dir}/{user_id}/"""
        user_dir = self.voice_cache_dir / model.user_id
        user_dir.mkdir(parents=True, exist_ok=True)

        # Sauvegarder les m√©tadonn√©es: {user_id}/metadata.json
        metadata_path = user_dir / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(model.to_dict(), f, indent=2)

        # Sauvegarder l'embedding: {user_id}/{userId}_{profileId}_{timestamp}.pkl
        if model.embedding is not None:
            with open(model.embedding_path, 'wb') as f:
                pickle.dump(model.embedding, f)

        logger.info(f"[VOICE_CLONE] üíæ Mod√®le sauvegard√©: {user_dir}")

    async def schedule_quarterly_recalibration(self):
        """
        T√¢che planifi√©e pour recalibrer les mod√®les de voix trimestriellement (tous les 3 mois).
        √Ä ex√©cuter via un cron job ou un scheduler.
        S√©lectionne le meilleur audio: le plus long, le plus clair, sans bruit, le plus r√©cent.
        """
        logger.info("[VOICE_CLONE] üîÑ D√©marrage recalibration trimestrielle...")

        # Lister tous les mod√®les en cache
        all_models = await self._list_all_cached_models()

        recalibrated = 0
        for model in all_models:
            if model.next_recalibration_at and datetime.now() >= model.next_recalibration_at:
                logger.info(f"[VOICE_CLONE] üîÑ Recalibration pour {model.user_id}")

                # S√©lectionner le meilleur audio bas√© sur les crit√®res de qualit√©
                best_audio = await self._get_best_audio_for_cloning(model.user_id)

                if best_audio:
                    # Utiliser le meilleur audio pour r√©g√©n√©rer le mod√®le
                    await self._create_voice_model(
                        model.user_id,
                        [best_audio.file_path],
                        best_audio.duration_ms
                    )
                    recalibrated += 1
                    logger.info(
                        f"[VOICE_CLONE] ‚úÖ Mod√®le recalibr√© pour {model.user_id} "
                        f"avec audio {best_audio.attachment_id} (score: {best_audio.overall_score:.2f})"
                    )
                else:
                    # Fallback: utiliser l'historique audio classique
                    recent_audios = await self._get_user_audio_history(model.user_id)
                    if recent_audios:
                        total_duration = await self._calculate_total_duration(recent_audios)
                        await self._create_voice_model(
                            model.user_id,
                            recent_audios,
                            total_duration
                        )
                        recalibrated += 1

        logger.info(f"[VOICE_CLONE] ‚úÖ Recalibration trimestrielle termin√©e: {recalibrated} mod√®les mis √† jour")

    async def _list_all_cached_models(self) -> List[VoiceModel]:
        """Liste tous les mod√®les en cache: parcourt {voice_cache_dir}/{user_id}/"""
        models = []
        for user_dir in self.voice_cache_dir.iterdir():
            if user_dir.is_dir():
                model = await self._load_cached_model(user_dir.name)
                if model:
                    models.append(model)
        return models

    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du service"""
        all_models = await self._list_all_cached_models()
        return {
            "service": "VoiceCloneService",
            "initialized": self.is_initialized,
            "openvoice_available": OPENVOICE_AVAILABLE,
            "audio_processing_available": AUDIO_PROCESSING_AVAILABLE,
            "cache_dir": str(self.voice_cache_dir),
            "device": self.device,
            "cached_models_count": len(all_models),
            "min_audio_duration_ms": self.MIN_AUDIO_DURATION_MS,
            "max_age_days": self.VOICE_MODEL_MAX_AGE_DAYS
        }

    async def close(self):
        """Lib√®re les ressources"""
        logger.info("[VOICE_CLONE] üõë Fermeture du service")
        self.tone_color_converter = None
        self.se_extractor_module = None
        self.is_initialized = False


# Fonction helper pour obtenir l'instance singleton
def get_voice_clone_service() -> VoiceCloneService:
    """Retourne l'instance singleton du service de clonage vocal"""
    return VoiceCloneService()
