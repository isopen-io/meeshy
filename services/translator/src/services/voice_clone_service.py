"""
Service de clonage vocal - Singleton
GÃ¨re les modÃ¨les de voix des utilisateurs avec cache et amÃ©lioration continue.
Architecture: OpenVoice V2 pour extraction d'embedding, cache fichier pour persistance.
Fonctionne sur CPU, CUDA, et MPS (Apple Silicon).
"""

import os
import logging
import time
import asyncio
import threading
import pickle
import json
import hashlib
import struct
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path

# Import settings for centralized configuration
from config.settings import get_settings

# Configuration du logging
logger = logging.getLogger(__name__)

# Flags de disponibilitÃ© des dÃ©pendances
OPENVOICE_AVAILABLE = False
AUDIO_PROCESSING_AVAILABLE = False

try:
    from openvoice import se_extractor
    from openvoice.api import ToneColorConverter
    OPENVOICE_AVAILABLE = True
    logger.info("âœ… [VOICE_CLONE] OpenVoice disponible")
except ImportError:
    logger.warning("âš ï¸ [VOICE_CLONE] OpenVoice non disponible - clonage vocal dÃ©sactivÃ©")

try:
    import numpy as np
    from pydub import AudioSegment
    import soundfile as sf
    AUDIO_PROCESSING_AVAILABLE = True
    logger.info("âœ… [VOICE_CLONE] Audio processing disponible")
except ImportError:
    logger.warning("âš ï¸ [VOICE_CLONE] numpy/pydub/soundfile non disponibles")
    import numpy as np  # numpy should be available

# Import PerformanceOptimizer for device detection
try:
    from utils.performance import get_performance_optimizer
    PERF_OPTIMIZER_AVAILABLE = True
except ImportError:
    PERF_OPTIMIZER_AVAILABLE = False
    logger.debug("[VOICE_CLONE] PerformanceOptimizer not available, using manual device selection")

# Import Redis cache service for voice profile caching
import base64
from services.redis_service import get_audio_cache_service, AudioCacheService

# Import unified voice models
from models.voice_models import VoiceCharacteristics

# Import voice clone modules (refactored from this file)
from services.voice_clone.voice_fingerprint import VoiceFingerprint
from services.voice_clone.voice_metadata import (
    SpeakerInfo,
    RecordingMetadata,
    AudioQualityMetadata,
    VoiceModel,
    TemporaryVoiceProfile,
    MultiSpeakerTranslationContext
)
from services.voice_clone.voice_analyzer import VoiceAnalyzer, get_voice_analyzer


# NOTE: Les classes suivantes ont Ã©tÃ© dÃ©placÃ©es vers services/voice_clone/:
# - VoiceFingerprint â†’ voice_fingerprint.py
# - SpeakerInfo, RecordingMetadata, AudioQualityMetadata â†’ voice_metadata.py
# - VoiceModel, TemporaryVoiceProfile, MultiSpeakerTranslationContext â†’ voice_metadata.py
# - VoiceAnalyzer, get_voice_analyzer â†’ voice_analyzer.py

# Les dÃ©finitions originales ont Ã©tÃ© supprimÃ©es ci-dessous.
# Elles sont maintenant importÃ©es depuis les modules refactorisÃ©s.


# ==============================================================================
# VoiceCloneService - Service principal de clonage vocal
# ==============================================================================
# TODO: Refactoriser cette classe en modules plus petits (~400L chacun):
# - voice_clone_init.py: Initialisation et configuration
# - voice_clone_model.py: CrÃ©ation et amÃ©lioration de modÃ¨les
# - voice_clone_cache.py: Gestion de cache et stockage
# - voice_clone_audio.py: OpÃ©rations audio et multi-locuteurs
# ==============================================================================


# Classes refactorisÃ©es importÃ©es ci-dessus depuis services/voice_clone/
# Anciennes dÃ©finitions supprimÃ©es (1392 lignes)


class VoiceCloneService:
    """
    Service de clonage vocal - Singleton

    FonctionnalitÃ©s:
    - CrÃ©ation de modÃ¨les de voix Ã  partir d'audios
    - Cache des modÃ¨les (90 jours / 3 mois)
    - AgrÃ©gation d'audios si durÃ©e insuffisante
    - AmÃ©lioration continue des modÃ¨les
    - Recalibration trimestrielle
    - SÃ©lection du meilleur audio (le plus long, le plus clair, sans bruit)
    """

    _instance = None
    _lock = threading.Lock()

    # Configuration
    MIN_AUDIO_DURATION_MS = 10_000  # 10 secondes minimum pour clonage de qualitÃ©
    VOICE_MODEL_MAX_AGE_DAYS = 90   # Recalibration trimestrielle (3 mois)
    MAX_AUDIO_HISTORY = 20          # Nombre max d'audios Ã  agrÃ©ger
    IMPROVEMENT_WEIGHT_OLD = 0.7    # Poids de l'ancien embedding
    IMPROVEMENT_WEIGHT_NEW = 0.3    # Poids du nouveau

    def __new__(cls, *args, **kwargs):
        """Singleton pattern"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(
        self,
        voice_cache_dir: Optional[str] = None,
        device: str = "auto",
        database_service = None
    ):
        if self._initialized:
            return

        # Load centralized settings
        self._settings = get_settings()

        # Configuration - utiliser le chemin centralisÃ© des settings
        self.voice_cache_dir = Path(voice_cache_dir or os.getenv('VOICE_MODEL_CACHE_DIR', self._settings.voice_models_path))

        # Device detection: Use PerformanceOptimizer if available, else fallback to settings
        env_device = os.getenv('VOICE_CLONE_DEVICE', self._settings.voice_clone_device)
        if env_device == "auto" and PERF_OPTIMIZER_AVAILABLE:
            perf_opt = get_performance_optimizer()
            self.device = perf_opt.device
            logger.info(f"[VOICE_CLONE] Device auto-detected: {self.device}")
        else:
            # Manual device selection or explicit device specified
            self.device = env_device if env_device != "auto" else "cpu"

        # Service de persistance MongoDB (optionnel, pour fallback)
        self.database_service = database_service

        # Service de cache Redis pour les profils vocaux
        self._audio_cache: Optional[AudioCacheService] = None

        # OpenVoice components
        self.tone_color_converter = None
        self.se_extractor_module = None

        # Etat
        self.is_initialized = False
        self._init_lock = asyncio.Lock()

        # Repertoire temporaire pour fichiers audio
        self.voice_cache_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"[VOICE_CLONE] Service cree: device={self.device}, models_path={self._settings.models_path}")
        self._initialized = True

    def set_database_service(self, database_service):
        """Injecte le service de base de donnees MongoDB (optionnel, fallback)"""
        self.database_service = database_service

    def _get_audio_cache(self) -> AudioCacheService:
        """Retourne le service de cache audio Redis (lazy init)"""
        if self._audio_cache is None:
            self._audio_cache = get_audio_cache_service(self._settings)
        return self._audio_cache

    async def initialize(self) -> bool:
        """Initialise OpenVoice pour le clonage vocal"""
        if self.is_initialized:
            return True

        async with self._init_lock:
            if self.is_initialized:
                return True

            if not OPENVOICE_AVAILABLE:
                logger.warning("[VOICE_CLONE] OpenVoice non disponible - mode dÃ©gradÃ©")
                self.is_initialized = True
                return True

            try:
                start_time = time.time()
                logger.info("[VOICE_CLONE] ğŸ”„ Initialisation d'OpenVoice...")

                # Charger dans un thread pour ne pas bloquer
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._load_openvoice)

                load_time = time.time() - start_time
                logger.info(f"[VOICE_CLONE] âœ… OpenVoice initialisÃ© en {load_time:.2f}s")

                self.is_initialized = True
                return True

            except Exception as e:
                logger.error(f"[VOICE_CLONE] âŒ Erreur initialisation OpenVoice: {e}")
                import traceback
                traceback.print_exc()
                self.is_initialized = True  # Mode dÃ©gradÃ©
                return True

    def _load_openvoice(self):
        """Charge OpenVoice (appelÃ© dans un thread)"""
        # Utiliser le chemin centralisÃ© depuis settings
        checkpoints_dir = self._settings.openvoice_checkpoints_path
        logger.info(f"[VOICE_CLONE] Chargement OpenVoice depuis {checkpoints_dir}")

        # Chemins des fichiers requis (OpenVoice V2 les met dans converter/)
        checkpoints_path = Path(checkpoints_dir)
        converter_path = checkpoints_path / "converter"
        config_path = converter_path / "config.json"
        checkpoint_path = converter_path / "checkpoint.pth"

        # TÃ©lÃ©charger les checkpoints OpenVoice V2 s'ils n'existent pas
        if not config_path.exists() or not checkpoint_path.exists():
            logger.info("[VOICE_CLONE] ğŸ“¥ TÃ©lÃ©chargement des checkpoints OpenVoice V2...")
            self._download_openvoice_checkpoints(checkpoints_path)

        # VÃ©rifier que les fichiers existent maintenant
        if not config_path.exists():
            raise FileNotFoundError(f"OpenVoice config.json non trouvÃ©: {config_path}")

        # ToneColorConverter attend le chemin vers config.json, pas le rÃ©pertoire
        self.tone_color_converter = ToneColorConverter(
            str(config_path),
            device=self.device
        )

        # Charger le checkpoint
        if checkpoint_path.exists():
            self.tone_color_converter.load_ckpt(str(checkpoint_path))
            logger.info(f"[VOICE_CLONE] âœ… Checkpoint chargÃ©: {checkpoint_path}")

        self.se_extractor_module = se_extractor

    def _download_openvoice_checkpoints(self, checkpoints_path: Path):
        """TÃ©lÃ©charge les checkpoints OpenVoice V2 depuis MyShell S3"""
        import zipfile
        import urllib.request
        import tempfile

        OPENVOICE_V2_URL = "https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip"

        checkpoints_path.mkdir(parents=True, exist_ok=True)

        try:
            # TÃ©lÃ©charger le zip
            with tempfile.NamedTemporaryFile(suffix=".zip", delete=False) as tmp_file:
                tmp_path = tmp_file.name
                logger.info(f"[VOICE_CLONE] TÃ©lÃ©chargement depuis {OPENVOICE_V2_URL}...")
                urllib.request.urlretrieve(OPENVOICE_V2_URL, tmp_path)

            # Extraire le zip
            logger.info(f"[VOICE_CLONE] Extraction vers {checkpoints_path}...")
            with zipfile.ZipFile(tmp_path, 'r') as zip_ref:
                # Le zip contient un dossier checkpoints_v2/, on extrait son contenu
                for member in zip_ref.namelist():
                    # Retirer le prÃ©fixe "checkpoints_v2/" du chemin
                    if member.startswith("checkpoints_v2/"):
                        target_path = member[len("checkpoints_v2/"):]
                        if target_path:  # Ignorer le dossier racine
                            source = zip_ref.read(member)
                            dest_path = checkpoints_path / target_path
                            dest_path.parent.mkdir(parents=True, exist_ok=True)
                            if not member.endswith('/'):  # C'est un fichier
                                dest_path.write_bytes(source)

            # Nettoyer
            Path(tmp_path).unlink(missing_ok=True)
            logger.info("[VOICE_CLONE] âœ… Checkpoints OpenVoice V2 tÃ©lÃ©chargÃ©s")

        except Exception as e:
            logger.error(f"[VOICE_CLONE] âŒ Erreur tÃ©lÃ©chargement checkpoints: {e}")
            raise

    async def get_or_create_voice_model(
        self,
        user_id: str,
        current_audio_path: Optional[str] = None,
        current_audio_duration_ms: int = 0
    ) -> VoiceModel:
        """
        RÃ©cupÃ¨re ou crÃ©e un modÃ¨le de voix pour un utilisateur.

        Logique:
        1. Si modÃ¨le en cache et rÃ©cent â†’ utiliser
        2. Si modÃ¨le en cache mais ancien â†’ amÃ©liorer avec nouvel audio
        3. Si pas de modÃ¨le et audio trop court â†’ agrÃ©ger historique
        4. CrÃ©er nouveau modÃ¨le

        Args:
            user_id: ID de l'utilisateur
            current_audio_path: Audio actuel pour le clonage (optionnel)
            current_audio_duration_ms: DurÃ©e de l'audio actuel

        Returns:
            VoiceModel prÃªt Ã  l'emploi
        """
        # 1. VÃ©rifier le cache
        cached_model = await self._load_cached_model(user_id)

        if cached_model:
            age_days = (datetime.now() - cached_model.updated_at).days

            # ModÃ¨le rÃ©cent â†’ utiliser directement
            if age_days < self.VOICE_MODEL_MAX_AGE_DAYS:
                logger.info(f"[VOICE_CLONE] ğŸ“¦ ModÃ¨le en cache pour {user_id} (age: {age_days}j)")

                # Charger l'embedding si pas en mÃ©moire
                if cached_model.embedding is None:
                    cached_model = await self._load_embedding(cached_model)

                return cached_model

            # ModÃ¨le ancien â†’ amÃ©liorer si on a un nouvel audio
            if current_audio_path:
                logger.info(f"[VOICE_CLONE] ğŸ”„ ModÃ¨le obsolÃ¨te pour {user_id}, amÃ©lioration...")
                return await self._improve_model(cached_model, current_audio_path)

            # Sinon utiliser l'ancien modÃ¨le
            logger.info(f"[VOICE_CLONE] âš ï¸ ModÃ¨le obsolÃ¨te pour {user_id} mais pas de nouvel audio")
            if cached_model.embedding is None:
                cached_model = await self._load_embedding(cached_model)
            return cached_model

        # 2. Pas de modÃ¨le â†’ crÃ©er
        if not current_audio_path:
            # Essayer de rÃ©cupÃ©rer l'historique audio
            audio_paths = await self._get_user_audio_history(user_id)
            if not audio_paths:
                raise ValueError(f"Aucun audio disponible pour crÃ©er le modÃ¨le de voix de {user_id}")
            current_audio_path = audio_paths[0]
            current_audio_duration_ms = await self._get_audio_duration_ms(current_audio_path)

        audio_paths = [current_audio_path]
        total_duration = current_audio_duration_ms

        # Si audio trop court, chercher l'historique
        if total_duration < self.MIN_AUDIO_DURATION_MS:
            logger.info(f"[VOICE_CLONE] âš ï¸ Audio trop court ({total_duration}ms), agrÃ©gation historique...")
            historical_audios = await self._get_user_audio_history(user_id, exclude=[current_audio_path])
            audio_paths.extend(historical_audios)
            total_duration = await self._calculate_total_duration(audio_paths)

            logger.info(f"[VOICE_CLONE] ğŸ“š {len(audio_paths)} audios agrÃ©gÃ©s, total: {total_duration}ms")

        # CrÃ©er le modÃ¨le avec ce qu'on a
        return await self._create_voice_model(user_id, audio_paths, total_duration)

    async def create_voice_model_from_gateway_profile(
        self,
        profile_data: Dict[str, Any],
        user_id: str
    ) -> Optional[VoiceModel]:
        """
        CrÃ©e un VoiceModel Ã  partir du profil vocal reÃ§u de Gateway.

        Cette mÃ©thode permet Ã  Gateway d'envoyer un profil vocal existant
        (par exemple celui de l'Ã©metteur original d'un message transfÃ©rÃ©)
        sans que Translator ait besoin d'accÃ©der Ã  MongoDB.

        Args:
            profile_data: DonnÃ©es du profil vocal envoyÃ©es par Gateway:
                - profileId: str - ID unique du profil
                - userId: str - ID de l'utilisateur propriÃ©taire du profil
                - embedding: str - Embedding Base64 encoded (numpy array)
                - qualityScore: float - Score de qualitÃ© 0-1
                - fingerprint: Dict - Empreinte vocale (optionnel)
                - voiceCharacteristics: Dict - CaractÃ©ristiques vocales (optionnel)
                - version: int - Version du profil
                - audioCount: int - Nombre d'audios agrÃ©gÃ©s
                - totalDurationMs: int - DurÃ©e totale des audios

            user_id: ID de l'utilisateur (pour logs)

        Returns:
            VoiceModel prÃªt Ã  l'emploi, ou None si Ã©chec
        """
        if not profile_data:
            logger.warning(f"[VOICE_CLONE] âš ï¸ Pas de profil fourni par Gateway pour {user_id}")
            return None

        try:
            logger.info(f"[VOICE_CLONE] ğŸ“¦ CrÃ©ation VoiceModel depuis profil Gateway pour {user_id}")

            # DÃ©coder l'embedding Base64
            import base64
            embedding_base64 = profile_data.get('embedding')
            if not embedding_base64:
                logger.error(f"[VOICE_CLONE] âŒ Embedding manquant dans le profil Gateway")
                return None

            embedding_bytes = base64.b64decode(embedding_base64)
            embedding = np.frombuffer(embedding_bytes, dtype=np.float32)

            logger.info(f"[VOICE_CLONE] âœ… Embedding dÃ©codÃ©: shape={embedding.shape}")

            # CrÃ©er les caractÃ©ristiques vocales si fournies
            voice_characteristics = None
            voice_chars_data = profile_data.get('voiceCharacteristics')
            if voice_chars_data:
                try:
                    voice_characteristics = VoiceCharacteristics(
                        pitch_mean_hz=voice_chars_data.get('pitch_mean_hz', 0),
                        pitch_std_hz=voice_chars_data.get('pitch_std_hz', 0),
                        pitch_range_hz=voice_chars_data.get('pitch_range_hz', (0, 0)),
                        estimated_gender=voice_chars_data.get('estimated_gender', 'unknown'),
                        speaking_rate_wpm=voice_chars_data.get('speaking_rate_wpm', 0),
                        spectral_centroid_hz=voice_chars_data.get('spectral_centroid_hz', 0),
                        spectral_bandwidth_hz=voice_chars_data.get('spectral_bandwidth_hz', 0),
                        energy_mean=voice_chars_data.get('energy_mean', 0),
                        energy_std=voice_chars_data.get('energy_std', 0),
                        mfcc_signature=voice_chars_data.get('mfcc_signature'),
                        formants_hz=voice_chars_data.get('formants_hz'),
                        jitter_percent=voice_chars_data.get('jitter_percent'),
                        shimmer_percent=voice_chars_data.get('shimmer_percent'),
                        confidence=voice_chars_data.get('confidence', 0.8)
                    )
                except Exception as e:
                    logger.warning(f"[VOICE_CLONE] âš ï¸ Impossible de recrÃ©er VoiceCharacteristics: {e}")

            # CrÃ©er l'empreinte vocale si fournie
            fingerprint = None
            fingerprint_data = profile_data.get('fingerprint')
            if fingerprint_data:
                try:
                    fingerprint = VoiceFingerprint(
                        fingerprint_id=fingerprint_data.get('fingerprint_id', ''),
                        signature=fingerprint_data.get('signature', ''),
                        signature_short=fingerprint_data.get('signature_short', ''),
                        audio_duration_ms=fingerprint_data.get('audio_duration_ms', 0),
                        created_at=datetime.fromisoformat(fingerprint_data.get('created_at', datetime.now().isoformat()))
                    )
                except Exception as e:
                    logger.warning(f"[VOICE_CLONE] âš ï¸ Impossible de recrÃ©er VoiceFingerprint: {e}")

            # CrÃ©er un dossier temporaire pour l'embedding (nÃ©cessaire pour TTS)
            profile_user_id = profile_data.get('userId', user_id)
            user_dir = self.voice_cache_dir / profile_user_id
            user_dir.mkdir(parents=True, exist_ok=True)

            profile_id = profile_data.get('profileId', f"vfp_{profile_user_id[:8]}")
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            embedding_filename = f"{profile_user_id}_{profile_id}_{timestamp}_gateway.pkl"
            embedding_path = str(user_dir / embedding_filename)

            # Sauvegarder l'embedding dans un fichier temporaire (pickle dÃ©jÃ  utilisÃ© dans le service)
            with open(embedding_path, 'wb') as f:
                pickle.dump(embedding, f)

            logger.info(f"[VOICE_CLONE] ğŸ’¾ Embedding sauvegardÃ©: {embedding_path}")

            # CrÃ©er le VoiceModel
            model = VoiceModel(
                user_id=profile_user_id,
                embedding_path=embedding_path,
                audio_count=profile_data.get('audioCount', 1),
                total_duration_ms=profile_data.get('totalDurationMs', 0),
                quality_score=profile_data.get('qualityScore', 0.8),
                profile_id=profile_id,
                version=profile_data.get('version', 1),
                created_at=datetime.now(),
                updated_at=datetime.now(),
                embedding=embedding,
                voice_characteristics=voice_characteristics,
                fingerprint=fingerprint
            )

            logger.info(
                f"[VOICE_CLONE] âœ… VoiceModel crÃ©Ã© depuis Gateway: "
                f"user={profile_user_id}, quality={model.quality_score:.2f}, "
                f"profile_id={profile_id}"
            )

            return model

        except Exception as e:
            logger.error(f"[VOICE_CLONE] âŒ Erreur crÃ©ation VoiceModel depuis Gateway: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _create_voice_model(
        self,
        user_id: str,
        audio_paths: List[str],
        total_duration_ms: int
    ) -> VoiceModel:
        """
        CrÃ©e un nouveau modÃ¨le de voix Ã  partir des audios.

        IMPORTANT: Extrait uniquement les segments du locuteur principal
        pour garantir que le clonage ne concerne que sa voix.
        """
        import uuid as uuid_module
        start_time = time.time()
        logger.info(f"[VOICE_CLONE] ğŸ¤ CrÃ©ation modÃ¨le pour {user_id} ({len(audio_paths)} audios)")

        # Filtrer les audios valides
        valid_paths = [p for p in audio_paths if os.path.exists(p)]
        if not valid_paths:
            raise ValueError("Aucun fichier audio valide trouvÃ©")

        # CrÃ©er le dossier utilisateur: {voice_cache_dir}/{user_id}/
        user_dir = self.voice_cache_dir / user_id
        user_dir.mkdir(parents=True, exist_ok=True)

        # =====================================================================
        # EXTRACTION DU LOCUTEUR PRINCIPAL UNIQUEMENT
        # Pour chaque audio, extraire uniquement les segments du locuteur principal
        # =====================================================================
        voice_analyzer = get_voice_analyzer()
        extracted_paths = []
        primary_voice_chars = None
        recording_metadata = None

        for audio_path in valid_paths:
            try:
                # Extraire uniquement les segments du locuteur principal
                extracted_path, metadata = await voice_analyzer.extract_primary_speaker_audio(
                    audio_path,
                    output_path=str(user_dir / f"primary_{os.path.basename(audio_path)}"),
                    min_segment_duration_ms=100
                )
                extracted_paths.append(extracted_path)

                # Conserver les caractÃ©ristiques vocales du premier locuteur principal
                if primary_voice_chars is None and metadata.primary_speaker:
                    primary_voice_chars = metadata.primary_speaker.voice_characteristics
                    recording_metadata = metadata
                    logger.info(
                        f"[VOICE_CLONE] Locuteur principal dÃ©tectÃ©: "
                        f"gender={primary_voice_chars.estimated_gender}, "
                        f"pitch={primary_voice_chars.pitch_mean_hz:.1f}Hz"
                    )

            except Exception as e:
                logger.warning(f"[VOICE_CLONE] Erreur extraction locuteur principal: {e}")
                # Fallback: utiliser l'audio complet
                extracted_paths.append(audio_path)

        # Recalculer la durÃ©e totale aprÃ¨s extraction
        extracted_duration_ms = 0
        for path in extracted_paths:
            extracted_duration_ms += await self._get_audio_duration_ms(path)

        logger.info(
            f"[VOICE_CLONE] Audio extrait: {extracted_duration_ms}ms "
            f"(original: {total_duration_ms}ms, {len(extracted_paths)} fichiers)"
        )

        # ConcatÃ©ner les audios extraits si multiples
        if len(extracted_paths) > 1:
            combined_audio = await self._concatenate_audios(extracted_paths, user_id)
        else:
            combined_audio = extracted_paths[0]

        # GÃ©nÃ©rer un profile_id unique
        profile_id = uuid_module.uuid4().hex[:12]
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")

        # Extraire l'embedding de voix (du locuteur principal uniquement)
        embedding = await self._extract_voice_embedding(combined_audio, user_dir)

        # Calculer score de qualitÃ©
        quality_score = self._calculate_quality_score(extracted_duration_ms, len(valid_paths))

        # Chemin de l'embedding avec nouvelle convention: {userId}_{profileId}_{timestamp}.pkl
        embedding_filename = f"{user_id}_{profile_id}_{timestamp}.pkl"
        embedding_path = str(user_dir / embedding_filename)

        # CrÃ©er le modÃ¨le avec les caractÃ©ristiques vocales du locuteur principal
        model = VoiceModel(
            user_id=user_id,
            embedding_path=embedding_path,
            audio_count=len(valid_paths),
            total_duration_ms=extracted_duration_ms,  # DurÃ©e extraite, pas originale
            quality_score=quality_score,
            profile_id=profile_id,
            version=1,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            next_recalibration_at=datetime.now() + timedelta(days=self.VOICE_MODEL_MAX_AGE_DAYS),
            embedding=embedding,
            voice_characteristics=primary_voice_chars  # CaractÃ©ristiques du locuteur principal
        )

        # GÃ©nÃ©rer l'empreinte vocale unique
        if model.voice_characteristics or model.embedding is not None:
            fingerprint = model.generate_fingerprint()
            if fingerprint:
                logger.info(f"[VOICE_CLONE] Empreinte vocale: {fingerprint.fingerprint_id}")

        # Sauvegarder
        await self._save_model_to_cache(model)

        processing_time = int((time.time() - start_time) * 1000)
        logger.info(f"[VOICE_CLONE] âœ… ModÃ¨le crÃ©Ã© pour {user_id}: quality={quality_score:.2f}, time={processing_time}ms")

        return model

    async def _improve_model(
        self,
        existing_model: VoiceModel,
        new_audio_path: str
    ) -> VoiceModel:
        """
        AmÃ©liore un modÃ¨le existant avec un nouvel audio.

        RÃˆGLE: La mise Ã  jour n'est effectuÃ©e QUE si la signature vocale
        du nouvel audio correspond au profil existant (similaritÃ© > 80%).
        """
        logger.info(f"[VOICE_CLONE] ğŸ”„ VÃ©rification amÃ©lioration modÃ¨le pour {existing_model.user_id}")

        voice_analyzer = get_voice_analyzer()

        # Charger l'embedding existant si nÃ©cessaire
        if existing_model.embedding is None:
            existing_model = await self._load_embedding(existing_model)

        # VÃ©rifier si la signature correspond avant mise Ã  jour
        if existing_model.fingerprint:
            metadata = await voice_analyzer.analyze_audio(new_audio_path)
            can_update, reason, matched_speaker = voice_analyzer.can_update_user_profile(
                metadata,
                existing_model.fingerprint,
                similarity_threshold=0.80
            )

            if not can_update:
                logger.warning(
                    f"[VOICE_CLONE] âš ï¸ Mise Ã  jour refusÃ©e pour {existing_model.user_id}: {reason}"
                )
                # Retourner le modÃ¨le existant sans modification
                return existing_model

            logger.info(f"[VOICE_CLONE] âœ… Signature vocale vÃ©rifiÃ©e: {reason}")

        # Extraire embedding du nouvel audio
        user_dir = self.voice_cache_dir / existing_model.user_id / "temp"
        user_dir.mkdir(parents=True, exist_ok=True)

        new_embedding = await self._extract_voice_embedding(new_audio_path, user_dir)

        if existing_model.embedding is not None and new_embedding is not None:
            # Moyenne pondÃ©rÃ©e (plus de poids aux anciens pour stabilitÃ©)
            improved_embedding = (
                self.IMPROVEMENT_WEIGHT_OLD * existing_model.embedding +
                self.IMPROVEMENT_WEIGHT_NEW * new_embedding
            )
        else:
            improved_embedding = new_embedding if new_embedding is not None else existing_model.embedding

        # Mettre Ã  jour le modÃ¨le
        existing_model.embedding = improved_embedding
        existing_model.updated_at = datetime.now()
        existing_model.audio_count += 1
        existing_model.quality_score = min(1.0, existing_model.quality_score + 0.05)
        existing_model.version += 1
        existing_model.next_recalibration_at = datetime.now() + timedelta(days=self.VOICE_MODEL_MAX_AGE_DAYS)

        # RÃ©gÃ©nÃ©rer l'empreinte vocale avec le nouvel embedding
        if existing_model.voice_characteristics:
            existing_model.generate_fingerprint()

        # Sauvegarder
        await self._save_model_to_cache(existing_model)

        logger.info(f"[VOICE_CLONE] âœ… ModÃ¨le amÃ©liorÃ© pour {existing_model.user_id} (v{existing_model.version})")
        return existing_model

    async def _extract_voice_embedding(self, audio_path: str, target_dir: Path) -> Optional[np.ndarray]:
        """Extrait l'embedding de voix d'un fichier audio"""
        if not OPENVOICE_AVAILABLE or self.se_extractor_module is None:
            logger.warning("[VOICE_CLONE] OpenVoice non disponible, embedding factice")
            return np.zeros(256)  # Embedding factice

        try:
            loop = asyncio.get_event_loop()
            # get_se retourne un tuple (embedding, audio_name)
            result = await loop.run_in_executor(
                None,
                lambda: self.se_extractor_module.get_se(
                    audio_path,
                    self.tone_color_converter,
                    target_dir=str(target_dir)
                )
            )
            # Extraire l'embedding du tuple
            embedding, _audio_name = result

            # Convertir le tensor PyTorch en numpy array si nÃ©cessaire
            if hasattr(embedding, 'cpu'):
                embedding = embedding.cpu().detach().numpy()

            return embedding
        except Exception as e:
            logger.error(f"[VOICE_CLONE] âŒ Erreur extraction embedding: {e}")
            return np.zeros(256)

    async def _concatenate_audios(self, audio_paths: List[str], user_id: str) -> str:
        """ConcatÃ¨ne plusieurs fichiers audio en un seul"""
        if not AUDIO_PROCESSING_AVAILABLE:
            return audio_paths[0]  # Retourner le premier si pas de processing

        try:
            combined = AudioSegment.empty()
            for path in audio_paths:
                try:
                    audio = AudioSegment.from_file(path)
                    combined += audio
                except Exception as e:
                    logger.warning(f"[VOICE_CLONE] Impossible de lire {path}: {e}")

            # Sauvegarder le fichier combinÃ©
            output_path = self.voice_cache_dir / user_id / "combined_audio.wav"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            combined.export(str(output_path), format="wav")

            return str(output_path)

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur concatÃ©nation: {e}")
            return audio_paths[0]

    async def _get_user_audio_history(
        self,
        user_id: str,
        exclude: Optional[List[str]] = None,
        limit: int = None
    ) -> List[str]:
        """
        RÃ©cupÃ¨re l'historique des messages audio d'un utilisateur.
        Utilise la base de donnÃ©es pour trouver les attachements audio.
        """
        limit = limit or self.MAX_AUDIO_HISTORY
        exclude = exclude or []

        if not self.database_service:
            logger.warning("[VOICE_CLONE] Database service non disponible")
            return []

        try:
            # RequÃªte pour rÃ©cupÃ©rer les audios de l'utilisateur
            attachments = await self.database_service.prisma.messageattachment.find_many(
                where={
                    "message": {
                        "senderId": user_id
                    },
                    "mimeType": {
                        "startswith": "audio/"
                    }
                },
                order={"createdAt": "desc"},
                take=limit
            )

            # Filtrer les fichiers existants
            audio_paths = []
            for att in attachments:
                if att.filePath and att.filePath not in exclude and os.path.exists(att.filePath):
                    audio_paths.append(att.filePath)

            logger.info(f"[VOICE_CLONE] ğŸ“š {len(audio_paths)} audios historiques trouvÃ©s pour {user_id}")
            return audio_paths

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur rÃ©cupÃ©ration historique: {e}")
            return []

    async def _get_best_audio_for_cloning(
        self,
        user_id: str,
        limit: int = 10
    ) -> Optional[AudioQualityMetadata]:
        """
        SÃ©lectionne le meilleur audio pour le clonage vocal.
        CritÃ¨res (par ordre de prioritÃ©):
        1. Le plus long
        2. Le plus clair (sans bruit)
        3. Sans autres locuteurs
        4. Le plus rÃ©cent

        Returns:
            AudioQualityMetadata du meilleur audio, ou None si aucun audio trouvÃ©
        """
        if not self.database_service:
            logger.warning("[VOICE_CLONE] Database service non disponible")
            return None

        try:
            # RequÃªte pour rÃ©cupÃ©rer les audios avec mÃ©tadonnÃ©es de qualitÃ©
            attachments = await self.database_service.prisma.messageattachment.find_many(
                where={
                    "message": {
                        "senderId": user_id
                    },
                    "mimeType": {
                        "startswith": "audio/"
                    }
                },
                order={"createdAt": "desc"},
                take=limit
            )

            if not attachments:
                return None

            # Convertir en AudioQualityMetadata et calculer les scores
            quality_audios: List[AudioQualityMetadata] = []
            for att in attachments:
                if att.filePath and os.path.exists(att.filePath):
                    duration_ms = await self._get_audio_duration_ms(att.filePath)

                    # Extraire les mÃ©tadonnÃ©es de qualitÃ© si disponibles
                    # Ces champs doivent Ãªtre ajoutÃ©s au schÃ©ma Prisma de MessageAttachment
                    noise_level = getattr(att, 'noiseLevel', 0.0) or 0.0
                    clarity_score = getattr(att, 'clarityScore', 1.0) or 1.0
                    has_other_speakers = getattr(att, 'hasOtherSpeakers', False) or False

                    audio_meta = AudioQualityMetadata(
                        attachment_id=att.id,
                        file_path=att.filePath,
                        duration_ms=duration_ms,
                        noise_level=noise_level,
                        clarity_score=clarity_score,
                        has_other_speakers=has_other_speakers,
                        created_at=att.createdAt if hasattr(att, 'createdAt') else datetime.now()
                    )
                    audio_meta.calculate_overall_score()
                    quality_audios.append(audio_meta)

            if not quality_audios:
                return None

            # Trier par score dÃ©croissant et retourner le meilleur
            quality_audios.sort(key=lambda x: x.overall_score, reverse=True)
            best_audio = quality_audios[0]

            logger.info(
                f"[VOICE_CLONE] ğŸ¯ Meilleur audio sÃ©lectionnÃ© pour {user_id}: "
                f"id={best_audio.attachment_id}, duration={best_audio.duration_ms}ms, "
                f"score={best_audio.overall_score:.2f}"
            )
            return best_audio

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur sÃ©lection meilleur audio: {e}")
            return None

    async def _calculate_total_duration(self, audio_paths: List[str]) -> int:
        """Calcule la durÃ©e totale de plusieurs fichiers audio"""
        total = 0
        for path in audio_paths:
            duration = await self._get_audio_duration_ms(path)
            total += duration
        return total

    async def _get_audio_duration_ms(self, audio_path: str) -> int:
        """RÃ©cupÃ¨re la durÃ©e d'un fichier audio en millisecondes.

        Utilise librosa en premier, puis pydub comme fallback pour les formats
        non supportÃ©s par soundfile (ex: webm, mp4).
        """
        if not AUDIO_PROCESSING_AVAILABLE:
            return 0

        loop = asyncio.get_event_loop()

        # Essayer d'abord avec librosa
        try:
            import librosa
            duration = await loop.run_in_executor(
                None,
                lambda: librosa.get_duration(path=audio_path)
            )
            if duration > 0:
                return int(duration * 1000)
        except Exception as e:
            logger.debug(f"[VOICE_CLONE] librosa n'a pas pu lire {audio_path}: {e}")

        # Fallback avec pydub (supporte plus de formats via ffmpeg)
        try:
            def get_duration_with_pydub():
                audio = AudioSegment.from_file(audio_path)
                return len(audio)  # pydub retourne dÃ©jÃ  en ms

            duration_ms = await loop.run_in_executor(None, get_duration_with_pydub)
            logger.debug(f"[VOICE_CLONE] DurÃ©e obtenue via pydub: {duration_ms}ms")
            return duration_ms
        except Exception as e:
            logger.warning(f"[VOICE_CLONE] Impossible de lire la durÃ©e de {audio_path}: {e}")
            return 0

    def _calculate_quality_score(self, duration_ms: int, audio_count: int) -> float:
        """
        Calcule un score de qualitÃ© basÃ© sur la durÃ©e et le nombre d'audios.

        - 0-10s: 0.3 (faible)
        - 10-30s: 0.5 (moyen)
        - 30-60s: 0.7 (bon)
        - 60s+: 0.9 (excellent)
        - Bonus: +0.05 par audio supplÃ©mentaire (max +0.1)
        """
        if duration_ms < 10_000:
            base_score = 0.3
        elif duration_ms < 30_000:
            base_score = 0.5
        elif duration_ms < 60_000:
            base_score = 0.7
        else:
            base_score = 0.9

        audio_bonus = min(0.1, (audio_count - 1) * 0.05)
        return min(1.0, base_score + audio_bonus)

    async def _load_cached_model(self, user_id: str) -> Optional[VoiceModel]:
        """
        Charge un modele vocal depuis le cache Redis.

        Architecture: Redis est utilisÃ© comme cache, Gateway gÃ¨re la persistance MongoDB.
        """
        try:
            audio_cache = self._get_audio_cache()
            cached_profile = await audio_cache.get_voice_profile(user_id)

            if cached_profile:
                model = self._cache_profile_to_voice_model(cached_profile)
                logger.debug(f"[VOICE_CLONE] Modele charge depuis cache Redis: {user_id}")
                return model
        except Exception as e:
            logger.warning(f"[VOICE_CLONE] Erreur lecture cache Redis pour {user_id}: {e}")

        return None

    def _db_profile_to_voice_model(self, db_profile: Dict[str, Any]) -> VoiceModel:
        """Convertit un profil MongoDB en VoiceModel"""
        model = VoiceModel(
            user_id=db_profile["userId"],
            embedding_path="",
            audio_count=db_profile.get("audioCount", 1),
            total_duration_ms=db_profile.get("totalDurationMs", 0),
            quality_score=db_profile.get("qualityScore", 0.5),
            profile_id=db_profile.get("profileId", ""),
            version=db_profile.get("version", 1),
            source_audio_id="",
            created_at=datetime.fromisoformat(db_profile["createdAt"]) if db_profile.get("createdAt") else datetime.now(),
            updated_at=datetime.fromisoformat(db_profile["updatedAt"]) if db_profile.get("updatedAt") else datetime.now(),
            next_recalibration_at=datetime.fromisoformat(db_profile["nextRecalibrationAt"]) if db_profile.get("nextRecalibrationAt") else None
        )

        if db_profile.get("voiceCharacteristics"):
            vc_data = db_profile["voiceCharacteristics"]
            model.voice_characteristics = VoiceCharacteristics(
                pitch_mean_hz=vc_data.get("pitch", {}).get("mean_hz", 0),
                pitch_std_hz=vc_data.get("pitch", {}).get("std_hz", 0),
                pitch_min_hz=vc_data.get("pitch", {}).get("min_hz", 0),
                pitch_max_hz=vc_data.get("pitch", {}).get("max_hz", 0),
                voice_type=vc_data.get("classification", {}).get("voice_type", "unknown"),
                estimated_gender=vc_data.get("classification", {}).get("estimated_gender", "unknown"),
                estimated_age_range=vc_data.get("classification", {}).get("estimated_age_range", "unknown"),
                brightness=vc_data.get("spectral", {}).get("brightness", 0),
                warmth=vc_data.get("spectral", {}).get("warmth", 0),
                breathiness=vc_data.get("spectral", {}).get("breathiness", 0),
                nasality=vc_data.get("spectral", {}).get("nasality", 0),
                speech_rate_wpm=vc_data.get("prosody", {}).get("speech_rate_wpm", 0),
                energy_mean=vc_data.get("prosody", {}).get("energy_mean", 0),
                energy_std=vc_data.get("prosody", {}).get("energy_std", 0),
                silence_ratio=vc_data.get("prosody", {}).get("silence_ratio", 0),
            )

        if db_profile.get("fingerprint"):
            model.fingerprint = VoiceFingerprint.from_dict(db_profile["fingerprint"])

        return model

    def _cache_profile_to_voice_model(self, cached_profile: Dict[str, Any]) -> VoiceModel:
        """Convertit un profil du cache Redis en VoiceModel"""
        model = VoiceModel(
            user_id=cached_profile["userId"],
            embedding_path="",
            audio_count=cached_profile.get("audioCount", 1),
            total_duration_ms=cached_profile.get("totalDurationMs", 0),
            quality_score=cached_profile.get("qualityScore", 0.5),
            profile_id=cached_profile.get("profileId", ""),
            version=cached_profile.get("version", 1),
            source_audio_id="",
            created_at=datetime.fromisoformat(cached_profile["createdAt"]) if cached_profile.get("createdAt") else datetime.now(),
            updated_at=datetime.fromisoformat(cached_profile["updatedAt"]) if cached_profile.get("updatedAt") else datetime.now(),
            next_recalibration_at=datetime.fromisoformat(cached_profile["nextRecalibrationAt"]) if cached_profile.get("nextRecalibrationAt") else None
        )

        # Charger l'embedding encodÃ© en base64
        if cached_profile.get("embeddingBase64"):
            try:
                embedding_bytes = base64.b64decode(cached_profile["embeddingBase64"])
                model.embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
            except Exception as e:
                logger.warning(f"[VOICE_CLONE] Erreur dÃ©codage embedding base64: {e}")

        if cached_profile.get("voiceCharacteristics"):
            vc_data = cached_profile["voiceCharacteristics"]
            model.voice_characteristics = VoiceCharacteristics(
                pitch_mean_hz=vc_data.get("pitch", {}).get("mean_hz", 0),
                pitch_std_hz=vc_data.get("pitch", {}).get("std_hz", 0),
                pitch_min_hz=vc_data.get("pitch", {}).get("min_hz", 0),
                pitch_max_hz=vc_data.get("pitch", {}).get("max_hz", 0),
                voice_type=vc_data.get("classification", {}).get("voice_type", "unknown"),
                estimated_gender=vc_data.get("classification", {}).get("estimated_gender", "unknown"),
                estimated_age_range=vc_data.get("classification", {}).get("estimated_age_range", "unknown"),
                brightness=vc_data.get("spectral", {}).get("brightness", 0),
                warmth=vc_data.get("spectral", {}).get("warmth", 0),
                breathiness=vc_data.get("spectral", {}).get("breathiness", 0),
                nasality=vc_data.get("spectral", {}).get("nasality", 0),
                speech_rate_wpm=vc_data.get("prosody", {}).get("speech_rate_wpm", 0),
                energy_mean=vc_data.get("prosody", {}).get("energy_mean", 0),
                energy_std=vc_data.get("prosody", {}).get("energy_std", 0),
                silence_ratio=vc_data.get("prosody", {}).get("silence_ratio", 0),
            )

        if cached_profile.get("fingerprint"):
            model.fingerprint = VoiceFingerprint.from_dict(cached_profile["fingerprint"])

        return model

    async def _load_embedding(self, model: VoiceModel) -> VoiceModel:
        """
        Charge l'embedding d'un modele depuis le cache Redis.

        L'embedding est stockÃ© encodÃ© en base64 dans le cache Redis.
        Architecture: Redis = cache, Gateway = persistance MongoDB.
        """
        # L'embedding est dÃ©jÃ  chargÃ© par _cache_profile_to_voice_model si disponible
        if model.embedding is not None and len(model.embedding) > 0:
            return model

        # Fallback: essayer de recharger depuis le cache
        try:
            audio_cache = self._get_audio_cache()
            cached_profile = await audio_cache.get_voice_profile(model.user_id)
            if cached_profile and cached_profile.get("embeddingBase64"):
                embedding_bytes = base64.b64decode(cached_profile["embeddingBase64"])
                model.embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
                logger.debug(f"[VOICE_CLONE] Embedding chargÃ© depuis cache Redis: {model.user_id}")
                return model
        except Exception as e:
            logger.warning(f"[VOICE_CLONE] Erreur lecture embedding depuis cache Redis: {e}")

        # Default: embedding vide
        model.embedding = np.zeros(256, dtype=np.float32)
        return model

    async def _save_model_to_cache(self, model: VoiceModel):
        """
        Sauvegarde un modele vocal dans le cache Redis.

        Stocke l'embedding encodÃ© en base64 + metadonnees JSON.

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ARCHITECTURE:
        Redis = cache pour accÃ¨s rapide aux profils vocaux
        Gateway = responsable de la persistance MongoDB
        Le Translator met en cache pour rÃ©utiliser les embeddings existants.
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        try:
            audio_cache = self._get_audio_cache()

            # Encoder l'embedding en base64 pour stockage JSON
            embedding_b64 = None
            if model.embedding is not None:
                embedding_bytes = model.embedding.astype(np.float32).tobytes()
                embedding_b64 = base64.b64encode(embedding_bytes).decode('utf-8')

            voice_chars_dict = model.voice_characteristics.to_dict() if model.voice_characteristics else None
            fingerprint_dict = model.fingerprint.to_dict() if model.fingerprint else None

            cache_profile = {
                "userId": model.user_id,
                "profileId": model.profile_id or "",
                "embeddingBase64": embedding_b64,
                "embeddingModel": "openvoice_v2",
                "embeddingDimension": len(model.embedding) if model.embedding is not None else 256,
                "audioCount": model.audio_count,
                "totalDurationMs": model.total_duration_ms,
                "qualityScore": model.quality_score,
                "version": model.version,
                "voiceCharacteristics": voice_chars_dict,
                "fingerprint": fingerprint_dict,
                "signatureShort": model.fingerprint.signature_short if model.fingerprint else None,
                "createdAt": model.created_at.isoformat() if model.created_at else datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat(),
                "nextRecalibrationAt": model.next_recalibration_at.isoformat() if model.next_recalibration_at else None,
            }

            await audio_cache.set_voice_profile(model.user_id, cache_profile)
            logger.info(f"[VOICE_CLONE] Modele sauvegarde dans cache Redis: {model.user_id}")

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur sauvegarde cache Redis: {e}")

    async def schedule_quarterly_recalibration(self):
        """
        TÃ¢che planifiÃ©e pour recalibrer les modÃ¨les de voix trimestriellement (tous les 3 mois).
        Ã€ exÃ©cuter via un cron job ou un scheduler.
        SÃ©lectionne le meilleur audio: le plus long, le plus clair, sans bruit, le plus rÃ©cent.
        """
        logger.info("[VOICE_CLONE] ğŸ”„ DÃ©marrage recalibration trimestrielle...")

        # Lister tous les modÃ¨les en cache
        all_models = await self._list_all_cached_models()

        recalibrated = 0
        for model in all_models:
            if model.next_recalibration_at and datetime.now() >= model.next_recalibration_at:
                logger.info(f"[VOICE_CLONE] ğŸ”„ Recalibration pour {model.user_id}")

                # SÃ©lectionner le meilleur audio basÃ© sur les critÃ¨res de qualitÃ©
                best_audio = await self._get_best_audio_for_cloning(model.user_id)

                if best_audio:
                    # Utiliser le meilleur audio pour rÃ©gÃ©nÃ©rer le modÃ¨le
                    await self._create_voice_model(
                        model.user_id,
                        [best_audio.file_path],
                        best_audio.duration_ms
                    )
                    recalibrated += 1
                    logger.info(
                        f"[VOICE_CLONE] âœ… ModÃ¨le recalibrÃ© pour {model.user_id} "
                        f"avec audio {best_audio.attachment_id} (score: {best_audio.overall_score:.2f})"
                    )
                else:
                    # Fallback: utiliser l'historique audio classique
                    recent_audios = await self._get_user_audio_history(model.user_id)
                    if recent_audios:
                        total_duration = await self._calculate_total_duration(recent_audios)
                        await self._create_voice_model(
                            model.user_id,
                            recent_audios,
                            total_duration
                        )
                        recalibrated += 1

        logger.info(f"[VOICE_CLONE] âœ… Recalibration trimestrielle terminÃ©e: {recalibrated} modÃ¨les mis Ã  jour")

    async def _list_all_cached_models(self) -> List[VoiceModel]:
        """
        Liste tous les modeles vocaux depuis le cache Redis.

        Note: Cette methode ne charge pas les embeddings pour des raisons de performance.
        Utiliser _load_embedding() si l'embedding est necessaire.
        """
        models = []

        try:
            audio_cache = self._get_audio_cache()
            # Lister toutes les clÃ©s de profils vocaux
            profile_keys = await audio_cache.redis.keys("voice:profile:*")

            for key in profile_keys:
                try:
                    data = await audio_cache.redis.get(key)
                    if data:
                        import json
                        cached_profile = json.loads(data)
                        model = self._cache_profile_to_voice_model(cached_profile)
                        models.append(model)
                except Exception as e:
                    logger.warning(f"[VOICE_CLONE] Erreur lecture profil {key}: {e}")

        except Exception as e:
            logger.error(f"[VOICE_CLONE] Erreur listing modeles Redis: {e}")

        return models

    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du service"""
        models_count = 0
        cache_available = False

        try:
            audio_cache = self._get_audio_cache()
            cache_stats = audio_cache.get_stats()
            cache_available = cache_stats.get("redis_available", False) or cache_stats.get("memory_entries", 0) > 0

            # Compter les modÃ¨les en cache
            profile_keys = await audio_cache.redis.keys("voice:profile:*")
            models_count = len(profile_keys)
        except Exception as e:
            logger.warning(f"[VOICE_CLONE] Erreur comptage modeles: {e}")

        return {
            "service": "VoiceCloneService",
            "initialized": self.is_initialized,
            "openvoice_available": OPENVOICE_AVAILABLE,
            "audio_processing_available": AUDIO_PROCESSING_AVAILABLE,
            "storage": "Redis",
            "device": self.device,
            "voice_models_count": models_count,
            "min_audio_duration_ms": self.MIN_AUDIO_DURATION_MS,
            "max_age_days": self.VOICE_MODEL_MAX_AGE_DAYS,
            "cache_available": cache_available,
        }

    # =========================================================================
    # TRADUCTION MULTI-VOIX
    # =========================================================================

    async def prepare_multi_speaker_translation(
        self,
        audio_path: str,
        user_id: str,
        temp_dir: str
    ) -> MultiSpeakerTranslationContext:
        """
        PrÃ©pare le contexte pour une traduction audio multi-locuteurs.

        Cette mÃ©thode:
        1. Analyse l'audio pour dÃ©tecter tous les locuteurs
        2. Extrait l'audio de chaque locuteur sÃ©parÃ©ment
        3. CrÃ©e des profils temporaires (non cachÃ©s)
        4. Si l'utilisateur a un profil existant, identifie sa voix

        Args:
            audio_path: Chemin vers l'audio source
            user_id: ID de l'utilisateur Ã©metteur
            temp_dir: RÃ©pertoire pour les fichiers temporaires

        Returns:
            MultiSpeakerTranslationContext avec tous les profils prÃªts
        """
        logger.info(f"[VOICE_CLONE] ğŸ­ PrÃ©paration traduction multi-voix: {audio_path}")

        voice_analyzer = get_voice_analyzer()

        # 1. Extraire l'audio de chaque locuteur
        speakers_audio = await voice_analyzer.extract_all_speakers_audio(
            audio_path,
            temp_dir,
            min_segment_duration_ms=100
        )

        if not speakers_audio:
            raise ValueError("Aucun locuteur dÃ©tectÃ© dans l'audio")

        # 2. RÃ©cupÃ©rer le profil utilisateur existant (si disponible)
        user_model = await self._load_cached_model(user_id)
        user_fingerprint = user_model.fingerprint if user_model else None

        # 3. CrÃ©er les profils temporaires
        profiles: List[TemporaryVoiceProfile] = []
        user_profile: Optional[TemporaryVoiceProfile] = None

        # RÃ©cupÃ©rer la durÃ©e totale
        total_duration_ms = await self._get_audio_duration_ms(audio_path)

        for speaker_id, (speaker_audio_path, speaker_info) in speakers_audio.items():
            # Extraire l'embedding temporaire
            temp_embedding = await self._extract_voice_embedding(
                speaker_audio_path,
                Path(temp_dir)
            )

            profile = TemporaryVoiceProfile(
                speaker_id=speaker_id,
                speaker_info=speaker_info,
                audio_path=speaker_audio_path,
                embedding=temp_embedding,
                original_segments=speaker_info.segments
            )

            # VÃ©rifier si ce locuteur correspond Ã  l'utilisateur
            if user_fingerprint and speaker_info.fingerprint:
                similarity = user_fingerprint.similarity_score(speaker_info.fingerprint)
                if similarity >= 0.75:
                    profile.matched_user_id = user_id
                    profile.is_user_match = True
                    user_profile = profile
                    logger.info(
                        f"[VOICE_CLONE] ğŸ¯ Utilisateur {user_id} identifiÃ©: "
                        f"{speaker_id} (similaritÃ©: {similarity:.0%})"
                    )

            profiles.append(profile)

        # 4. CrÃ©er le contexte
        context = MultiSpeakerTranslationContext(
            source_audio_path=audio_path,
            source_duration_ms=total_duration_ms,
            speaker_count=len(profiles),
            profiles=profiles,
            user_profile=user_profile
        )

        logger.info(
            f"[VOICE_CLONE] âœ… Contexte multi-voix prÃªt: "
            f"{len(profiles)} locuteurs, utilisateur identifiÃ©: {user_profile is not None}"
        )

        return context

    async def should_update_user_profile(
        self,
        user_id: str,
        audio_path: str
    ) -> Tuple[bool, str]:
        """
        DÃ©termine si le profil utilisateur doit Ãªtre mis Ã  jour avec cet audio.

        RÃ¨gles:
        - CrÃ©ation: Un seul locuteur principal (>70% du temps de parole)
        - Mise Ã  jour: Signature vocale doit correspondre au profil existant (>80%)

        Args:
            user_id: ID de l'utilisateur
            audio_path: Chemin vers l'audio

        Returns:
            Tuple[bool, str]: (doit mettre Ã  jour, raison)
        """
        voice_analyzer = get_voice_analyzer()

        # Analyser l'audio
        metadata = await voice_analyzer.analyze_audio(audio_path)

        # Charger le profil existant
        existing_model = await self._load_cached_model(user_id)

        if existing_model and existing_model.fingerprint:
            # VÃ©rifier si on peut METTRE Ã€ JOUR
            can_update, reason, _ = voice_analyzer.can_update_user_profile(
                metadata,
                existing_model.fingerprint,
                similarity_threshold=0.80
            )
            if can_update:
                return True, f"Mise Ã  jour possible: {reason}"
            else:
                return False, f"Mise Ã  jour impossible: {reason}"
        else:
            # VÃ©rifier si on peut CRÃ‰ER
            can_create, reason = voice_analyzer.can_create_user_profile(metadata)
            if can_create:
                return True, f"CrÃ©ation possible: {reason}"
            else:
                return False, f"CrÃ©ation impossible: {reason}"

    async def cleanup_temp_profiles(self, context: MultiSpeakerTranslationContext):
        """
        Nettoie les fichiers temporaires d'une traduction multi-voix.

        Args:
            context: Contexte de traduction Ã  nettoyer
        """
        for profile in context.profiles:
            try:
                if os.path.exists(profile.audio_path):
                    os.remove(profile.audio_path)
                    logger.debug(f"[VOICE_CLONE] Nettoyage: {profile.audio_path}")
            except Exception as e:
                logger.warning(f"[VOICE_CLONE] Erreur nettoyage {profile.audio_path}: {e}")

    async def close(self):
        """LibÃ¨re les ressources"""
        logger.info("[VOICE_CLONE] ğŸ›‘ Fermeture du service")
        self.tone_color_converter = None
        self.se_extractor_module = None
        self.is_initialized = False


# Fonction helper pour obtenir l'instance singleton
def get_voice_clone_service() -> VoiceCloneService:
    """Retourne l'instance singleton du service de clonage vocal"""
    return VoiceCloneService()
