# ═══════════════════════════════════════════════════════════════════════════
# MEESHY TRANSLATOR SERVICE - Environment Configuration
# Copy this file to .env and adjust values for your environment
# ═══════════════════════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════════════════════
# PYTORCH BACKEND CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
#
# Installation commands (using uv):
# ---------------------------------
# CPU (default, ~2GB, no CUDA required):
#   uv sync --extra cpu --extra dev
#
# GPU CUDA 12.4 (recommended for new NVIDIA drivers, ~8GB):
#   uv sync --extra gpu --extra dev
#
# GPU CUDA 12.1 (older drivers):
#   uv sync --extra gpu-cu121 --extra dev
#
# GPU CUDA 11.8 (legacy):
#   uv sync --extra gpu-cu118 --extra dev
#
# Docker build:
# -------------
# CPU:  docker build --build-arg TORCH_BACKEND=cpu -t meeshy-translator:cpu .
# GPU:  docker build --build-arg TORCH_BACKEND=gpu -t meeshy-translator:gpu .
#
# Makefile shortcuts:
# -------------------
#   make uv-sync-cpu           # Install CPU version
#   make uv-sync-gpu           # Install GPU CUDA 12.4
#   make build-translator-cpu  # Build CPU Docker image
#   make build-translator-gpu  # Build GPU Docker image
#   make uv-info               # Show PyTorch installation info
#
# ═══════════════════════════════════════════════════════════════════════════

# Backend selection (cpu, gpu, gpu-cu121, gpu-cu118)
# This is used by Makefile and Docker, not directly by the app
TORCH_BACKEND=cpu

# ─────────────────────────────────────────────────────────────────────────────
# GENERAL CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────
DEBUG=false
NODE_ENV=production
WORKERS=16

# ─────────────────────────────────────────────────────────────────────────────
# SERVER PORTS
# ─────────────────────────────────────────────────────────────────────────────
FASTAPI_PORT=8000
GRPC_PORT=50051
ZMQ_PORT=5555

# ─────────────────────────────────────────────────────────────────────────────
# DATABASE (MongoDB)
# ─────────────────────────────────────────────────────────────────────────────
# For local development:
DATABASE_URL=mongodb://localhost:27017/meeshy?directConnection=true
# For Docker (with replica set):
# DATABASE_URL=mongodb://database:27017/meeshy?replicaSet=rs0&directConnection=true
PRISMA_POOL_SIZE=15

# ─────────────────────────────────────────────────────────────────────────────
# REDIS CACHE
# ─────────────────────────────────────────────────────────────────────────────
REDIS_URL=redis://localhost:6379
TRANSLATION_CACHE_TTL=3600
CACHE_MAX_ENTRIES=10000

# ─────────────────────────────────────────════════════════════════════════════
# AUDIO SERVICES
# ─────────────────────────────────────────────────────────────────────────────
ENABLE_AUDIO_SERVICES=true
ENABLE_VOICE_API=true

# ─────────────────────────────────────────────────────────────────────────────
# WHISPER (Transcription)
# ─────────────────────────────────────────────────────────────────────────────
WHISPER_MODEL=base
WHISPER_DEVICE=auto
WHISPER_COMPUTE_TYPE=float16

# ═══════════════════════════════════════════════════════════════════════════
# TTS MODEL SELECTION
# ═══════════════════════════════════════════════════════════════════════════
#
# Available models:
#
# 1. CHATTERBOX (RECOMMENDED - Apache 2.0 License)
#    - Best quality/speed balance
#    - Full commercial use allowed
#    - 17 languages supported
#    TTS_MODEL=chatterbox
#
# 2. CHATTERBOX-TURBO (Apache 2.0 License)
#    - Faster than standard Chatterbox
#    - 350M parameters (lighter)
#    - Full commercial use allowed
#    TTS_MODEL=chatterbox-turbo
#
# 3. HIGGS-AUDIO-V2 (State-of-the-art quality)
#    ⚠️ WARNING: Commercial use LIMITED to < 100,000 users/year
#    - 50+ languages supported
#    - Best quality available
#    - Requires commercial license for > 100k users
#    - Contact: https://www.boson.ai/contact
#    TTS_MODEL=higgs-audio-v2
#
# 4. XTTS-V2 (Legacy - NON-COMMERCIAL)
#    ⚠️ WARNING: NO commercial use allowed!
#    - Coqui Public Model License
#    - Personal/research use only
#    - Company shut down in 2024
#    TTS_MODEL=xtts-v2
#
# ═══════════════════════════════════════════════════════════════════════════

TTS_MODEL=chatterbox
TTS_DEVICE=auto
TTS_OUTPUT_DIR=./audio_output
TTS_DEFAULT_FORMAT=mp3

# Chatterbox specific settings
CHATTERBOX_EXAGGERATION=0.5
CHATTERBOX_CFG_WEIGHT=0.5

# Legacy XTTS settings (only if using xtts-v2)
# Note: XTTS models are stored in MODELS_PATH/xtts (configured automatically)
XTTS_DEVICE=auto

# ─────────────────────────────────────────────────────────────────────────────
# VOICE CLONING (OpenVoice V2)
# ─────────────────────────────────────────────────────────────────────────────
# Note: OpenVoice checkpoints are stored in MODELS_PATH/openvoice (configured automatically)
VOICE_MODEL_CACHE_DIR=./voice_models
VOICE_CLONE_DEVICE=cpu
# Voice profile cache TTL in seconds (default: 90 days)
VOICE_PROFILE_CACHE_TTL=7776000

# ─────────────────────────────────────────────────────────────────────────────
# AUDIO OUTPUT
# ─────────────────────────────────────────────────────────────────────────────
AUDIO_OUTPUT_DIR=./audio_output
ANALYTICS_DATA_DIR=./analytics_data
MAX_CONCURRENT_JOBS=10

# ═══════════════════════════════════════════════════════════════════════════
# ML MODELS - CENTRALIZED STORAGE
# ═══════════════════════════════════════════════════════════════════════════
#
# All models are stored under MODELS_PATH with the following structure:
#
#   models/
#   ├── huggingface/     # Chatterbox, Higgs Audio, NLLB translation models
#   │                    # Auto-downloaded via HuggingFace Hub
#   ├── openvoice/       # OpenVoice V2 checkpoints for voice cloning
#   │                    # Download: https://github.com/myshell-ai/OpenVoice
#   ├── xtts/            # XTTS v2 models (legacy, non-commercial)
#   │                    # Auto-downloaded via TTS library
#   └── whisper/         # Whisper STT models
#                        # Auto-downloaded via faster-whisper
#
# The paths are computed automatically from MODELS_PATH:
#   - huggingface_cache_path = MODELS_PATH/huggingface
#   - openvoice_checkpoints_path = MODELS_PATH/openvoice
#   - xtts_models_path = MODELS_PATH/xtts
#   - whisper_models_path = MODELS_PATH/whisper
#
# ═══════════════════════════════════════════════════════════════════════════

MODELS_PATH=./models

# ═══════════════════════════════════════════════════════════════════════════
# TRANSLATION MODELS - NLLB Only
# ═══════════════════════════════════════════════════════════════════════════
#
# Two models available:
#   - BASIC (NLLB 600M): Fast, good quality, ~2.4GB
#   - PREMIUM (NLLB 1.3B): Best quality, slower, ~5.2GB
#
# Both support 200+ languages with same API.
# Models are downloaded to MODELS_PATH/huggingface on first use.
#
# ═══════════════════════════════════════════════════════════════════════════

BASIC_MODEL=facebook/nllb-200-distilled-600M
PREMIUM_MODEL=facebook/nllb-200-distilled-1.3B

# ─────────────────────────────────────────────────────────────────────────────
# ML PERFORMANCE
# ─────────────────────────────────────────────────────────────────────────────
ML_BATCH_SIZE=16
GPU_MEMORY_FRACTION=0.8
QUANTIZATION_LEVEL=float16
TRANSLATION_TIMEOUT=20
MAX_TEXT_LENGTH=100000
CONCURRENT_TRANSLATIONS=4

# ─────────────────────────────────────────────────────────────────────────────
# LANGUAGE SETTINGS
# ─────────────────────────────────────────────────────────────────────────────
DEFAULT_LANGUAGE=fr
AUTO_DETECT_LANGUAGE=true
SUPPORTED_LANGUAGES=af,ar,bg,bn,cs,da,de,el,en,es,fa,fi,fr,he,hi,hr,hu,hy,id,ig,it,ja,ko,ln,lt,ms,nl,no,pl,pt,ro,ru,sv,sw,th,tr,uk,ur,vi,zh

# ─────────────────────────────────────────────────────────────────────────────
# HUGGINGFACE CACHE
# ─────────────────────────────────────────────────────────────────────────────
# These should point to MODELS_PATH/huggingface for consistent storage
HF_HOME=./models/huggingface
TRANSFORMERS_CACHE=./models/huggingface
HUGGINGFACE_HUB_CACHE=./models/huggingface

# ─────────────────────────────────────────────────────────────────────────────
# SPEAKER DIARIZATION (Identification des locuteurs)
# ─────────────────────────────────────────────────────────────────────────────
# Active la détection et l'identification des locuteurs dans les audios
# Permet de savoir qui parle et d'afficher les segments avec des couleurs par locuteur
ENABLE_DIARIZATION=true

# Token HuggingFace pour pyannote.audio (OPTIONNEL mais RECOMMANDÉ)
# Fournit la meilleure précision pour la diarisation
#
# Pour obtenir un token:
# 1. Créer un compte sur https://huggingface.co/
# 2. Aller dans Settings > Access Tokens
# 3. Créer un nouveau token (READ access suffisant)
# 4. Accepter les conditions d'utilisation de pyannote/speaker-diarization-3.1
#    https://huggingface.co/pyannote/speaker-diarization-3.1
#
# Si absent, le service utilisera le fallback pitch clustering (moins précis)
HF_TOKEN=
