# Parall√©lisation GPU - ThreadPoolExecutor vs asyncio.gather

## Probl√®me R√©solu

### AVANT (asyncio.gather - FAUX parall√©lisme)
```python
# audio_message_pipeline.py - S√âQUENTIEL!
results = await asyncio.gather(
    *[process_single_language(lang, cloning_params) for lang in languages],
    return_exceptions=True
)
# ‚Üí Utilise une SEULE event loop
# ‚Üí Les op√©rations GPU s'ex√©cutent S√âQUENTIELLEMENT
# ‚Üí Temps total = somme des temps individuels
```

**Probl√®me**: `asyncio.gather` utilise une seule event loop. M√™me si les coroutines sont lanc√©es "en parall√®le", les op√©rations GPU (TTS, clonage vocal) sont thread-safe mais pas async-safe. R√©sultat: ex√©cution S√âQUENTIELLE.

### APR√àS (ThreadPoolExecutor - VRAIE parall√©lisation)
```python
# Chaque thread a sa propre event loop ‚Üí vraie parall√©lisation GPU
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    futures = {executor.submit(process_language_sync, task): task[0]
               for task in tasks}

    for future in as_completed(futures):
        lang = futures[future]
        result = future.result()
```

**Solution**: ThreadPoolExecutor cr√©e un thread par langue. Chaque thread a sa propre event loop asyncio. Les op√©rations GPU peuvent s'ex√©cuter VRAIMENT en parall√®le.

## Architecture

### Pattern iOS Script (lignes 866-903)
```python
def process_language(args: Tuple) -> Dict:
    """Fonction SYNCHRONE pour ThreadPoolExecutor"""
    (reference_path, target_lang, translated_text, output_path,
     config, cloner) = args

    # Clone voice (thread-safe)
    cloner.clone(...)

    # Analyze similarity
    comparison = VoiceAnalyzer.compare(original, cloned)

    return {'success': True, 'lang': target_lang, ...}

# Parallel execution
with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
    futures = {executor.submit(process_language, task): task[1]
               for task in tasks}

    for future in as_completed(futures):
        result = future.result()
```

### Impl√©mentation Pipeline Audio

```python
def process_language_sync(task_args: Tuple) -> Tuple:
    """Fonction SYNCHRONE qui cr√©e sa propre event loop"""
    target_lang, lang_cloning_params = task_args

    # Cr√©er une nouvelle boucle d'√©v√©nements pour ce thread
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        # Ex√©cuter le traitement asynchrone dans cette boucle
        result = loop.run_until_complete(
            self._process_single_language_async(
                target_lang=target_lang,
                # ... autres param√®tres
            )
        )
        return result
    finally:
        loop.close()

async def _process_single_language_async(self, ...):
    """Logique asynchrone (traduction + TTS + cache)"""
    # 1. Traduire le texte
    translated_text = await self._translate_text_with_cache(...)

    # 2. G√©n√©rer audio avec voix clon√©e (GPU)
    tts_result = await self.tts_service.synthesize_with_voice(...)

    # 3. Mettre en cache
    await self.audio_cache.set_translated_audio_by_hash(...)

    return (target_lang, TranslatedAudioVersion(...))
```

## Configuration

### Variable d'Environnement
```bash
# Nombre de workers parall√®les (d√©faut: min(nb_langues, 4))
export TTS_MAX_WORKERS=4
```

### Calcul Automatique
```python
max_workers = min(len(languages_to_process), int(os.getenv("TTS_MAX_WORKERS", "4")))
```

- **2 langues** ‚Üí 2 workers (parall√©lisation compl√®te)
- **3 langues** ‚Üí 3 workers (parall√©lisation compl√®te)
- **5 langues** ‚Üí 4 workers (limite pour √©viter surcharge GPU)

## Performance

### Sc√©nario: Traduction en 3 langues (fr, es, de)

#### AVANT (asyncio.gather)
```
[PIPELINE] üîÑ Traitement PARALL√àLE de 3 langues: ['fr', 'es', 'de']
[TTS] Synth√®se fr: 2500ms
[TTS] Synth√®se es: 2300ms  # Attend que fr finisse
[TTS] Synth√®se de: 2400ms  # Attend que es finisse
[PIPELINE] ‚ö° 3 langues trait√©es en 7200ms
```
**Temps total**: ~7200ms (s√©quentiel)

#### APR√àS (ThreadPoolExecutor)
```
[PIPELINE] üîÑ Traitement PARALL√àLE de 3 langues: ['fr', 'es', 'de']
[PIPELINE] üîß ThreadPoolExecutor: 3 workers pour 3 langues
[TTS] Synth√®se fr: 2500ms  # Thread 1
[TTS] Synth√®se es: 2300ms  # Thread 2 (en parall√®le)
[TTS] Synth√®se de: 2400ms  # Thread 3 (en parall√®le)
[PIPELINE] ‚ö° Progression: 1/3 langues compl√©t√©es (es)
[PIPELINE] ‚ö° Progression: 2/3 langues compl√©t√©es (de)
[PIPELINE] ‚ö° Progression: 3/3 langues compl√©t√©es (fr)
[PIPELINE] ‚úÖ 3/3 langues trait√©es avec succ√®s en 2500ms (parall√©lisation r√©elle)
```
**Temps total**: ~2500ms (parall√®le - temps de la plus longue)

### Gain de Performance
- **2-3 langues**: **2-3x plus rapide**
- **4+ langues**: **3-4x plus rapide** (limit√© par max_workers)

## Logs et Monitoring

### Progress Tracking
```python
for future in as_completed(futures):
    lang = futures[future]
    try:
        result = future.result()
        completed_count += 1
        logger.info(
            f"[PIPELINE] ‚ö° Progression: {completed_count}/{len(languages_to_process)} "
            f"langues compl√©t√©es ({lang})"
        )
```

### R√©sum√© Final
```python
logger.info(
    f"[PIPELINE] ‚úÖ {success_count}/{len(languages_to_process)} langues trait√©es "
    f"avec succ√®s en {parallel_time}ms (parall√©lisation r√©elle)"
)
```

## Consid√©rations GPU

### Thread Safety
- **TTS Service**: Thread-safe (chaque thread charge son propre mod√®le)
- **Voice Clone**: Thread-safe (lecture seule des embeddings)
- **Redis Cache**: Thread-safe (connexions ind√©pendantes)

### Limite de Workers
```python
# √âviter surcharge GPU (4 workers par d√©faut)
max_workers = min(len(languages_to_process), 4)
```

### Gestion M√©moire
- Chaque thread a sa propre event loop (overhead minimal)
- Les mod√®les GPU peuvent √™tre partag√©s (selon backend)
- Cache Redis partag√© (pas de duplication)

## Migration Checklist

- [x] Import ThreadPoolExecutor et as_completed
- [x] Cr√©er process_language_sync() avec new_event_loop
- [x] Extraire _process_single_language_async()
- [x] Remplacer asyncio.gather par ThreadPoolExecutor
- [x] Ajouter progress tracking (as_completed)
- [x] Configurer max_workers (env var)
- [x] V√©rifier syntaxe Python
- [ ] Tests d'int√©gration multi-langues
- [ ] Benchmarks de performance

## Tests Recommand√©s

```bash
# Test 2 langues (parall√©lisation compl√®te)
pytest tests/test_parallel_processing.py::test_two_languages -v

# Test 4 langues (limite de workers)
pytest tests/test_parallel_processing.py::test_four_languages -v

# Benchmark asyncio.gather vs ThreadPoolExecutor
pytest tests/benchmark_parallel.py -v
```

## R√©f√©rences

- Script iOS: `ios-simulator/scripts/ios_batch_voice_cloning.py` (lignes 866-903)
- Pattern ThreadPoolExecutor: [Python Docs](https://docs.python.org/3/library/concurrent.futures.html)
- GIL et GPU: [Why ThreadPoolExecutor for GPU](https://stackoverflow.com/questions/68104420/)
