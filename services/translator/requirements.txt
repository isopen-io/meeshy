# Configuration d'environnement
python-dotenv==1.1.1

# Framework web
fastapi==0.115.6
uvicorn[standard]==0.34.0
pydantic-settings==2.7.1
pydantic==2.10.5

# Communication gRPC et ZeroMQ
grpcio==1.69.0
grpcio-tools==1.69.0
grpcio-reflection==1.69.0
protobuf==5.29.3
pyzmq==26.2.0

# Machine Learning et NLP
# Versions gérées par chatterbox-tts pour compatibilité
# torch, torchaudio, transformers, librosa installés via chatterbox-tts
accelerate>=1.0.0
datasets>=3.0.0
sentencepiece>=0.2.0
huggingface_hub>=0.20.0
safetensors>=0.4.0

# Cache et base de données
redis==5.2.1
prisma==0.15.0
asyncpg==0.30.0
aiosqlite==0.20.0

# Détection de langue et analyse de texte
langdetect==1.0.9
nltk==3.9.1
textstat==0.7.4

# Utilitaires système
psutil==6.1.1
# numpy version managed by dependencies (chatterbox-tts requires numpy<1.26.0)
# pandas 2.0.x works with numpy<1.26.0 on Python 3.12
pandas>=2.0.0,<2.1.0
aiohttp==3.11.11
python-multipart==0.0.20
aiofiles==24.1.0

# ═══════════════════════════════════════════════════════════════════════════════
# VOICE FEATURES - MANDATORY FOR MEESHY PLATFORM
# ═══════════════════════════════════════════════════════════════════════════════
# Meeshy requires both STT (Speech-to-Text) and TTS (Text-to-Speech) for:
# - Voice message transcription
# - Translated audio generation with voice cloning
# - Real-time voice translation pipeline
# ═══════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────────
# STT (Speech-to-Text) - MANDATORY
# ─────────────────────────────────────────────────────────────────────────────────
# faster-whisper provides high-quality transcription with language detection
# Supports: Python 3.8+ including Python 3.12 ✓
# Note: 1.1.0+ removed tokenizers dependency (uses ctranslate2's tokenizer)
# and allows av>=11 which has pre-built wheels (av-12.x+)
faster-whisper==1.2.1

# ─────────────────────────────────────────────────────────────────────────────────
# TTS (Text-to-Speech) - MANDATORY - Multi-model support
# Select model via TTS_MODEL environment variable
# Default: chatterbox (recommended for commercial use)
# ─────────────────────────────────────────────────────────────────────────────────

# CHATTERBOX (DEFAULT - MANDATORY) - Apache 2.0 License
# ✓ Best quality/speed balance for production
# ✓ Commercial use allowed
# ✓ Requires Python 3.11 (configured via .python-version)
# Set TTS_MODEL=chatterbox or TTS_MODEL=chatterbox-turbo
chatterbox-tts==0.1.6

# HIGGS AUDIO V2 (OPTIONAL) - State-of-the-art quality
# ✓ Uses transformers (already included above)
# ⚠️ Commercial use limited to <100k users/year
# Set TTS_MODEL=higgs-audio-v2

# ─────────────────────────────────────────────────────────────────────────────────
# LEGACY TTS OPTIONS (NOT INSTALLED BY DEFAULT)
# ─────────────────────────────────────────────────────────────────────────────────

# XTTS v2 (LEGACY - NOT RECOMMENDED)
# ❌ Coqui Public License - NO commercial use allowed
# ❌ REQUIRES Python < 3.12 (incompatible with Python 3.12+)
# ❌ Coqui closed in 2024, model no longer maintained
# ❌ Has dependency conflicts with CPU-only torch from PyTorch index
# To install manually (Python 3.11 only): pip install TTS>=0.22.0
# Set TTS_MODEL=xtts-v2
# TTS==0.22.0  # Commented out - use chatterbox-tts instead (Apache 2.0 license)

# OpenVoice V2 (LEGACY) - Voice Cloning
# To install: pip install git+https://github.com/myshell-ai/OpenVoice.git

# Audio Processing utilities
pydub==0.25.1
# librosa version managed by chatterbox-tts (0.11.0)
soundfile>=0.12.1
scipy>=1.10.0
ffmpeg-python==0.2.0

# ─────────────────────────────────────────────────────────────────────────────────
# SPEAKER DIARIZATION - Identification des locuteurs (MANDATORY)
# ─────────────────────────────────────────────────────────────────────────────────
# pyannote.audio provides state-of-the-art speaker diarization with neural embeddings
# Automatically detects who is speaking and when with ≥95% accuracy
# Supports short utterances (e.g., "Oui") and prevents false positives on mono-speaker audio
#
# Fonctionnalités:
#   ✅ Détection automatique de plusieurs locuteurs
#   ✅ Identification du locuteur principal
#   ✅ Détection de courtes interjections (1 mot)
#   ✅ Prévention des faux positifs sur audio mono-speaker
#   ✅ Support pyannote.audio (précis) avec fallback pitch clustering
#
# Configuration:
#   ENABLE_DIARIZATION=true dans .env (activé par défaut)
#   HF_TOKEN=your_token (optionnel mais recommandé pour pyannote.audio)
# ─────────────────────────────────────────────────────────────────────────────────
pyannote.audio>=3.1.0
scikit-learn>=1.3.0
# librosa déjà inclus dans requirements.txt via chatterbox-tts

# Logging et monitoring
loguru==0.7.3
prometheus-client==0.21.1

# Tests
pytest==8.3.4
pytest-asyncio==0.25.2
pytest-cov==6.0.0
# FIX: httpx 0.24.x pour compatibilité avec Prisma 0.15.0
httpx==0.24.1
httpcore==0.17.3
